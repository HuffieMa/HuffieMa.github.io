<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>马浩飞丨博客</title>
  
  <subtitle>无限进步！！！</subtitle>
  <link href="https://www.mahaofei.com/atom.xml" rel="self"/>
  
  <link href="https://www.mahaofei.com/"/>
  <updated>2023-05-15T09:05:05.000Z</updated>
  <id>https://www.mahaofei.com/</id>
  
  <author>
    <name>马浩飞</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ROS系统Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/4add66b0.html"/>
    <id>https://www.mahaofei.com/post/4add66b0.html</id>
    <published>2023-05-15T09:05:05.000Z</published>
    <updated>2023-05-15T09:05:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、安装问题</h1><h2 id="ROS安装时rosdep-init与rosdep-update问题解决方法">ROS安装时rosdep_init与rosdep_update问题解决方法</h2><p><strong>解决方法</strong></p><p>使用下面的命令替代上面两行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3-pip</span><br><span class="line">sudo pip3 install rosdepc</span><br><span class="line">sudo rosdepc init</span><br><span class="line">rosdepc update</span><br></pre></td></tr></table></figure><h1>二、环境问题</h1><h2 id="Unable-to-find-either-executable-‘empy’-or-Python-module-‘em’…-try-installing-the-package-‘python3-empy’">Unable to find either executable ‘empy’ or Python module ‘em’…  try  installing the package ‘python3-empy’</h2><p><strong>（1）问题原因</strong></p><p>Anaconda使用的是Python3版本，但是ROS使用的Python2</p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="Could-not-find-a-package-configuration-file-provided-by-“某某包”-with-any-of-the-following-names">Could not find a package configuration file provided by “某某包” with any of  the following names</h2><p><strong>（1）问题原因</strong></p><p>缺少<code>某某包</code></p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ros-noetic-某某包</span><br></pre></td></tr></table></figure><h1>三、配置问题</h1><h2 id="ERROR-cannot-launch-node-of-type-robot-state-publisher-state-publisher-Cannot-locate-node-of-type-state-publisher-in-package-robot-state-publisher-Make-sure-file-exists-in-package-path-and-permission-is-set-to-executable-chmod-x）">ERROR: cannot launch node of type [robot_state_publisher/state_publisher]: Cannot locate node of type [state_publisher] in package [robot_state_publisher]. Make sure file exists in package path and permission is set to executable (chmod +x）</h2><p><strong>（1）问题原因</strong></p><p>使用launch文件启动某个节点时出现这个问题，是因为launch文件中name、pkg、type不统一导致的。</p><p><strong>（2）解决方法</strong></p><p>检查launch文件，确保name、pkg、type一样，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot; /&gt;</span><br></pre></td></tr></table></figure><h2 id="joint-state-publisher-gui没有显示">joint state publisher gui没有显示</h2><p><strong>（1）问题描述</strong></p><p>使用ROS进行仿真，想用joint state publisher进行机械臂控制，但是启动launch文件后没有报错信息，但也没有joint state publisher gui。</p><p><strong>（2）解决方法</strong></p><p>2020年开始，gui已经移出了 joint state publisher, 并且成为了一个新的package：joint state publisher gui. 之前那种使用gui参数的方式调用joint state publisher 是仍然可行的，但是不会调用gui。</p><p>在launch文件中，将joint state publisher 替换成joint__state__publisher_gui。</p>]]></content>
    
    
    <summary type="html">在使用ROS系统进行机器人实验中，遇到的各种错误信息汇总，不定时更新。</summary>
    
    
    
    <category term="机器人" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="ros" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/ros/"/>
    
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
  </entry>
  
  <entry>
    <title>_ROS Gazebo机械臂抓取仿真</title>
    <link href="https://www.mahaofei.com/post/e597bf8f.html"/>
    <id>https://www.mahaofei.com/post/e597bf8f.html</id>
    <published>2023-05-15T07:23:56.000Z</published>
    <updated>2023-05-15T07:23:56.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、基础知识</h1><h2 id="1-1-URDF">1.1 URDF</h2><p>URDF是ROS中机器人模型的描述格式，包括机器人的外观、物理属性、关节类型等方面。</p><ul><li><code>&lt;robot&gt;</code>：最顶层标签</li><li><code>&lt;link&gt;</code>：描述刚提的外观形状、碰撞几何、颜色、惯性矩阵等</li><li><code>&lt;joint&gt;</code>：描述两个link之间的关系，有6种类型，最常用的是<code>revolute</code>类型，有关节位置限制的旋转关节</li></ul><p>xacro模型可以将部分URDF打包成一个&quot;类&quot;，在其他模型中调用。</p><p>功能包中一般包括以下四个部分</p><ol><li><code>cfg</code>：配置文件</li><li><code>launch</code>：加载urdf模型，并在rviz中展示</li><li><code>meshes</code>：urdf用到的外观模型</li><li><code>urdf</code>：urdf模型定义</li></ol><h2 id="1-2-Gazebo">1.2 Gazebo</h2><h2 id="1-3-Moveit控制">1.3 Moveit控制</h2><p><strong>（1）Moveit!大致功能</strong></p><ul><li>运动学计算</li><li>运动规划</li><li>碰撞检测</li></ul><p>最重要的节点是<code>move_group</code>，输入可以是RVIZ中的数据或点云和深度图。路径规划一般使用的OMPL库，碰撞检测使用FCL库。最后发送个机械臂让机械臂执行轨迹。</p><h2 id="1-4-机械臂运动规划">1.4 机械臂运动规划</h2><h2 id="1-5-基于深度学习的视觉避障">1.5 基于深度学习的视觉避障</h2><h1>二、实验环境搭建</h1><h2 id="1-1-安装ROS">1.1 安装ROS</h2><p>参考<a href="https://www.mahaofei.com/post/b278544f.html">Ubuntu20.04安装ROS Noetic</a>文章</p><p>![[01-Ubuntu20.04安装ROS Noetic#二、安装ROS]]</p><h2 id="1-2-安装Moveit">1.2 安装Moveit!</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ros-noetic-moveit</span><br></pre></td></tr></table></figure><h2 id="1-3-安装UR机器人及驱动">1.3 安装UR机器人及驱动</h2><p>复制代码后，修改下面的内容，使其能在noetic版本的ros上运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/srv/SetPayload.srv</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">float32 payload</span><br><span class="line">geometry_msgs/Vector3 center_of_gravity</span><br><span class="line">-----------------------</span><br><span class="line">bool success</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/CMakeLists.txt</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8.3)</span><br><span class="line">project(ur_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Find catkin macros and libraries</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># is used, also find other catkin packages</span></span></span><br><span class="line">find_package(catkin REQUIRED COMPONENTS message_generation std_msgs geometry_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate messages in the &#x27;msg&#x27; folder</span></span></span><br><span class="line">add_message_files(</span><br><span class="line">   FILES</span><br><span class="line">   Analog.msg</span><br><span class="line">   Digital.msg</span><br><span class="line">   IOStates.msg</span><br><span class="line">   RobotStateRTMsg.msg</span><br><span class="line">   MasterboardDataMsg.msg</span><br><span class="line">   RobotModeDataMsg.msg</span><br><span class="line">   ToolDataMsg.msg</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate services in the &#x27;srv&#x27; folder</span></span></span><br><span class="line">add_service_files(</span><br><span class="line">   FILES</span><br><span class="line">   SetPayload.srv</span><br><span class="line">   SetSpeedSliderFraction.srv</span><br><span class="line">   SetIO.srv</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate added messages and services with any dependencies listed here</span></span></span><br><span class="line">generate_messages(</span><br><span class="line">   DEPENDENCIES</span><br><span class="line">   std_msgs</span><br><span class="line">   geometry_msgs</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># catkin specific configuration ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line">catkin_package(</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> INCLUDE_DIRS include</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> LIBRARIES ur_msgs</span></span><br><span class="line">   CATKIN_DEPENDS message_runtime std_msgs geometry_msgs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> DEPENDS system_lib</span></span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Build ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Install ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Testing ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/package.xml</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;package format=&quot;2&quot;&gt;</span><br><span class="line">  &lt;name&gt;ur_msgs&lt;/name&gt;</span><br><span class="line">  &lt;version&gt;1.2.5&lt;/version&gt;</span><br><span class="line">  &lt;description&gt;The ur_msgs package&lt;/description&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;author&gt;Andrew Glusiec&lt;/author&gt;</span><br><span class="line">  &lt;author&gt;Felix Messmer&lt;/author&gt;</span><br><span class="line">  &lt;maintainer email=&quot;g.a.vanderhoorn@tudelft.nl&quot;&gt;G.A. vd. Hoorn&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;miguel.prada@tecnalia.com&quot;&gt;Miguel Prada Sarasola&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;nhg@ipa.fhg.de&quot;&gt;Nadia Hammoudeh Garcia&lt;/maintainer&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;license&gt;BSD&lt;/license&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt;</span><br><span class="line">  &lt;build_depend&gt;message_generation&lt;/build_depend&gt;</span><br><span class="line">  &lt;depend&gt;std_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;depend&gt;geometry_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;</span><br><span class="line"></span><br><span class="line">  &lt;export&gt;</span><br><span class="line">  &lt;/export&gt;</span><br><span class="line">&lt;/package&gt;</span><br></pre></td></tr></table></figure><p>完成之后，就可以编译了。<code>source</code>之后使用<code>roslaunch ur5_moveit_config demo.launch</code></p><h2 id="1-4-简单测试">1.4 简单测试</h2><p><strong>（1）Rviz打开UR5模型</strong></p><p>机械臂夹爪模型路径为<code>universal_robot/urdf/ur5_gripper_joint_limited_robot.urdf.xacro</code>。</p><p>可以通过<code>universal_robot/ur_description/launch/view_ur5_with_gripper.launch</code>启动，从Rviz中查看模型情况，并使用<code>joint_state_publisher_gui</code>对机械臂模型拖动控制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5_with_gripper.launch</span><br></pre></td></tr></table></figure><p><strong>（2）Rviz中Moveit测试</strong></p><p>使用下面的程序可以在 Rviz 中进行 Moveit 轨迹规划测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur5_moveit_config demo.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur5_gripper_moveit_config demo.launch</span><br></pre></td></tr></table></figure><p><strong>（3）Gazebo中Moveit测试</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5.launch</span><br><span class="line">roslaunch ur5_moveit_config ur5_moveit_planning_execution.launch sim:=true</span><br><span class="line">roslaunch ur5_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><code>ur5.launch</code>：用于启动 gazebo 仿真环境。具体包括以下几个部分，启动空环境、定义 robot_description 参数服务器、发送到gazebo中生成机器人、启动并加载控制器。</p><p><code>ur5_moveit_planning_execution.launch</code>：用于启动 MoveIt 相关组件。具体包括以下几个部分：设置 sim参数， 根据 sim 参数重映射 follow_joint_trajectory 话题，启动MoveIt。</p><p><code>moveit_rviz.launch</code>：用于启动 Rviz 相关组件。具体包括以下几个部分：加载配置参数，启动Rviz。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">roslaunch ur_gazebo ur5_with_gripper.launch</span><br><span class="line">roslaunch ur5_single_arm_moveit_config ur5_moveit_planning_execution.launch</span><br><span class="line">roslaunch ur5_gripper_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><h2 id="1-5-导入自定义物体">1.5 导入自定义物体</h2><p><strong>（1）网络方法</strong></p><p>使用 MeshLab 加载自己的物体模型。</p><p>点击【Filters -&gt; Normals … -&gt; Compute normals for points sets】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Remeshing -&gt; Surface Reconstruction: Screened Poisson】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Trivial Per-Triangle】，按照如下设置，重要的是Method。</p><p><img src="https://img.mahaofei.com/img/202305242228283.png" alt="image.png"></p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】，按照如下设置，重要的是Source Mesh和Target Mesh。</p><p><img src="https://img.mahaofei.com/img/202305242231021.png" alt="image.png"></p><p><strong>（2）摸索方法</strong></p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Flat Plane】</p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】</p>]]></content>
    
    
    <summary type="html">使用Gazebo搭建6D机械臂仿真环境，并添加相机，然后实现抓取仿真实验。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E5%AE%9E%E9%AA%8C/"/>
    
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>【目标检测算法】YOLOV8代码复现</title>
    <link href="https://www.mahaofei.com/post/16b5f6b3.html"/>
    <id>https://www.mahaofei.com/post/16b5f6b3.html</id>
    <published>2023-05-06T06:13:12.000Z</published>
    <updated>2023-05-06T06:13:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、算法笔记</h1><h1>二、代码复现</h1><h2 id="2-1-搭建环境">2.1 搭建环境</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolov8 python=3.7</span><br><span class="line">conda activate yolov8</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>下载作者开源的程序，并安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/ultralytics</span><br><span class="line">cd ultralytics</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-命令行使用教程">2.2 命令行使用教程</h2><p><strong>（1）语法规则</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yolo TASK MODE ARGS</span><br><span class="line"></span><br><span class="line">Where   TASK (optional) is one of [detect, segment, classify]</span><br><span class="line">        MODE (required) is one of [train, val, predict, export, track]</span><br><span class="line">        ARGS (optional) are any number of custom &#x27;arg=value&#x27; pairs like &#x27;imgsz=320&#x27; that override defaults.</span><br></pre></td></tr></table></figure><p><strong>（2）训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01</span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo predict model=yolov8n-seg.pt source=&#x27;https://youtu.be/Zgi9g1ksQHc&#x27; imgsz=320</span><br></pre></td></tr></table></figure><p><strong>（4）评价</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640</span><br></pre></td></tr></table></figure><h2 id="2-3-Python使用教程">2.3 Python使用教程</h2><p><strong>（1）训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n.pt&#x27;</span>) <span class="comment"># 从预训练模型开始</span></span><br><span class="line">model.train(epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><strong>（2）评价</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line">model.val()  <span class="comment"># 使用model.pt的data yaml进行评价</span></span><br><span class="line">model.val(data=<span class="string">&#x27;coco128.yaml&#x27;</span>)  <span class="comment"># 或指定数据进行评价</span></span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><p>获取预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line"><span class="comment"># 接受所有类型 - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam</span></span><br><span class="line"><span class="comment"># 从摄像头</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;0&quot;</span>)</span><br><span class="line"><span class="comment"># 从文件夹</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>, show=<span class="literal">True</span>) <span class="comment"># Display preds. Accepts all YOLO predict arguments</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL图像</span></span><br><span class="line">im1 = Image.<span class="built_in">open</span>(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im1, save=<span class="literal">True</span>)  <span class="comment"># save plotted images</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从ndarray</span></span><br><span class="line">im2 = cv2.imread(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im2, save=<span class="literal">True</span>, save_txt=<span class="literal">True</span>)  <span class="comment"># save predictions as labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL/ndarray的列表</span></span><br><span class="line">results = model.predict(source=[im1, im2])</span><br></pre></td></tr></table></figure><p>预测结果分析（results会包含预测所有结果的列表，当有很多图像的时候要注意避免内存溢出，特别是在实例分割时）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. return as a list</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  return as a generator (stream=True)</span></span><br><span class="line">results = model.predict(source=<span class="number">0</span>, stream=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># Detection</span></span><br><span class="line">    result.boxes.xyxy   <span class="comment"># box with xyxy format, (N, 4)</span></span><br><span class="line">    result.boxes.xywh   <span class="comment"># box with xywh format, (N, 4)</span></span><br><span class="line">    result.boxes.xyxyn  <span class="comment"># box with xyxy format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.xywhn  <span class="comment"># box with xywh format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.conf   <span class="comment"># confidence score, (N, 1)</span></span><br><span class="line">    result.boxes.cls    <span class="comment"># cls, (N, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Segmentation</span></span><br><span class="line">    result.masks.data      <span class="comment"># masks, (N, H, W)</span></span><br><span class="line">    result.masks.xy        <span class="comment"># x,y segments (pixels), List[segment] * N</span></span><br><span class="line">    result.masks.xyn       <span class="comment"># x,y segments (normalized), List[segment] * N</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Classification</span></span><br><span class="line">    result.probs     <span class="comment"># cls prob, (num_class, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Each result is composed of torch.Tensor by default, </span></span><br><span class="line"><span class="comment"># in which you can easily use following functionality:</span></span><br><span class="line">result = result.cuda()</span><br><span class="line">result = result.cpu()</span><br><span class="line">result = result.to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">result = result.numpy()</span><br></pre></td></tr></table></figure><h2 id="2-3-数据集制作（实例分割）">2.3 数据集制作（实例分割）</h2><p><strong>（1）使用Labelme创建实例分割数据集</strong></p><p>安装labelme</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme</span><br></pre></td></tr></table></figure><p>安装完成后直接在命令行输入<code>labelme</code>即可打开。</p><p>使用label进行标注，将生成的json文件和原始图像jpg，放入同一个文件夹中。</p><p><strong>（2）Labelme格式转COCO格式</strong></p><p>参考<a href="https://pypi.org/project/labelme2coco/">pypi的labelme2coco包</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme2coco</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者使用清华源</span></span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple labelme2coco</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labelme2coco path/to/labelme/dir --train_split_rate 0.85</span><br></pre></td></tr></table></figure><p><strong>（3）COCO格式转YOLO格式</strong></p><p>使用<code>labelme</code>制作实例分割的coco格式数据集，然后使用<a href="https://github.com/ultralytics/JSON2YOLO">ultralytics/JSON2YOLO</a>项目将json文件转换成yolo的训练格式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/JSON2YOLO.git</span><br><span class="line">cd JSON2YOLO</span><br></pre></td></tr></table></figure><p>创建一个虚拟环境，然后使用下面的命令安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第387行，设置为刚才得到的COCO注释的位置，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> source == <span class="string">&#x27;COCO&#x27;</span>:</span><br><span class="line">convert_coco_json(<span class="string">&#x27;datasets/20230223_Phone_4Obj_Coco/annotations&#x27;</span>,  <span class="comment"># directory with *.json</span></span><br><span class="line">  use_segments=<span class="literal">True</span>,</span><br><span class="line">  cls91to80=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第289行，因为不是coco80中的物体类型，是自己设置的，因此需要修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cls = coco80[ann[&#x27;category_id&#x27;] - 1] if cls91to80 else ann[&#x27;category_id&#x27;] - 1  # class</span></span><br><span class="line">cls = ann[<span class="string">&#x27;category_id&#x27;</span>]  <span class="comment"># class</span></span><br></pre></td></tr></table></figure><p>运行程序将COCO格式json文件转换为YOLO格式txt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python general_json2yolo.py</span><br></pre></td></tr></table></figure><p>结果保存在new_dir中，需要手动把images复制过去。最后得到的数据集如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_root</span><br><span class="line">├── images</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">└── labels</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br></pre></td></tr></table></figure><p><strong>（4）创建数据集的YAML文件</strong></p><p>打开目录<code>ultralytics/datasets</code>，复制一份其中的<code>coco128-seg.yaml</code>，重命名为<code>custom-seg.yaml</code>，然后根据自己的数据集进行修改。</p><p>例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span></span><br><span class="line"><span class="attr">path:</span> <span class="string">/media/mahaofei/OneTouch/Dataset/Program_data/image_processing/ultralytics/20230223_Phone_4Obj_YOLO</span>  <span class="comment"># dataset root dir</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">images/train2017</span>  <span class="comment"># train images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">images/train2017</span>  <span class="comment"># val images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">test:</span>  <span class="comment"># test images (optional)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classes</span></span><br><span class="line"><span class="attr">names:</span></span><br><span class="line">  <span class="attr">0:</span> <span class="string">ammeter</span></span><br><span class="line">  <span class="attr">1:</span> <span class="string">coffeebox</span></span><br><span class="line">  <span class="attr">2:</span> <span class="string">realsensebox</span></span><br><span class="line">  <span class="attr">3:</span> <span class="string">sucker</span></span><br></pre></td></tr></table></figure><h2 id="2-4-开始训练">2.4 开始训练</h2><p>新建一个python文件如<code>train.py</code>，添加内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a model</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;)  # build a new model from YAML</span></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n-seg.pt&#x27;</span>)  <span class="comment"># load a pretrained model (recommended for training)</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;).load(&#x27;yolov8n.pt&#x27;)  # build from YAML and transfer weights</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.train(data=<span class="string">&#x27;custom-seg.yaml&#x27;</span>, epochs=<span class="number">100</span>, imgsz=<span class="number">3904</span>, batch=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="2-5-结果预测">2.5 结果预测</h2>]]></content>
    
    
    <summary type="html">使用经典的YOLO算法进行实例分割</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【抓取姿态估计算法】RGB Matters论文笔记与复现</title>
    <link href="https://www.mahaofei.com/post/a1b0a01b.html"/>
    <id>https://www.mahaofei.com/post/a1b0a01b.html</id>
    <published>2023-05-04T00:30:31.000Z</published>
    <updated>2023-05-04T00:30:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><p><strong>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</strong></p><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="1-1-目标问题-3">1.1 目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="1-2-方法-2">1.2 方法</h2><p>大致流程为：</p><ol><li>使用Angle View Net网络生成图像不同位置的抓取器方向 $P_{img}=(u,v,r_x,r_y,r_z,c)$，即图像中坐标的位置，其对应的夹爪旋转姿态，以及置信度。</li><li>对置信度高的预测，结合深度图计算距离和夹爪宽度$P_{cam}=x,y,z,rx,ry,rz,w$</li></ol><p><strong>（0）定义</strong></p><p>抓握姿势定义为 (x, y, z, rx, ry, rz, w)，其中(x, y, z)代表夹持器的位置，(rx, ry, rz)代表夹持器的旋转，w代表夹持器的宽度。</p><p>夹持器本文仅考虑平行夹爪，使用 (h, l, wmax) 定义，三个参数分别代表夹具的 高度、长度和最大宽度。</p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒（因为同一个位置进行抓取有不止一个可行的旋转）。</p><p>可以使用下面的模型，将方向解耦为接近方向和绕平面的旋转，将夹持器旋转预测作为一个分类问题进行预测，共有VxA类方向。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>网络通过将RGB图像栅格化，对于每一个网格，AVN预测一个1维VxA个元素的向量，包含每个方向的置信度。最终得到(VxA)xGHxGW的tensor。AVN最终的输出表示为每个角度的heatmap。</p><p>作者在代码中给出的是V=60,A=6的测试。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="1-3-思考-2">1.3 思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1>二、复现过程</h1><h2 id="2-1-环境搭建-2">2.1 环境搭建</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n rgb_matters python=3.7</span><br><span class="line">conda activate rgb_matters</span><br></pre></td></tr></table></figure><p>下载程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/GouMinghao/rgb_matters</span><br><span class="line">cd rgb_matters</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-测试Demo">2.2 测试Demo</h2><p>下载作者训练好的模型：<a href="https://drive.google.com/drive/folders/1upW4gvQk5ftXfpLHtvCogudpP4kNyoGq?usp=sharing">Google Drive</a></p><p>在代码目录创建一个<code>weights</code>的目录，然后将下载的模型放入其中，完成后文件夹结构如下</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rgbd_graspnet/</span><br><span class="line">├── check_label_integrity.py</span><br><span class="line">├── train.py</span><br><span class="line">├── train.sh</span><br><span class="line">├── vis_label.py</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">└── weights</span><br><span class="line">    ├── kn_jitter_79200.pth</span><br><span class="line">    ├── kn_no_norm_76800.pth</span><br><span class="line">    ├── kn_norm_63200.pth</span><br><span class="line">    ├── kn_norm_only_73600.pth</span><br><span class="line">    └── rs_norm_56400.pth</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230424162408.png" alt=""></p><h1>三、代码分析</h1><h2 id="3-1-输出结果分析">3.1 输出结果分析</h2><p><strong>（1）热力图获取</strong></p><p>使用下面的代码进行预测热力图</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = RGBNormalNet(num_layers=args.num_layers, use_normal=args.use_normal, normal_only=args.normal_only)</span><br><span class="line">state_dict = torch.load(weights_path)</span><br><span class="line">net.load_state_dict(state_dict[&quot;net&quot;], strict=False)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net.eval()</span><br><span class="line"></span><br><span class="line">rgb, _ = load_data(rgb_path, depth_path)</span><br><span class="line"></span><br><span class="line">rgb = rgb.unsqueeze(0).to(device)</span><br><span class="line"></span><br><span class="line">prob_map = net(rgb)</span><br></pre></td></tr></table></figure><p>其中的<code>prob_mat</code>是预测的热力图<code>shape=(batch_size, 360, h, w)</code>一共360张热力图（360张包括接近方向v=60和平面内旋转A=6）</p><p><strong>（2）夹爪姿态获取</strong></p><p>使用<code>convert_grasp()</code>函数从360张热力图中提取夹爪姿态。</p><p>夹爪姿态为<code>Grasp</code>实例，包括以下几个参数：</p><ul><li><code>score</code>：float类型，抓取得分</li><li><code>width</code>：float类型，夹爪宽度</li><li><code>height</code>：float类型，夹爪高度</li><li><code>depth</code>：float类型，夹爪深度</li><li><code>rotation_matrix</code>：shape(3, 3)数组，旋转矩阵</li><li><code>translation</code>：shape(3)数组，平移向量</li><li><code>object_id</code>：int类型，抓取物体类别</li></ul><p>具体参考下面两张图：</p><p><img src="https://img.mahaofei.com/img/202305142107952.png" alt="image.png"></p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p>]]></content>
    
    
    <summary type="html">复现上交提出的RGB Matters算法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>抓取姿态估计算法调研</title>
    <link href="https://www.mahaofei.com/post/791dd0f5.html"/>
    <id>https://www.mahaofei.com/post/791dd0f5.html</id>
    <published>2023-04-23T07:16:08.000Z</published>
    <updated>2023-04-23T07:16:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Hybrid Physical Metric For 6-DoF Grasp Pose Detection</h1><blockquote><p><strong>标题</strong>：用于6D抓取检测的混合物理度量<br><strong>作者团队</strong>：清华大学（王生进）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/luyh20/FGC-GraspNet">https://github.com/luyh20/FGC-GraspNet</a></p></blockquote><h2 id="一、目标问题">一、目标问题</h2><p>单个物理指标会导致离散的抓取置信度分数，在百万抓取数据训练时会导致预测结果不准确。</p><p>本文定义了一种新的度量方式，基于力封闭度量、物体平面度、重力和碰撞测量。</p><p>本文设计了平面重力碰撞FGC-GraspNet，适用于多任务多分辨率学习体系。</p><h2 id="二、混合物理度量">二、混合物理度量</h2><p><img src="https://img.mahaofei.com/img/202304231518654.png" alt="image.png"></p><p><strong>（1）平面度</strong></p><p>平面度越高的抓的越稳。利用点的局部法向量的相似性计算平坦度得分。</p><p><strong>（2）重心度量</strong></p><p>夹持力更接近物体重心的更稳定。计算物体重心到两个接触点的连线的距离作为重力得分。</p><p><strong>（3）碰撞扰动度量</strong></p><p>当夹爪接近物体时容易发生碰撞，因此取夹爪两个最大行程端点与物体接触点的欧氏距离最小值作为碰撞扰动得分。</p><p><strong>（4）混合物理度量</strong></p><p>混合物理度量是上面度量的加权组合。</p><h2 id="三、-FGC-GraspNet">三、 FGC-GraspNet</h2><p>通过最远点采样FPS得到20000x3对点云输入，网络由PointNet++，FA分支、RD分支组成。</p><p><img src="https://img.mahaofei.com/img/202304231530817.png" alt=""></p><ul><li>PointNEt++用于提取点特征</li><li>低分辨率的特征进入FA分支进行前景分割和逐点逼近方向得分回归</li><li>高分辨率的特征用于RD旋转分支。</li></ul><h2 id="四、思考">四、思考</h2><p>将混合物理度量纳入LOSS的计算过程确实有意义。而且具有一定的复用性，其它算法也可借鉴此设计。</p><h1>Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter</h1><blockquote><p><strong>标题</strong>：基于体素的抓取网络：杂乱场景中实时6D抓取检测<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：CoRL2020<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/vgn">https://github.com/ethz-asl/vgn</a></p></blockquote><h2 id="一、目标问题-2">一、目标问题</h2><p>本文提出了一种网络从深度相机中获得场景信息，预测6D抓取的网络。</p><h2 id="二、论文方法">二、论文方法</h2><p><strong>（1）网络架构</strong></p><p>由滤波器、卷积层组成的感知模块将输入体素映射为特征图，然后进行卷积、上采样操作，最后是三个独立的分支用于预测抓取质量、旋转和夹爪宽度。</p><p><strong>（2）抓取检测</strong></p><p>使用一些方法去除不可能的抓取姿势，然后应用非极大抑制来获得候选的抓取列表。</p><h2 id="三、思考">三、思考</h2><p>非常基础的方法，已经有人在此基础上进行了扩展并发表了顶会。</p><h1>Efficient Learning of Goal-Oriented Push-Grasping Synergy in Clutter</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：浙江大学（熊蓉）<br><strong>期刊会议</strong>：RAL<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy">https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy</a></p></blockquote><h2 id="一、目标问题-3">一、目标问题</h2><p>在混乱场景中抓取物体时，有时需要一些预抓取动作，例如推动。使机械臂能够分离目标对象并稳定的实现抓取。</p><h2 id="二、方法">二、方法</h2><p><img src="https://img.mahaofei.com/img/202304231536100.png" alt=""></p><p>环境准备：固定的RGBD相机拍摄工作空间，将RGBD投影到重力方向，使用颜色高度图和深度高度图表示每个状态。</p><p><strong>（1）有目标的抓取训练</strong></p><p>训练一个有目标条件下的抓取网络，当有足够的训练之后，成功抓取的Q值稳定。</p><p><strong>（2）有目标的推动训练</strong></p><p>训练一个有目标条件下的推动网络，推动的奖励函数是基于抓取网络反向训练设计的。</p><p><strong>（3）交替训练</strong></p><p>利用交替训练来解决物体分布不匹配的问题，进一步提高混乱环境中抓取策略性能。</p><h2 id="三、思考-2">三、思考</h2><p>推物体再抓取物体相当于一个两阶段方法，可以不用在底层进行训练，而是在高层的规划决策层来进行判断，发布任务是推物体还是抓物体。</p><h1>TransGrasp: Grasp Pose Estimation ofaCategory ofObjects byTransferring Grasps fromOnlyOne Labeled Instance</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：大连理工大学（孙怡）<br><strong>期刊会议</strong>：ECCV<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/yanjh97/TransGrasp">https://github.com/yanjh97/TransGrasp</a></p></blockquote><h2 id="一、目标问题-4">一、目标问题</h2><p>现有大多数方法需要大量的抓取数据来训练，为了解决这个问题，本文实现只标记一个对象预测一类对象的抓取姿态。</p><h2 id="二、方法-2">二、方法</h2><p><strong>（1）学习类别的对应关系</strong></p><ol><li>Shape Encoder和DIFDecoder组成神经网络，训练得到对象变形到模板的密集对应关系</li></ol><p><strong>（2）抓取姿态估计</strong></p><ol><li>点云首先从相机坐标系转换到对象坐标系</li><li>生成对象实例的变形到模板</li><li>将带有抓取注释的模型输入到DeformNet中获得模型的变形</li><li>由两者的共同模板见你对应关系，通过对准物体表面上的抓握点来引导抓握姿势的变换</li><li>通过refine模块进行优化</li><li>将优化后的抓握知识转换为相机坐标进行抓取</li></ol><h2 id="三、思考-3">三、思考</h2><p>这种算法只能实现与模板形状相似的物体进行抓取，而且每个类别要先手工标记1000个抓握姿势。</p><h1>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</h1><blockquote><p><strong>标题</strong>：ContactGraspNet：在杂乱场景中高效生成6-DoF抓取<br><strong>作者团队</strong>：NVIDIA<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a></p></blockquote><h2 id="1-目标问题-22">1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="2-方法-22">2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/202304231545643.png" alt=""></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/202304231546650.png" alt=""></p><h2 id="3-思考-22">3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</h1><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="一、目标问题-5">一、目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="二、方法-3">二、方法</h2><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒。可以使用下面的模型，将夹持器旋转预测作为一个分类问题进行预测。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>AVN最终的输出表示为角视图热图。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="三、思考-4">三、思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1>CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation</h1><blockquote><p><strong>标题</strong>：CaTGrasp：从仿真中学习杂乱场景的类别级抓取<br><strong>作者团队</strong>：Rutgers University（美国罗格斯大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/wenbowen123/catgrasp">https://github.com/wenbowen123/catgrasp</a></p></blockquote><h2 id="一、目标问题-6">一、目标问题</h2><p>提出了一个框架学习工业对象的抓取，不需要真实的数据或手动注释</p><h2 id="二、方法-4">二、方法</h2><p>给定同一类别的3D模型的数据库，该方法学习</p><ul><li>以对象为中心的NUNOCS表示</li><li>hotmap：抓握过程中手-对象接触区域的任务实现成功的可能性</li><li>抓取姿势的编码本</li></ul><p><strong>（1）类别级标准NUNOCS表示</strong></p><p>将同一个类别的不同实例对象转换到标准空间，并缩放为标准大小。</p><p><strong>（2）稳定抓取学习</strong></p><p>首先给定从当前实例到规范模型的9D变换，将相同的变换应用于抓取来得到抓取建议。</p><p>将生成的抓取用于训练基于PointNet构建的网络，预测抓取质量。</p><p><strong>（3）实例分割</strong></p><p>使用了3D U-Net，将整个场景的点云作为输入，预测每点偏移到物体中心，将偏移点聚类为实例段。</p><p><strong>（4）仿真中生成训练数据</strong></p><p>利用PyBullet模拟生成合成数据。</p><h2 id="三、思考-5">三、思考</h2><p>该方法提出了一种使用仿真数据进行训练，减少人工标注的方法。抓取的方法没有太多创新，仍然需要每个类别提供多个预先的实例以及抓取姿态用于训练。</p><h1>Closed-Loop Next-Best-View Planning for Target-Driven Grasping</h1><blockquote><p><strong>标题</strong>：闭环次优视图规划用于目标驱动的抓取<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/active_grasp">https://github.com/ethz-asl/active_grasp</a></p></blockquote><h2 id="一、目标问题-7">一、目标问题</h2><p>从密集遮挡环境中抓取物体</p><h2 id="二、方法-5">二、方法</h2><p>该方法具有以下前提条件</p><ul><li>机械臂末端连接深度相机</li><li>相机光学中心和手抓中心已经校准</li><li>已知物体的部分视图和3D边界框</li></ul><p><img src="https://img.mahaofei.com/img/202304231603042.png" alt=""></p><p>首先将点云观测yt和相机姿态xt继承，重建为体素图。计算体素的可抓取性，以及可能的抓取姿态。如果可抓取性不满足要求，就调整机械臂位置计算下一张图</p><p><strong>（1）抓取检测</strong></p><p>使用体积抓取网络VGN进行抓取。该网络将体素网格M映射到抓握质量分数Q、平行抓握方向R、开口宽度W。</p><p>过滤掉指尖不在目标边界框的抓取姿态、无法找到反向运动学解的抓取姿态。</p><p><strong>（2）次优视图规划器</strong></p><p><strong>世界表示</strong>：使用TSDF（截断有符号距离函数）表示大小为 l 的立方体体素。</p><p><strong>视图生成</strong>：将候选视图生成在目标边界上半球内。</p><p><strong>信息增益</strong>：TSDF 重建的完整性对于抓取的检测和预测准确性有很大影响。因此使用了后侧体素 IG 公式的变体，对被遮挡具有负距离的体素使用光线投影来计算隐藏对象体素的数量。</p><ol><li>在策略更新的最大数量中加入时间预算</li><li>如果抓取分数低于给定的阈值，就会停止算法，因为获取不到有用信息</li><li>如果VGN在几帧内保持稳定的抓取配置，就停止</li></ol><h2 id="三、思考-6">三、思考</h2><p>该方法是在位姿估计的基础上进行的抓取预测，可以将该方法与Gen6D结合起来，获得物体的6D抓取位姿。</p><h1>Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection</h1><blockquote><p><strong>标题</strong>：边缘抓取网络：一种基于图的SE(3)不变的抓取检测方法<br><strong>作者团队</strong>：Northeastern University（美国东北大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/HaojHuang/Edge-Grasp-Network">https://github.com/HaojHuang/Edge-Grasp-Network</a></p></blockquote><h2 id="一、目标问题-8">一、目标问题</h2><p>以单个视角观察到的点云为输入，得到一组抓取姿态</p><h2 id="二、方法-6">二、方法</h2><p><strong>（1）裁剪点云</strong></p><p>给定一个点云p和接近点pa，只有接近点pa的相邻点会影响抓取，因此以pa为中心裁剪一个球。</p><p><strong>（2）PointNet卷积</strong></p><p>使用PointNet计算接近点与最近邻点，每个点的特征。</p><p><strong>（3）计算全局特征</strong></p><p>将逐点特征传递给MLP，用最大池化层生成一级全局特征。这些全局特征再与点特征相连传递到第二个MLP计算全局特征。</p><p>对于每个抓取，通过将全局特征与点特征连接来计算边缘特征，用分类器表示边缘抓取。</p><p><strong>（4）抓取评估</strong></p><p>使用sigmoid函数的四层MLP来预测抓取成功率，以边缘特征为输入计算抓取是否成功。</p><h2 id="三、思考-7">三、思考</h2><p>该方法类似于DenseFusion的思想，即提取逐点特征和全局特征，进行特征融合，本文得到的融合特征即边缘特征，利用该特征再使用分类器得到抓取位姿。</p>]]></content>
    
    
    <summary type="html">调研近三年顶会顶刊上的抓取姿态估计的论文</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="抓取姿态估计" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>CVPR2022高被引论文笔记</title>
    <link href="https://www.mahaofei.com/post/6ec34466.html"/>
    <id>https://www.mahaofei.com/post/6ec34466.html</id>
    <published>2023-04-23T02:45:17.000Z</published>
    <updated>2023-04-23T02:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、目标检测</h1><h2 id="1-1-视频目标检测">1.1 视频目标检测</h2><h3 id="Video-Swin-Transformer">Video Swin Transformer</h3><blockquote><p><strong>标题</strong>：视频 Swin Transformer<br><strong>作者团队</strong>：Microsoft Research Asia<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>现今大多数的视觉识别模型都是基于Transformer建立的，本文在此基础上进行调整，得到更好的速度和精度。</p><p><strong>（2）方法</strong></p><ol><li>总体架构</li></ol><p>视频定义为TxHxWx3，patch为2x4x4x3的块，每个patch有96个特征维度。该架构的主要组件是Video Swin Transformer模块，通过将标准的Transformer的Multihead self-attention(MSA)模块替换为基于3D Shift Window的MSA模块，来实现。</p><p><img src="https://img.mahaofei.com/img/202305071021361.png" alt="image.png"></p><ol start="2"><li>3D MSA模块</li></ol><p>由于视频有时间维度，全局自注意模块会导致巨大的计算和内存成本。MSA模块就比传统的全局自注意模块要高效。</p><p>更进一步，基于Swin Transformer的2D移位窗口扩展到3D，实现了跨窗口链接，保证了体系结构的表达能力。</p><p><img src="https://img.mahaofei.com/img/202305071033856.png" alt="image.png"></p><h1>二、图像分割</h1><h1>三、图像处理</h1><h2 id="3-1-图像合成">3.1 图像合成</h2><h3 id="High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models">High-Resolution Image Synthesis with Latent Diffusion Models</h3><blockquote><p><strong>标题</strong>：具有潜在扩散模型的高分辨率图像合成<br><strong>作者团队</strong>：海德堡大学；Runway ML<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>扩散模型已经在包括图像数据在内的很多数据上，实现了很好的数据合成效果。但这些模型由于直接操作像素，需要昂贵的GPU资源。</p><p>本文提出的潜在扩散模型，达到了降低复杂性和保留细节的平衡点。</p><p><strong>（2）方法</strong></p><p>主要方法是：使用自动编码模型，学习一个在感知上与图像空间等效的空间，压缩学习阶段和生成学习阶段来减少资源需求。</p><ol><li>感知压缩模型<br>利用了结合perceptual loss, patch-based, adversarial objective的自动编码器。</li><li>潜在扩散模型<br>扩散模型是概率模型，通过逐渐对正态分布变量去噪来学习数据分布。<br>通过由自动编码器得到的高效、低维的空间，与高维像素空间相比更适合生成模型。</li><li>调节机制<br>通过使用交叉注意力机制增强基础网络UNet，能够处理各种模态的输入。</li></ol><p><strong>（3）思考</strong></p><p>将需要高运算量的像素操作，通过自动编码转换为了低维空间的操作，节省了计算量。</p><h1>四、三维视觉</h1><h1>五、位姿估计</h1><h1>六、机器人</h1><h1>七、神经网络</h1><h2 id="7-1-神经网络结构设计">7.1 神经网络结构设计</h2><h3 id="A-ConvNet-for-the-2020s">A ConvNet for the 2020s</h3><blockquote><p><strong>标题</strong>：2020s的ConvNet<br><strong>作者团队</strong>：Facebook AI<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/facebookresearch/ConvNeXt">https://github.com/facebookresearch/ConvNeXt</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>20年以来，由于Vision Transformers的引入，它开始快速取代卷积神经网络。但只使用Transformers也有些问题，因此后来又出现了hierarchical Transformers，其中加入了几个卷积神经网络作为先验。但这些方法都可以归结为Transformers的优势。</p><p>本文想要探讨纯卷积神经网络所能实现的极限。</p><p><strong>（2）最佳方法</strong></p><ol><li><strong>训练技术</strong>：使用AdamW优化器、数据增强、随机擦除、正则化等方法可以显著提高训练模型的性能</li><li><strong>宏观设计</strong>：<ul><li>阶段比例：ResNet中各阶段的比例很大程度是经验获得的，SwinTransformer的比例是1:1:3:1，传统的ResNet比例是(3,4,6,3)，此处调整为(3,3,9,3)与SwinT相同，发现也提高了模型准确率</li><li>模块设计：标准的ResNet模块包括一个7x7步长2的卷积层，然后是一个最大池化层。此处模仿SwinT，设计为4x4步长为4的卷积层作为基础模块。</li></ul></li><li><strong>使用分组卷积技术</strong>，可以有效提高网络性能</li><li><strong>反向瓶颈</strong>：使MLP的隐藏维度比输入维度宽4倍，这在几个ConvNet中以及Transformer中设计思路相同。</li><li><strong>更大的卷积核</strong>：尽管堆叠小卷积核可以有效利用硬件，但测试证明，总体上大卷积核能够提高模型性能</li><li><strong>微观设计</strong>：<ul><li>更少的归一化层</li><li>使用层归一化LN代替批归一化BatchNorm</li><li>分离下采样层：ResNet中，下采样是通过每个阶段开始的残差块实现的，在层和层之间加入单独的下采样层发现可以提高准确率</li></ul></li></ol><p><strong>（3）总结</strong></p><ol><li>尽可能丰富数据，增大随机化程度：使用AdamW优化器、数据增强、随机擦除、正则化等方法</li><li>使用更优化的网络结构：调整各阶段卷积比例、使用反向瓶颈设计、更少的归一化层、更大的卷积核、在每个阶段之间加入下采样层。</li></ol>]]></content>
    
    
    <summary type="html">阅读CVPR的高被印论文，开拓视野。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="顶会顶刊" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E9%A1%B6%E4%BC%9A%E9%A1%B6%E5%88%8A/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>如何使用Git管理项目代码</title>
    <link href="https://www.mahaofei.com/post/dd16f220.html"/>
    <id>https://www.mahaofei.com/post/dd16f220.html</id>
    <published>2023-04-17T07:00:50.000Z</published>
    <updated>2023-04-17T07:00:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、注册Github并创建仓库</h1><p>这一步不细说了，需要科学上网，参考<a href="https://www.mahaofei.com/post/96c83ac9.html">这篇文章</a>，[[03_如何访问Google。。学术来查阅文献|Google学术访问方法]]。</p><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><h1>二、下载Git并配置</h1><h2 id="2-1-Git安装">2.1 Git安装</h2><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><p>在资源管理器内右键，选择<code>Git bash here</code>打开Git界面。</p><h2 id="2-2-Git配置">2.2 Git配置</h2><p>输入下面的代码，按下回车，生成ssh密钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;邮箱地址&quot;</span><br></pre></td></tr></table></figure><p>出现要求设置密码，可以不用设置，连续回车两次就可以。</p><p>打开<code>C:\Users\用户名\.ssh</code>，可以看到有一个<code>id_rsa.pub</code>文件，这就是刚才生成的密钥。</p><p>使用记事本打开此文件，复制里面的密钥内容。</p><h2 id="2-3-Github添加ssh-key">2.3 Github添加ssh key</h2><p>进入<a href="https://github.com/">Github官网</a>，点击右上角【setting --&gt; SSH and GPG keys --&gt; New SSH key】，在这里添加密钥，其中</p><ul><li>Title：自己写一个ssh key的名字，用于区分多个ssh key</li><li>Key：刚刚复制的密钥<br>填写完成后点击Add SSH key添加。</li></ul><p>然后在git bash中输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>如果连接成功，会让你输入<code>yes/no</code>，输入yes即可。</p><h2 id="2-4-配置用户名和邮箱">2.4 配置用户名和邮箱</h2><p>输入下面的代码配置自己的用户名和邮箱，两个信息都要和Github账号的信息一致</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;用户名&quot;</span><br><span class="line">git config --global user.email &quot;邮箱&quot;</span><br></pre></td></tr></table></figure><h1>三、代码管理</h1><h2 id="3-1-代码上传">3.1 代码上传</h2><p><strong>（1）初始化</strong></p><p>创建一个文件夹，在这个文件夹内，右键<code>git bash here</code>，然后输入<code>git init</code>完成初始化。</p><p>可以看到目录中出现了一个<code>.git</code>隐藏文件夹，这说明已经完成了初始化。</p><p><strong>（2）链接远程仓库</strong></p><p>在刚刚的<code>git bash</code>窗口，输入下面的命令同步到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p>如果出现fatal: remote origin already exists.可按以下步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br><span class="line">git pull git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p><strong>（3）上传本地文件</strong></p><p>添加本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add.</span><br></pre></td></tr></table></figure><p>提交本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;说明信息，一般说明本次提交更新了什么&quot;</span><br></pre></td></tr></table></figure><p>推送到远端仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push git@github.com:用户名/仓库名.git</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><h2 id="3-2-拉取代码">3.2 拉取代码</h2><p>从项目中拉取代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><p>如果出现<code>fatal: refusing to merge unrelated histories</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br></pre></td></tr></table></figure><h2 id="3-3-分支管理">3.3 分支管理</h2><p><strong>（1）查看分支</strong></p><p>在命令行窗口的光标处，输入git branch命令，查看 Git 仓库的分支情况。分支前有*表示是当前所在的分支。</p><p><strong>（2）创建分支</strong></p><p>使用下面的命令创建一个名为a的分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch a</span><br></pre></td></tr></table></figure><p><strong>（3）分支切换</strong></p><p>在命令行窗口的光标处，输入git checkout a命令，切换到a分支。</p><p><strong>（4）合并分支</strong></p><p>切换到master分支，然后输入git merge a命令，将a分支合并到master分支。</p><p><strong>（5）删除分支</strong></p><p>在命令行窗口的光标处，输入git branch -d a命令，删除a分支。</p><p><strong>（6）为分支添加标签</strong></p><p>在命令行窗口的光标处，输入git tag test_tag命令，为当前分支添加标签test_tag</p><h2 id="3-4-修改分支名称">3.4 修改分支名称</h2><p>假设分支名称为oldName，想要修改为 newName</p><ol><li>本地分支重命名(还没有推送到远程)</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><ol start="2"><li>远程分支重命名 (已经推送远程-假设本地分支和远程对应分支名称相同)</li></ol><p>重命名远程分支对应的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><p>到github修改默认分支的分支名。</p><p>上传新命名的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin newName</span><br></pre></td></tr></table></figure><p>把修改后的本地分支与远程分支关联</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --set-upstream-to origin/newName</span><br></pre></td></tr></table></figure><p>注意：如果本地分支已经关联了远程分支，需要先解除原先的关联关系：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --unset-upstream </span><br></pre></td></tr></table></figure><h2 id="3-5-报错信息">3.5 报错信息</h2><p><strong>（1）error: src refspec master does not match any. error: failed to push some refs to</strong></p><p>仔细检查push的是<code>master</code>分支还是<code>main</code>分支。</p>]]></content>
    
    
    <summary type="html">当有多台设备，或者同一个项目有多个版本的代码时，利用git管理项目代码就十分必要了。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/tags/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    <category term="Git" scheme="https://www.mahaofei.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>【抓取算法】Contact GraspNet</title>
    <link href="https://www.mahaofei.com/post/c18d351e.html"/>
    <id>https://www.mahaofei.com/post/c18d351e.html</id>
    <published>2023-04-07T07:52:27.000Z</published>
    <updated>2023-04-07T07:52:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><blockquote><p><strong>标题</strong>：Contact-GraspNet: 在杂乱场景中高效生成6-DoF抓取<br><strong>期刊会议</strong>：ICRA2021<br><strong>作者团队</strong>：Martin Sundermeyer（NVIDIA）<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a><br><strong>数据集</strong>：</p></blockquote><h2 id="1-1-目标问题-2">1.1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="1-2-方法">1.2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/20230404152359.png" alt="image.png"></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/20230404153946.png" alt="image.png"></p><h2 id="1-3-思考">1.3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1>二、算法复现</h1><h2 id="2-1-准备工作-2">2.1 准备工作</h2><p><strong>（1）环境搭建</strong></p><p>下载代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/NVlabs/contact_graspnet.git</span><br></pre></td></tr></table></figure><p>创建虚拟环境<br>（这个环境是没问题的，如果出现依赖不满足要求的情况，可以先删掉那项，创建完环境后再手动安装）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f contact_graspnet_env.yml</span><br></pre></td></tr></table></figure><p>重新编译<code>pointnet_tfops</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh compile_pointnet_tfops.sh</span><br></pre></td></tr></table></figure><p><strong>（2）模型和数据准备</strong></p><p>从作者给出的连接下载<a href="https://drive.google.com/drive/folders/1tBHKf60K8DLM5arm-Chyf7jxkzOr5zGl?usp=sharing">trained models</a>，并将它们放到<code>./checkpoints/</code>，下载<a href="https://drive.google.com/drive/folders/1v0_QMTUIEOcu09Int5V6N2Nuq7UCtuAA?usp=sharing">test data</a>，并将它们放到<code>./test_data</code></p><h2 id="2-2-预测抓取">2.2 预测抓取</h2><p>给定一个深度图(.npy文件/单位m)，相机内参，2D实例分割图，运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/inference.py \</span><br><span class="line">       --np_path=test_data/0.npy \</span><br><span class="line">       --local_regions --filter_grasps</span><br></pre></td></tr></table></figure><p><code>--np_path</code>：输入的.npz/.npy文件，带有深度、内参、实力分割图、RGB信息<br><code>--ckpt_dir</code>：checkpoint目录，默认为<code>checkpoint/scene_test_2048_bs3_hor_sigma_001</code>，非常干净的深度数据使用<code>scene_2048_bs3_rad2_32</code>，非常混乱的深度数据使用<code>scene_test_2048_bs3_hor_sigma_0025</code><br><code>--local_regions</code>：裁剪的3D实例分割<br><code>--filter_grasps</code>：筛选抓取点，使他们只为于对象的表面<br><code>--skip_border_objects</code>：忽略碰到深度图边缘的实例分割<br><code>--forward_passes</code>：前向计算的次数，增加可以提高抓取的采样点<br><code>--z_range</code>：[min, max]的z值来裁剪输入点云<br><code>--arg_configs TEST.second_thres:0.19 TEST.first_thres:0.23</code>：覆盖抓取的配置置信度来获得更多或更少的抓取候选</p><h2 id="2-3-训练网络">2.3 训练网络</h2><p><strong>（1）下载数据集</strong></p><ul><li>下载<a href="https://drive.google.com/file/d/1zcPARTCQx2oeiKk7a-wdN_CN-RUVX56c/view?usp=sharing">Acronym</a>数据集</li><li>从<a href="https://www.shapenet.org/">https://www.shapenet.org/</a>下载ShapeNet meshe</li><li>创建watertiget<ul><li>下载并构建<a href="https://github.com/hjwdzh/Manifold">https://github.com/hjwdzh/Manifold</a></li><li>创建watertight mesh，假设物体路径为model.obj：<code>manifold model.obj temp.watertight.obj -s</code></li><li>简化：<code>simplify -i temp.watertight.obj -o model.obj -m -r 0.02</code></li></ul></li></ul><p>下载10000个带有Contact抓取信息的桌面训练场景<a href="https://drive.google.com/drive/folders/1eeEXAISPaStZyjMX8BHR08cdQY4HF4s0">Google Drive</a>，解压为下面的格式</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">acronym</span><br><span class="line">├── grasps</span><br><span class="line">├── meshes</span><br><span class="line">├── scene_contacts</span><br><span class="line">└── splits</span><br></pre></td></tr></table></figure><p><strong>（2）训练Contact-GraspNet</strong></p><p>如果在没有外设的服务器上训练，设置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYOPENGL_PLATFORM=&#x27;egl&#x27;</span><br></pre></td></tr></table></figure><p>使用配置文件<code>contact_graspnet/config.yaml</code>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/train.py --ckpt_dir checkpoints/your_model_name \</span><br><span class="line">                                 --data_path /path/to/acronym/data</span><br></pre></td></tr></table></figure><p><strong>（3）生成自己的Contact Grasps和场景</strong></p><p>所下载的<code>scene_contacts</code>是从Acronym数据集生成的，要生成自己的数据集，下载安装<a href="https://github.com/NVlabs/acronym">acronym_tools</a>。</p><p>第一步，对象的6D抓取被映射到保存在<code>mesh_contacts</code>的接触点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_contact_infos.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>根据生成的<code>mesh_contacts</code>，可以创建桌面场景保存到<code>scene_contacts</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>一个线程大约花费3天，可以多次运行命令在多个核上并行处理。</p><p>可视化显示创建的桌面场景和抓取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym \</span><br><span class="line">       --load_existing scene_contacts/000000.npz -vis</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">复现NVIDIA提出的抓取估计算法Contact GraspNet</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>使用内网穿透SakuraFrp远程连接服务器</title>
    <link href="https://www.mahaofei.com/post/9ed2c32f.html"/>
    <id>https://www.mahaofei.com/post/9ed2c32f.html</id>
    <published>2023-04-05T05:59:35.000Z</published>
    <updated>2023-04-05T05:59:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Linux端配置</h1><p><strong>（1）ssh配置</strong></p><p>安装ssh服务器与客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt -y install openssh-server</span><br><span class="line">sudo apt -y install openssh-client</span><br></pre></td></tr></table></figure><p>配置ssh客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><ul><li>​将<code>PermitRootLogin prohibt-password</code> 修改为 <code>PermitRootLogin yes</code></li><li>将<code>PasswordAuthentication yes</code> 前的#删除，取消注释</li></ul><p>重启ssh服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh restart</span><br></pre></td></tr></table></figure><p>查看ssh服务运行状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh status</span><br></pre></td></tr></table></figure><p><strong>（2）Sakura配置</strong></p><p><a href="https://www.natfrp.com/user/">SakuraFrp</a></p><p>进入隧道列表新建隧道</p><ul><li>尽量选择国内节点</li><li>隧道类型为TCP隧道</li><li>本机端口为SSH</li><li>主机ip默认127.0.0.1即可(代指内网穿透本机)</li></ul><p><img src="https://img.mahaofei.com/img/20230405140317.png" alt=""></p><p>在官网下载对应版本的frpc，复制下载链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget -O frpc &lt;下载地址&gt;</span><br><span class="line">chmod 755 frpc</span><br><span class="line">ls -ls frpc</span><br><span class="line">md5sum frpc</span><br><span class="line">frpc -v</span><br></pre></td></tr></table></figure><p>隧道配置文件中复制隧道密钥</p><p>Ubuntu中使用下面的命令开启隧道</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frpc -f &lt;复制的密钥&gt;</span><br></pre></td></tr></table></figure><h1>Windows端配置</h1><p>打开【设置-应用-添加功能】，添加OpenSSH 服务器和OpenSSH 客户端。</p><p>打开服务，找到 OpenSSH SSH Server 和 OpenSSH Authentication Agent -&gt; 启动服务并设为自动。</p><p>打开 power shell，使用以下命令检查安装和运行情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-Service sshd</span><br></pre></td></tr></table></figure><p>打开Sakura官网，打开隧道列表，点击要连接的隧道，点击一键认证，下载exe认证程序并运行。</p><p>然后使用<code>ssh -p &lt;端口号&gt; &lt;用户名&gt;@&lt;地址&gt;</code>进行远程连接</p><h1>VSCode远程ssh开发环境</h1><p>安装插件 <code>Remote - SSH</code></p>]]></content>
    
    
    <summary type="html">不想使用向日葵和todesk等工具远程连接桌面，而且个人电脑和服务器也不在一个局域网下，想要远程连接服务器，因此考虑使用内网穿透。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】Gen6D算法</title>
    <link href="https://www.mahaofei.com/post/76335f84.html"/>
    <id>https://www.mahaofei.com/post/76335f84.html</id>
    <published>2023-03-29T13:22:39.000Z</published>
    <updated>2023-03-29T13:22:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1>论文笔记</h1><h2 id="1-介绍">1. 介绍</h2><h3 id="1-1-目标问题">1.1 目标问题</h3><p>现有的位姿估计算法要么需要高质量的物体模型，要么需要提供额外的深度图或物体掩码图，这对于位姿估计的实际应用有很大的限制。本文提出的方法只需要一些物体的姿态图像，就能够在任意环境中预测物体位姿。</p><p>作者认为一个位姿估计器应该具有以下特点：</p><ul><li>通用性：可以应用于任意物体，而无需对对象或类别进行训练</li><li>无模型：用于一个未见过的物体时，只需要一些已知姿态的参考图像来定义物体参考坐标系即可</li><li>输入简单：仅输入RGB图像来估计位姿，而不需要深度图或物体掩码图</li></ul><p><strong>（1）如何设计视角选择器，从参考图像中找到与查询图像视角最接近的</strong></p><p>本文使用神经网络对查询图像和参考图像进行逐像素比较，产生相似性得分，并选择具有最高相似性得分的参考图像。并添加了全局归一化层和自注意层来共享不同参考图像之间的相似性信息，为选择最相似的参考图像提供了上下文信息。</p><p><strong>（2）实现没有模型的姿态优化</strong></p><p>本文提出了一种新的基于三维空间的姿态优化方法，给定一个查询图像和一个输入姿态，找到几个接近输入姿态的参考图像，将这些参考图像投影回3D空间中，构建特征空间，通过3D的CNN将构建的特征空间与查询图像的特征相匹配，来优化姿态。</p><h3 id="1-2-现有工作">1.2 现有工作</h3><p>现有位姿估计方法大都是基于特定实例的，不能推广到未见过的物体，通常都需要根据物体3D模型来渲染大量图像进行训练。有一些方法可以推广到类别级，也不需要对象的模型，但仍然无法预测没见过的类别的物体。</p><h2 id="2-实现方法">2. 实现方法</h2><p><strong>数据规范化</strong>：对于每个物体，通过对参考图像中的点进行三角测量等方法估计物体的大致大小，然后对物体坐标系进行归一化，使物体中心位于原点，大小为1，此时物体位于原点的单位球体内。</p><p>Gen6D包括一个物体检测器，一个视角选择器，一个姿态优化器。</p><p><img src="https://img.mahaofei.com/img/20230403203020.png" alt="image.png"></p><p>物体检测其首先利用查询图像和参考图像来检测物体所在区域。然后视角选择器将查询图像于参考图像相匹配，产生粗略的初始姿态。最后由姿态优化器进一步细化以得到精确的对象姿态。</p><h3 id="2-1-物体检测">2.1 物体检测</h3><p>将检测问题分解成两部分</p><ol><li>找到对象中心的2D投影点q</li><li>估计包围单位球体的正方形边界框。</li></ol><p><img src="https://img.mahaofei.com/img/20230403204751.png" alt="image.png"></p><p>物体中心的深度可以使用$d=2f/S_q$求得，其中2是单位球体的直径，f是虚拟焦距（将主点设为投影点q），$S_q$是边界框边长。这就是物体的初始平移。</p><blockquote><p>问题：这里将物体归一化之后求出的深度d还是真实深度吗？虚拟焦距又是如何确定的？</p></blockquote><p>检测器使用了VGG网络提取参考图像和查询图像的特征图，然后将所有参考图像的特征图作为卷积核与查询图像的特征图卷积，得到分数图。考虑尺度差异，设置再多个预定义尺度上进行卷积，最后得到热力图和比例图。选择热力图上的最大值位置作为对象中心2D投影，使用比例图上相同比例的比例作为边界框的大小$S_q=S_r*s$。</p><blockquote><p>问题：这里将所有参考图像的特征图都进行卷积，那么参考图像上物体特征和背景特征是如何区分的？</p></blockquote><h3 id="2-2-视角选择">2.2 视角选择</h3><p>将查询图像与每个参考图像比较，计算相似性得分。计算每个参考图像和查询图像的元素乘积，获得得分图，并计算相似性参数。</p><p><img src="https://img.mahaofei.com/img/20230404192903.png" alt="image.png"></p><p><strong>（1）平面内旋转</strong><br>为了考虑平面内旋转，本文将参考图像旋转Na个预定义角度，查询时使用所有旋转版本进行逐元素乘积。</p><p><strong>（2）全局归一化</strong><br>使用参考图像的所有特征图计算的均值和方差，对相似度网络生成的特征图进行归一化。这样做可以用特征图的分布来编码上下文相似性，并放大不同图像之间的相似性差异。</p><p><strong>（3）参考视角变换</strong><br>在所有参考图像的相似性特征向量上应用变换，包括它们的视角、注意力层。这样的变换器使得特征向量相互通信以编码上下文信息，有助于确定最相似的参考图像。</p><h3 id="2-3-姿态优化">2.3 姿态优化</h3><p>经过上面两个步骤，我们已经有了粗略的物体位姿。本步骤对位姿进行优化。</p><p>选择接近输入姿态的6个参考图像，通过CNN提取特征图，然后将特征图投影到3D空间中，并计算特征的均值和方差作为空间顶点的特征。<br>对于查询图像，使用同样的CNN提取特征图，将特征图投影到3D空间中，并将查询特征与参考图像特征的均值和方差连接起来。</p><p>最后在空间特征上使用3DCNN预测残差来更新输入姿态。</p><p><img src="https://img.mahaofei.com/img/20230404195340.png" alt="image.png"></p><h1>3. 实验分析</h1><h1>二、算法复现</h1><h2 id="2-1-环境搭建">2.1 环境搭建</h2><h3 id="2-1-1-Python环境">2.1.1 Python环境</h3><p>创建[[02_Anaconda的基本使用与在Pycharm中调用|Anaconda虚拟环境]]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gen6d python=3.7</span><br><span class="line">conda activate gen6d</span><br></pre></td></tr></table></figure><p>安装pytorch环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 -c pytorch</span><br></pre></td></tr></table></figure><p>安装依赖，打开<code>requirements.txt</code>，删除其中的pytorch, torchvision, cudatoolkit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="2-1-2-自制数据集工具">2.1.2 自制数据集工具</h3><p><strong>（1）COLMAP</strong></p><p>参考官网教程：<a href="https://colmap.github.io/install.html">https://colmap.github.io/install.html</a></p><p>安装依赖库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-regex-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev \</span><br><span class="line">    libcgal-qt5-dev</span><br></pre></td></tr></table></figure><p>下载COLMAP源代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/colmap/colmap</span><br><span class="line">cd colmap</span><br></pre></td></tr></table></figure><p>修改<code>CMakeLists.txt</code>文件，添加下面的内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_CUDA_ARCHITECTURES &quot;70&quot;)</span><br></pre></td></tr></table></figure><p>开始编译、安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -GNinja</span><br><span class="line">ninja</span><br><span class="line">sudo ninja install</span><br></pre></td></tr></table></figure><p><strong>（2）CloudCompare</strong></p><p>安装Flatpak</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install flatpak</span><br></pre></td></tr></table></figure><p>安装Software Flatpak plugin</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-software-plugin-flatpak</span><br></pre></td></tr></table></figure><p>添加Flathub repository</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo</span><br></pre></td></tr></table></figure><p>安装CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak install flathub org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p>运行CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><strong>（3）安装ffmpeg</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ffmpeg</span><br></pre></td></tr></table></figure><h2 id="2-2-数据集准备">2.2 数据集准备</h2><h3 id="2-2-1-官方数据集">2.2.1 官方数据集</h3><p><strong>（1）下载数据集</strong></p><p>从<a href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EkWESLayIVdEov4YlVrRShQBkOVTJwgK0bjF7chFg2GrBg?e=Y8UpXu">原作者给出的链接</a>中下载预训练模型，GenMOP数据集和processed LINEMOD数据集。</p><p><strong>（2）组织数据集</strong></p><p>将下载的文件按照下面的格式进行整理。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- model</span><br><span class="line">        |-- detector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- selector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- refiner_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br></pre></td></tr></table></figure><h3 id="2-2-2-自制数据集">2.2.2 自制数据集</h3><p><strong>（1）视频录制</strong></p><p>使用手机录制目标物体的参考视频和测试视频。注意：参考视频需要满足以下条件</p><ul><li>参考视频中对象是静态的</li><li>参考视频中背景尽可能纹理丰富且平整，摄像角度要尽可能覆盖每个角度，以便COLMAP恢复相机姿态</li></ul><p><strong>（2）组织文件</strong></p><p>将视频按照下面的路径进行组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- video</span><br><span class="line">           |-- mouse-ref.mp4</span><br><span class="line">           |-- mouse-test.mp4</span><br></pre></td></tr></table></figure><p><strong>（3）将参考视频拆分为图像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 每10帧保存一张图像，最大图像边长为960</span></span></span><br><span class="line">python prepare.py --action video2image \</span><br><span class="line">                  --input data/custom/video/ref/coffeebox-ref.mp4 \</span><br><span class="line">                  --output data/custom/coffeebox/images \</span><br><span class="line">                  --frame_inter 10 \</span><br><span class="line">                  --image_size 960 \</span><br><span class="line">                  --transpose</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 或者</span></span></span><br><span class="line">python prepare.py --action video2image --input data/custom/video/ammeter-ref.mp4 --output data/custom/ammeter/images --frame_inter 10 --image_size 960 --transpose</span><br></pre></td></tr></table></figure><p>拆分后的视频保存在<code>data/custom/mouse/images</code>中。</p><p><strong>（4）运行COLMAP SfM恢复相机姿态</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action sfm --database_name custom/ammeter --colmap &lt;path-to-your-colmap-exe&gt;</span><br></pre></td></tr></table></figure><p>注：<code>&lt;path-to-your-colmap-exe&gt;</code>可以通过命令<code>which colmap</code>来查找，一般ubuntu路径为<code>/usr/local/bin/colmap</code>，windows路径为<code>E:/Programming/COLMAP-3.8-windows-cuda/COLMAP.bat</code></p><p><strong>（5）手动处理点云</strong></p><p>通过裁减对象点云来手动确定对象所在区域。例如使用<a href="https://www.cloudcompare.org/">CloudCompare</a>来可视化处理COLMAP重建的点云，重建的点云位于<code>data/custom/mouse/colmap/pointcloud.ply</code>中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230327215520.png" alt=""></p><p>导出裁剪后的点云为<code>data/custom/mouse/object_point_cloud.ply</code>。</p><p><img src="https://img.mahaofei.com/img/20230327220042.png" alt=""></p><p><strong>（6）手动确定对象的X轴正方向和Y轴正方向</strong></p><p><img src="https://img.mahaofei.com/img/20230327220221.png" alt=""></p><p><img src="https://img.mahaofei.com/img/20230327220225.png" alt=""></p><p>编辑一个<code>data/custom/mouse/meta_info.txt</code>文件来保存你的X+和Z+信息，例如</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.297052 0.350839 -0.000593</span><br><span class="line">0.973488 0.054352 -0.222188</span><br></pre></td></tr></table></figure><p><strong>（7）确保您具有以下文件，这些文件由上述步骤生成</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- mouse</span><br><span class="line">           |-- object_point_cloud.ply  ## object point cloud</span><br><span class="line">           |-- meta_info.txt           ## meta information about z+/x+ directions</span><br><span class="line">           |-- images                  ## images</span><br><span class="line">           |-- colmap                  ## colmap project</span><br></pre></td></tr></table></figure><p><strong>（8）从处理后的参考图像中预测姿势</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python predict.py --cfg configs/gen6d_pretrain.yaml \</span><br><span class="line">                  --database custom/coffeebox_lied \</span><br><span class="line">                  --video data/custom/video/coffeebox-test.mp4 \</span><br><span class="line">                  --resolution 960 \</span><br><span class="line">                  --transpose \</span><br><span class="line">                  --output data/custom/ammeter_processed/test \</span><br><span class="line">                  --ffmpeg &lt;path-to-ffmpeg-exe&gt;</span><br></pre></td></tr></table></figure><h2 id="2-3-训练与评估">2.3 训练与评估</h2><p>将文件按照下面的形式组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br><span class="line">    |-- shapenet</span><br><span class="line">        |-- shapenet_cache</span><br><span class="line">        |-- shapenet_render</span><br><span class="line">        |-- shapenet_render_v1.pkl</span><br><span class="line">    |-- co3d_256_512</span><br><span class="line">        |-- apple</span><br><span class="line">            ...</span><br><span class="line">    |-- google_scanned_objects</span><br><span class="line">        |-- 06K3jXvzqIM</span><br><span class="line">            ...</span><br><span class="line">    |-- coco</span><br><span class="line">        |-- train2017</span><br></pre></td></tr></table></figure><h3 id="2-3-1-训练detector">2.3.1 训练detector</h3><p>修改<code>train_meta_info.py</code>的第86行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;genmop_train&#x27;</span>: [<span class="string">f&#x27;genmop/<span class="subst">&#123;name&#125;</span>-test&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">&#x27;ammeter&#x27;</span>, <span class="string">&#x27;coffeebox&#x27;</span>, <span class="string">&#x27;realsensebox&#x27;</span>]],</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第109行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">genmop_meta_info=&#123;</span><br><span class="line">    <span class="string">&#x27;ammeter&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0222805</span>, -<span class="number">0.409031</span>, <span class="number">0.912248</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.401556</span>, <span class="number">0.773825</span>, <span class="number">0.340199</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;coffeebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0718405</span>, -<span class="number">0.471545</span>, <span class="number">0.878911</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.582604</span>, -<span class="number">0.490501</span>, -<span class="number">0.219265</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;realsensebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.103463</span>, -<span class="number">0.521284</span>, <span class="number">0.847088</span>],np.float32), <span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">1.690831</span>, <span class="number">0.688506</span>, <span class="number">0.590004</span>],np.float32)&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第212行，修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras, images, points3d = read_model(<span class="string">f&#x27;<span class="subst">&#123;GenMOP_ROOT&#125;</span>/<span class="subst">&#123;seq_name&#125;</span>/colmap/sparse/0&#x27;</span>)</span><br></pre></td></tr></table></figure><p>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/detector/detector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-2-训练selector">2.3.2 训练selector</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/selector/selector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-3-训练refiner">2.3.3 训练refiner</h3><p>为refiner训练进行数据准备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database linemod/cat \</span><br><span class="line">                  --que_split linemod_val \</span><br><span class="line">                  --ref_database linemod/cat \</span><br><span class="line">                  --ref_split linemod_val</span><br><span class="line"></span><br><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database genmop/tformer-test \</span><br><span class="line">                  --que_split all \</span><br><span class="line">                  --ref_database genmop/tformer-ref \</span><br><span class="line">                  --ref_split all </span><br></pre></td></tr></table></figure><p>该命令会在<code>data/val</code>生成信息，该信息会被用于生成refiner的有效数据</p><p>训练refiner</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/refiner/refiner_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-4-评估所有组件">2.3.4 评估所有组件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object TFormer from the GenMOP dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name genmop/tformer</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object <span class="built_in">cat</span> from the LINEMOD dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name linemod/cat</span><br></pre></td></tr></table></figure><h1>三、现存问题</h1><p><strong>优点</strong></p><ol><li>只需要对给定物体录制1-2分钟的视频，使用程序1-2小时<strong>添加数据集</strong>，即可实现新物体的位姿估计，不需要再训练网络</li><li><strong>精度</strong>还可以</li></ol><p><strong>缺点</strong></p><ol><li>对于<strong>方形凸形物体识别较好，对于物体内部存在中空区域</strong>，例如圆环等物体识别效果较差</li><li>由于<strong>参考视频要求物体静止，因此无法录到物体底面的特征</strong>，对于物体底面识别效果较差（可考虑物体正反放置录制两次，对于同一个物体使用两个参考视频进行预测，选择置信度高的位姿）</li><li>当进行识别时，如果<strong>图像中不存在物体也会生成一个估计位姿</strong>（可以考虑根据置信度判断输出，或者在位姿估计前使用yolo等算法预判断物体位置）</li><li>当存在<strong>遮挡时位姿估计效果较差</strong>，可能会出现只框处未被遮挡的部分，或者在遮挡物体上强行进行位姿估计。</li><li>当要同时识别的物体很多时，对于显卡显存要求比较大，而且计算会很慢，服务器1.5s/it。如果每次只对某个特定物体进行识别，速度还可以。</li></ol>]]></content>
    
    
    <summary type="html">算法复现</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="Gen6D" scheme="https://www.mahaofei.com/tags/Gen6D/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】GDRNPP算法</title>
    <link href="https://www.mahaofei.com/post/250dc866.html"/>
    <id>https://www.mahaofei.com/post/250dc866.html</id>
    <published>2023-03-27T01:23:53.000Z</published>
    <updated>2023-04-01T01:23:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</h1><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Gu Wang,  Tsinghua University, BNRist<br><strong>关键词</strong>：位姿估计；端到端<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/THU-DA-6D-Pose-Group/GDR-Net">https://github.com/THU-DA-6D-Pose-Group/GDR-Net</a></p></blockquote><h2 id="1-目标问题-21">1 目标问题</h2><p>提出一种端到端的位姿估计算法。</p><h2 id="2-方法-21">2 方法</h2><p><img src="https://img.mahaofei.com/img/20230316104020.png" alt=""></p><p><strong>（1）网络架构</strong></p><p>首先向GDR-Net提供256x256的ROI图，预测出三个64x64的中间特征图</p><ul><li>稠密对应图$M_{2D-3D}$：将稠密坐标映射$M_{XYZ}$对跌倒2D像素坐标上得到，反映了对象的几何形状信息。</li><li>表面区域注意图$M_{SRA}$：采用最远点采样从$M_{XYZ}$中到处，代表了对象的对称性。</li><li>可见对象掩码$M_{vis}$</li></ul><p>使用一个简单的2D卷积Patch Pnp模块直接从特征图中回归6D对象姿态。Patch PnP模块由三个卷积层组成，然后用两个全连接层用于扁平化特征，最后连个全连接层输出R6D旋转和tSITE平移。</p><h2 id="3-思考-21">3 思考</h2><p>本文专注于图像的特征提取和处理工作，实现从单一图片预测位姿的功能。</p><h1>二、算法复现</h1><h2 id="2-1-数据集准备">2.1 数据集准备</h2><p>下载<a href="https://bop.felk.cvut.cz/datasets/">BOP数据集</a>和<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">VOC2012数据集</a>，从<a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EgOQzGZn9A5DlaQhgpTtHBwB2Bwyx8qmvLauiHFcJbnGSw?e=EZ60La">Onedrive (password: groupji)</a>或者<a href="https://pan.baidu.com/s/1FzTO4Emfu-DxYkNG40EDKw">百度网盘(密码: vp58)</a>下载test_boxes，完成后datasets文件夹的结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">datasets/</span><br><span class="line">├── BOP_DATASETS   # https://bop.felk.cvut.cz/datasets/</span><br><span class="line">    ├──tudl</span><br><span class="line">    ├──lmo</span><br><span class="line">    ├──ycbv</span><br><span class="line">    ├──icbin</span><br><span class="line">    ├──hb</span><br><span class="line">    ├──itodd</span><br><span class="line">    └──tless</span><br><span class="line">└──VOCdevkit</span><br></pre></td></tr></table></figure><p>从<a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EgOQzGZn9A5DlaQhgpTtHBwB2Bwyx8qmvLauiHFcJbnGSw?e=EZ60La">Onedrive (password: groupji)</a>或者<a href="https://pan.baidu.com/s/1LhXblEic6pYf1i6hOm6Otw">百度网盘(密码10t3)</a>下载预训练模型，并将其放到<code>./output</code>文件夹内。</p><h2 id="2-2-环境准备">2.2 环境准备</h2><p>要求Ubuntu 18.04/20.04, CUDA 10.1/10.2/11.6, python &gt;= 3.7, PyTorch &gt;= 1.9, torchvision</p><p><strong>（1）创建虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gdrnpp python=3.7</span><br><span class="line">conda activate grdnpp</span><br></pre></td></tr></table></figure><p><strong>（2）安装依赖</strong></p><p>打开<code>requirements/requirement.txt</code>，修改第48行为<code>pytorch-lightning==1.6.0</code></p><p>运行<code>sh scripts/install_deps.sh</code></p><p><strong>（3）从<a href="https://github.com/facebookresearch/detectron2">源码</a>安装detectron2</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install &#x27;git+https://github.com/facebookresearch/detectron2.git&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">(add --user <span class="keyword">if</span> you don<span class="string">&#x27;t have permission)</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Or, to install it from a local clone:</span></span></span><br><span class="line">git clone https://github.com/facebookresearch/detectron2.git</span><br><span class="line">python -m pip install -e detectron2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">On macOS, you may need to prepend the above commands with a few environment variables:</span></span></span><br><span class="line">CC=clang CXX=clang++ ARCHFLAGS=&quot;-arch x86_64&quot; python -m pip install ...</span><br></pre></td></tr></table></figure><p><strong>（4）编译 fps 的cpp扩展</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh core/csrc/compile.sh</span><br></pre></td></tr></table></figure><p><strong>（5）编译egl_renderer的cpp扩展</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh lib/egl_renderer/compile_cpp_egl_renderer.sh</span><br></pre></td></tr></table></figure><h2 id="2-3-目标检测算法">2.3 目标检测算法</h2><p>从 <a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EkCTrRfHUZVEtD7eHwLkYSkBCTXlh9ekDteSzK6jM4oo-A?e=m0aNCy">Onedrive</a> (password: groupji) or <a href="https://pan.baidu.com/s/1AU7DGCmZWsH9VgQnbTRjow">BaiDuYunPan</a>(password: aw68)中下载预训练模型。</p><p><strong>（1）训练</strong></p><p><code>./det/yolox/tools/train_yolox.sh &lt;config_path&gt; &lt;gpu_ids&gt; (other args)</code></p><p><strong>（2）测试</strong></p><p><code>./det/yolox/tools/test_yolox.sh &lt;config_path&gt; &lt;gpu_ids&gt; &lt;ckpt_path&gt; (other args)</code></p><h2 id="2-4-位姿估计算法">2.4 位姿估计算法</h2><p><strong>（1）训练</strong></p><p>打开<code>core/gdrn_modeling/datasets/lm_pbr.py</code>，注释第190行<code>assert osp.exists(xyz_path), xyz_path</code></p><p><code>./core/gdrn_modeling/train_gdrn.sh &lt;config_path&gt; &lt;gpu_ids&gt; (other args)</code></p><p>例如：</p><p><code>./core/gdrn_modeling/train_gdrn.sh configs/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv.py 0</code></p><p><strong>（2）测试</strong></p><p><code>./core/gdrn_modeling/test_gdrn.sh &lt;config_path&gt; &lt;gpu_ids&gt; &lt;ckpt_path&gt; (other args)</code></p><p>例如：</p><p><code>./core/gdrn_modeling/test_gdrn.sh configs/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv.py 0 output/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv/model_final_wo_optim.pth</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;一、GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;期刊 / 会议&lt;/stron</summary>
      
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="GDRNPP" scheme="https://www.mahaofei.com/tags/GDRNPP/"/>
    
  </entry>
  
  <entry>
    <title>TensorBoard的使用丨深度学习曲线生成</title>
    <link href="https://www.mahaofei.com/post/6db9da8f.html"/>
    <id>https://www.mahaofei.com/post/6db9da8f.html</id>
    <published>2023-03-23T14:14:20.000Z</published>
    <updated>2023-03-23T14:14:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1>TensorBoard的安装</h1><p>要求Pytorch版本必须在1.2.0以上。</p><p>使用下面的命令安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br></pre></td></tr></table></figure><h1>TensorBoard的代码调用</h1><p><strong>（1）导入包，并创建TensorBoard回调对象</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> TensorBoard</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs/learning_rate_scheduler&quot;</span>) <span class="comment">#指定TensorBoard日志目录</span></span><br></pre></td></tr></table></figure><p><strong>（2）在模型的训练过程中导入回调</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">global_step = <span class="number">0</span> <span class="comment"># 初始化 global_step 为 0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># 训练过程</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将学习率和训练损失添加到 TensorBoard</span></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Loss&#x27;</span>, loss, global_step=global_step)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Learning_Rate&#x27;</span>, lr, global_step=global_step)</span><br><span class="line">global_step += <span class="number">1</span>  <span class="comment"># 为每个batch更新 global_step 计数器</span></span><br></pre></td></tr></table></figure><h1>查看曲线</h1><p>训练开始后，打开一个终端，输入下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir logs/learning_rate_scheduler</span><br></pre></td></tr></table></figure><p>然后打开浏览器的<a href="http://localhost:6006/">http://localhost:6006/</a>页面，就可以看到曲线。</p><p><img src="https://img.mahaofei.com/img/20230323221410.png" alt=""></p>]]></content>
    
    
    <summary type="html">在深度学习训练过程中，我们必定会需要观察系统的Loss、Learning_rate等参数的变化，因此实时绘制曲线图是十分有必要的。本文就介绍了如何利用Pytorch的TensorBoard绘制曲线图。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="深度学习基础" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】ECCV2020-2022 6D位姿估计相关论文</title>
    <link href="https://www.mahaofei.com/post/f0f72f50.html"/>
    <id>https://www.mahaofei.com/post/f0f72f50.html</id>
    <published>2023-03-21T11:20:13.000Z</published>
    <updated>2023-03-21T11:20:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1>ECCV2020</h1><h2 id="01-CosyPose-Consistent-multi-view-multi-object-6D-pose-estimation">01. CosyPose: Consistent multi-view multi-object 6D pose estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2020<br><strong>作者 / 机构</strong>：Yann Labbe,  Ecole normale superieure, CNRS, PSL Research University, Paris, France<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ylabbe/cosypose">https://github.com/ylabbe/cosypose</a></p></blockquote><h4 id="1-目标问题-16">1 目标问题</h4><p>在相机位置未知的情况下，利用多视角信息来提高物体姿态估计的准确性和鲁棒性</p><h4 id="2-方法-16">2 方法</h4><p><strong>（1）建立6D姿态初始候选对象</strong></p><p>给定一组具有已知3D模型的对象和场景的单个图像，我们为每个对象输出一组候选检测，并为每个检测输出对象相对于与图像相关联的相机的6D姿态。</p><p><strong>（2）对象候选匹配</strong></p><p>匹配多个视图中可见的对象，以获得单个一致的场景。</p><p><strong>（3）全局场景细化</strong></p><p>所有物体和相机的6D姿态都经过了优化，以最大限度地减少全局重投影误差。</p><h4 id="3-思考-16">3 思考</h4><p><strong>（1）创新点</strong></p><ul><li>提出了一种基于渲染和比较的单视角单物体6D姿态估计方法，用于生成每个图像中的物体姿态假设。</li><li>开发了一种基于RANSAC的鲁棒方法，用于匹配不同图像中的单个物体姿态假设，并利用这些对象级别的对应关系来恢复相机之间的相对位置。</li><li>开发了一种基于对象级别捆绑调整（object-level bundle adjustment）的全局优化方法，用于在所有视角下最小化重投影误差，并改善噪声单视角物体姿态。</li></ul><p><strong>（2）实用性</strong></p><p>从多个视图中推测物体的6D位姿，对于抓取场景实用性较差。</p><h1>ECCV2022</h1><h2 id="01-DProST-Dynamic-Projective-Spatial-Transformer-Network-for-6D-Pose-Estimation">01. DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：<br><strong>关键词</strong>：<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/parkjaewoo0611/DProST">https://github.com/parkjaewoo0611/DProST</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-17">1 目标问题</h4><p>提出了一种新的基于投影网格的姿态估计系统。</p><p><img src="https://img.mahaofei.com/img/20230402093533.png" alt=""></p><h4 id="2-方法-17">2 方法</h4><ul><li>使用深度神经网络从RGB图像中提取特征，并预测物体位置和大小。</li><li>在相机空间上根据预测位置和大小生成一个锥形光束网格，并将其反向变换到物体空间。</li><li>使用参考图像和掩码从物体模型或重建特征中提取纹理特征，并将其映射到变换后的网格上。</li><li>使用双线性插值从映射后的纹理特征中采样得到重建图像，并与输入图像进行比较。</li><li>使用基于网格距离和网格匹配损失函数来优化网络参数和姿态参数。</li></ul><h4 id="3-思考-17">3 思考</h4><p>深度神经网络提取特征，投影网格重建图像，使用损失函数优化参数。</p><h2 id="02-DCL-Net-Deep-Correspondence-Learning-Network-for6D-Pose-Estimation">02. DCL-Net: Deep Correspondence Learning Network for6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Hongyang Li, South China University of Technology, Guangzhou, China<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/Gorilla-Lab-SCUT/DCL-Net">https://github.com/Gorilla-Lab-SCUT/DCL-Net</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-18">1 目标问题</h4><p>从点对应关系中直接估计6D物体姿态，而不是使用间接的对应学习目标</p><h4 id="2-方法-18">2 方法</h4><p>这篇论文的主要方法是提出了一种新的深度对应学习网络（DCL-Net），它利用双重特征解耦和对齐（FDA）模块，在特征空间中建立相机坐标系和物体坐标系之间的部分到部分对应关系和完整到完整对应关系。具体步骤如下：</p><ul><li>首先，对于部分物体观测和其CAD模型，分别提取它们的点特征图；</li><li>然后，设计两个FDA模块，分别建立部分到部分对应关系和完整到完整对应关系。具体来说，每个FDA模块将两个点特征图作为输入，并将每个特征图解耦为独立的姿态特征图和匹配特征图；然后利用匹配特征图学习一个注意力图来建立深度对应关系；最后，根据注意力图将两个系统的姿态特征图和匹配特征图进行对齐和配对，得到姿态特征对和匹配特征对；</li><li>接着，将两个FDA模块得到的两组对应关系进行融合，因为它们具有互补优势；然后利用融合后的匹配特征对学习置信度得分来衡量深度对应关系的质量；同时利用置信度得分加权融合后的姿态特征对来直接回归物体姿态；</li><li>最后，提出了一个基于置信度的姿态优化网络来进一步迭代地提高姿态精度。</li></ul><h4 id="3-思考-18">3 思考</h4><p>点特征方法。</p><h2 id="03-Perspective-Flow-Aggregation-for-Data-Limited-6D-Object-Pose-Estimation">03. Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Yinlin Hu, EPFL CVLab, Lausanne, Switzerland<br><strong>关键词</strong>：位姿估计；少数据情况<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/cvlab-epfl/perspective-flow-aggregation">https://github.com/cvlab-epfl/perspective-flow-aggregation</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-19">1 目标问题</h4><p>在数据有限的情况下，使用合成图像或少量真实图像来训练一个6D物体姿态估计的模型</p><h4 id="2-方法-19">2 方法</h4><ul><li>首先，利用合成图像和真实图像（如果有的话）来训练一个基于深度学习的特征提取器，用于从输入图像中提取出与物体姿态相关的特征。</li><li>然后，利用合成图像和真实图像（如果有的话）来训练一个基于透视流（perspective flow）的模块，用于将输入图像中的特征点映射到目标物体模型上。透视流是指由于相机和物体之间相对运动而导致的特征点在不同视角下的位移。</li><li>最后，利用一种称为透视流聚合（perspective flow aggregation）的技术，将多个透视流进行融合，并通过最小二乘法求解出最优的6D物体姿态。</li></ul><h4 id="3-思考-19">3 思考</h4><p>投影透视方法。</p><h2 id="04-Learning-Based-Point-Cloud-Registration-for-6D-Object-Pose-Estimation-in-the-Real-World">04. Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Zheng Dang, CVLab, EPFL, Lausanne, Switzerland<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/AnsonYanxin/MatchNorm">https://github.com/AnsonYanxin/MatchNorm</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-20">1 目标问题</h4><h4 id="2-方法-20">2 方法</h4><ul><li>首先，它提出了一种基于深度学习的点云匹配模块，用于从源点云和目标点云中提取特征，并计算两个点云之间的相似度矩阵。</li><li>然后，它提出了一种基于归一化的点云对齐模块，用于根据相似度矩阵找到最佳的刚性变换矩阵，使得源点云和目标点云之间的距离最小化</li></ul><h4 id="3-思考-20">3 思考</h4><p>代码不完全。</p>]]></content>
    
    
    <summary type="html">检索阅读近三年ECCV6D位姿估计相关论文并进行记录</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="ECCV" scheme="https://www.mahaofei.com/tags/ECCV/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】CVPR2020-2022 6D位姿估计相关论文</title>
    <link href="https://www.mahaofei.com/post/afed67af.html"/>
    <id>https://www.mahaofei.com/post/afed67af.html</id>
    <published>2023-03-15T01:29:33.000Z</published>
    <updated>2023-03-15T01:29:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1>CVPR 2020</h1><h2 id="01-HybridPose-6D-Object-Pose-Estimation-under-Hybrid-Representations">01. HybridPose: 6D Object Pose Estimation under Hybrid Representations</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Chen Song, The University of Texas at Austin<br><strong>关键词</strong>：位姿估计；混合特征<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/chensong1995/HybridPose">https://github.com/chensong1995/HybridPose</a></p></blockquote><h3 id="1-目标问题-3">1 目标问题</h3><p>6D位姿估计</p><h3 id="2-方法-3">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230315170643.png" alt=""></p><p>算法由中间特征预测网络和姿态回归网络组成：</p><p><strong>（1）预测模块</strong></p><p>将图像作为输入，用三个预测网络输出预测的关键点、边缘向量和对称对应关系</p><ul><li>关键点：利用PVNet的关键点预测方法</li><li>边缘向量：每两个关键点之间的向量</li><li>对称对应关系：扩展了FlowNet网络，结合了像素流和语义掩码</li></ul><p><strong>（2）姿态回归模块</strong></p><p>姿态回归网络：包括初始化子模块和优化子模块</p><ul><li>初始化子模块：使用中间特征回归初始姿态</li><li>优化子模块：使用GM鲁棒范数并优化，获得最终姿态</li></ul><h3 id="3-思考-3">3 思考</h3><p>方法比较直观，使用关键点、关键点向量和对称关系进行姿态预测。</p><p>但是实际应用比较困难，训练前需要使用FSP生成关键点标签、使用SymSeg生成对称性标签，并且还要提供分割模板。而且还需要PVNet的融合数据。</p><hr><h2 id="02-Single-Stage-6D-Object-Pose-Estimation">02. Single-Stage 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Yinlin Hu, CVLab, EPFL, Switzerland<br><strong>关键词</strong>：位姿估计；单阶段<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/cvlab-epfl/single-stage-pose">https://github.com/cvlab-epfl/single-stage-pose</a></p></blockquote><h3 id="1-目标问题-4">1 目标问题</h3><p>提出一种单阶段框架，解决两阶段框架（先建立3D对象关键点和2D图像的对应关系然后回归）的缺点，加快训练速度。</p><h3 id="2-方法-4">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316095154.png" alt=""></p><p>通过一些实例分割网络建立了3D物体和2D图像的关系后，使用三个主要模块来直接从这些对应聚类预测位姿：</p><ul><li><p>局部特征提取模块</p></li><li><p>特征聚合模块：在不同聚类中聚合特征</p></li><li><p>全局推理模块：有全连接层组成，用于将最终姿态估计为四元数和平移</p></li><li><p>提出了一种新颖的<strong>投影分布</strong>（Projection Distribution）表示法，将三维物体关键点在二维图像上的投影建模为一个概率分布，而不是一个确定的位置。</p></li><li><p>设计了一个<strong>单阶段6D姿态估计网络</strong>（Single-Stage 6D Pose Estimation Network），利用卷积神经网络和全连接层来预测每个物体关键点在图像上的投影分布参数。</p></li><li><p>采用了一种<strong>最大似然估计</strong>（Maximum Likelihood Estimation）方法，根据预测的投影分布参数和已知的三维物体模型来直接计算物体在相机坐标系下的旋转矩阵和平移向量。</p></li></ul><h3 id="3-思考-4">3 思考</h3><p>似乎需要与其它网络结合，从其他网络的中间层进行特征提取。</p><p>Github资料较少。</p><hr><h2 id="03-G2L-Net-Global-to-Local-Network-for-Real-time-6D-Pose-Estimation-with-Embedding-Vector-Features">03. G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Wei Chen, School of Computer Science, University of Birmingham<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/DC1991/G2L_Net">https://github.com/DC1991/G2L_Net</a></p></blockquote><h3 id="1-目标问题-5">1 目标问题</h3><p>提高位姿估计算法的准确度和速度。</p><h3 id="2-方法-5">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316101524.png" alt=""></p><p><strong>（1）全局定位</strong></p><p>使用2D检测器（例如yolo）来预测目标的边界框和标签，并将得到的概率图中的最大概率位置作为球体中心（结合深度图的3D坐标），来获得一个球体空间，减少后续3D搜索空间。</p><p><strong>（2）平移定位</strong></p><p>进行3D分割和平移残差预测，并将对象点的坐标系转换为局部规范坐标系。</p><p><strong>（3）旋转定位</strong></p><p>使用逐点嵌入向量特征提取器来提取嵌入向量特征，然后输入解码器回归出输入点云的旋转。</p><h3 id="3-思考-5">3 思考</h3><p>相当于将DenseFusion的实例分割先验步骤进行了替换，使用了yolo+点云分割来代替。最后的特征还是逐点特征。</p><h1>CVPR2021</h1><h2 id="01-GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation">01. GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Gu Wang,  Tsinghua University, BNRist<br><strong>关键词</strong>：位姿估计；端到端<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/THU-DA-6D-Pose-Group/GDR-Net">https://github.com/THU-DA-6D-Pose-Group/GDR-Net</a></p></blockquote><h3 id="1-目标问题-6">1 目标问题</h3><p>提出一种端到端的位姿估计算法。</p><h3 id="2-方法-6">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316104020.png" alt=""></p><p><strong>（1）网络架构</strong></p><p>首先向GDR-Net提供256x256的ROI图，预测出三个64x64的中间特征图</p><ul><li>稠密对应图$M_{2D-3D}$：将稠密坐标映射$M_{XYZ}$对跌倒2D像素坐标上得到，反映了对象的几何形状信息。</li><li>表面区域注意图$M_{SRA}$：采用最远点采样从$M_{XYZ}$中到处，代表了对象的对称性。</li><li>可见对象掩码$M_{vis}$</li></ul><p>使用一个简单的2D卷积Patch Pnp模块直接从特征图中回归6D对象姿态。Patch PnP模块由三个卷积层组成，然后用两个全连接层用于扁平化特征，最后连个全连接层输出R6D旋转和tSITE平移。</p><h3 id="3-思考-6">3 思考</h3><p>本文专注于图像的特征提取和处理工作，实现从单一图片预测位姿的功能。方法不够直观。</p><h2 id="02-FS-Net-Fast-Shape-based-Network-for-Category-Level-6D-Object-Pose-Estimation-with-Decoupled-Rotation-Mechanism">02. FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Wei Chen, School of Computer Science, University of Birmingham<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/DC1991/FS_Net">https://github.com/DC1991/FS_Net</a></p></blockquote><h3 id="1-目标问题-7">1 目标问题</h3><p>解决以往类别级姿态特征提取效率低，精度和推理速度低的问题。</p><h3 id="2-方法-7">2 方法</h3><p>设计了一种具有3D图卷积的方向感知自动编码器，用于潜在特征提取。</p><p>提出解耦旋转机制，利用两个解码器互补的访问旋转信息。</p><p>使用两个残差来估计平移。</p><p>提出一种在线box-cage的三维变形机制来增强训练数据。</p><p><img src="https://img.mahaofei.com/img/20230316151555.png" alt=""></p><ol><li>输入RGB图像。</li><li>使用yolov3检测对象的2D位置、类别标签、类概率图，并将最大概率的位置作为3D球体的中心。从而得到目标3D球体点云。</li><li>使用三维变形机制进行数据扩充。</li><li>使用基于形状的3DGC自动编码器来进行点云分割，用于旋转的潜在特征学习。<br>3DGC由m个单位向量组成，卷积值是核向量和n个最近向量之间的余弦相似度之和。</li><li>从潜在特征中将旋转信息解码为两个垂直向量。</li><li>利用残差估计网络预测平移。</li></ol><h3 id="3-思考-7">3 思考</h3><p>提出的使用三维变形机制进行数据扩充很有意思，或许后续很多方法都可以加上这个步骤，使得算法更具有鲁棒性。</p><p>需要训练yolo模型和FS_Net模型。</p><p><strong>NOCS数据集</strong></p><h1>CVPR2022</h1><h2 id="01-OVE6D-Object-Viewpoint-Encoding-for-Depth-based-6D-Object-Pose-Estimation">01. OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Dingding Cai, Tampere University<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/dingdingcai/OVE6D-pose">https://github.com/dingdingcai/OVE6D-pose</a></p></blockquote><h3 id="1-目标问题-8">1 目标问题</h3><p>已知物体的分割掩码，物体的三维mesh模型，预测从物体坐标系到相机坐标系的变换R+T。</p><h3 id="2-方法-8">2 方法</h3><p><strong>（1）训练阶段</strong></p><p>使用ShapeNet中的3D物体模型来训练OVE6D模型，这个阶段只进行一次，得到的模型参数在后续保持固定。</p><p><strong>（2）编码阶段</strong></p><p>将目标物体的3D网络模型转换为view points编码本，这个阶段对每个物体只进行一次。（view points编码本是一个特征向量的集合）</p><p><strong>（3）推理阶段</strong></p><p>从输入的物体深度图像和物体分割掩码中推理物体的6D姿态</p><ol><li>视角估计：将输入图像和物体ID作为输入，通过与view points编码本中的特征向量进行余弦相似度匹配找到最接近的预定义视角，并输出索引和置信度。</li><li>平面旋转估计：输入图像、ID、预定视角索引、置信度，通过卷积神经网络回归出相对于相机坐标系的旋转。</li><li>平移估计：输入图像、ID、预定义视角索引、置信度、平面旋转角度，通过另一个卷积神经网络输出物体的3D位置。</li></ol><h3 id="3-思考-8">3 思考</h3><p>算法需要预先训练好ShapeNet，然后确定一个view points编码本，过程较复杂不够简洁直观。</p><h2 id="02-OnePose-One-Shot-Object-Pose-Estimation-without-CAD-Models">02. OnePose: One-Shot Object Pose Estimation without CAD Models</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Jiaming Sun, Zhejiang University<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/zju3dv/OnePose">https://github.com/zju3dv/OnePose</a></p></blockquote><h3 id="1-目标问题-9">1 目标问题</h3><p>实现不依赖于CAD模型的位姿估计</p><h3 id="2-方法-9">2 方法</h3><p>借鉴了视觉定位的思想，只需要一个简单的RGB视频扫描物体，就可以构建一个稀疏的SfM模型。然后，利用一个通用的特征匹配网络将这个模型与新的查询图像对齐，从而得到物体姿态。</p><p>提出了一种新的图注意力网络（GATs），可以将同一个SfM点对应的2D特征聚合成3D特征，并与查询图像中的2D特征进行自注意力和交叉注意力匹配。</p><p><img src="https://img.mahaofei.com/img/20230316201824.png" alt=""></p><ol><li>对于每一个物体，使用视频扫描得到一组相机姿态以及物体的3D边界框。</li><li>利用SFM重建一个稀疏的点云模型</li><li>SfM的2D-3D对应关系被建立起来</li><li>使用注意力聚合层，将2D描述符聚合到3D描述符</li><li>通过PnP回归计算出物体位姿</li></ol><p><strong>总体实现流程如下</strong></p><ul><li>使用一些AR工具捕获物体数据，包括物体中心位置，尺寸，绕Z州的旋转角，相机姿态等。</li><li>从捕获的视频中提取图像，使用SfM重建稀疏点云，所有的对应图中提取2D关键点和描述符。</li><li>定位时，实时捕获一系列图像，提取2D关键点和描述符进行匹配，从数据库中查询候选图像，从而找到相机姿态。</li></ul><h3 id="3-思考-9">3 思考</h3><p>大概就是创建一个数据库，包括2D图像和重建出的点云，以及相应的2D-3D关键点和描述符，对每一个输入图像提取特征后进行匹配查询。</p><h2 id="03-Focal-Length-and-Object-Pose-Estimation-via-Render-and-Compare">03. Focal Length and Object Pose Estimation via Render and Compare</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Georgy Ponimatkin, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://ponimatkin.github.io/focalpose">https://ponimatkin.github.io/focalpose</a></p></blockquote><h3 id="1-目标问题-10">1 目标问题</h3><p>估计相机参数未知的照片中物体的6D姿态</p><h3 id="2-方法-10">2 方法</h3><ol><li>从一个3D模型库中选择一个与输入图像中物体最匹配的3D模型。</li><li>用一个CNN编码器将输入图像编码成一个特征向量。</li><li>用一个CNN解码器将特征向量解码成一个初始的6D姿态和焦距。</li><li>用渲染引擎根据初始的6D姿态和焦距渲染出一个虚拟视图，并与输入图像进行比较。</li><li>用一个损失函数计算虚拟视图和输入图像之间的差异，并反向传播更新6D姿态和焦距。</li><li>重复步骤4和5直到收敛或达到最大迭代次数。</li></ol><h3 id="3-思考-10">3 思考</h3><p>对于网络图像中物体的位姿估计，不知道实际应用场景是什么。</p><h2 id="04-ES6D-A-Computation-Efficient-and-Symmetry-Aware-6D-Pose-Regression-Framework">04. ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Ningkai Mo, ShenZhen Key Lab of Computer Vision and Pattern Recognition<br><strong>关键词</strong>：位姿估计；对称<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/GANWANSHUI/ES6D">https://github.com/GANWANSHUI/ES6D</a></p></blockquote><h3 id="1-目标问题-11">1 目标问题</h3><p>主要解决如何利用RGB-D数据来估计刚体物体的6D姿态，特别是对于具有对称性的物体</p><h3 id="2-方法-11">2 方法</h3><ul><li>设计了一个全卷积的特征提取网络，叫做XYZNet，它可以高效地从RGB和深度图中提取点云特征，并将不同模态的特征融合起来。</li><li>提出了一种新的形状表示方法，叫做分组基元（GP），它只与物体的对称性有关，而忽略了形状的细节。</li><li>基于GP，设计了一种新的姿态距离度量，叫做平均（最大）分组基元距离，或者A(M)GPD。这种度量可以作为损失函数来训练回归网络，并保证网络收敛到正确的姿态。</li></ul><p><img src="https://img.mahaofei.com/img/20230316211632.png" alt=""></p><ol><li>从RGB-D图像生成RGB-XYZ数据。RGB-XYZ数据被馈送到CNN模块以提取局部特征，该局部特征对颜色和几何信息进行编码</li><li>点云特征是通过类似PointNet的CNN模块获得的，并填充到与局部特征相同的大小</li><li>将局部特征和点云特征连接为用于姿态估计的逐点特征</li><li>选择具有最大置信度的姿势作为最终结果</li></ol><h3 id="3-思考-11">3 思考</h3><p>同样是逐点特征，这篇论文提出了XYZNet，可以更高效的提取提取点云和RGB特征，不需要提供掩码图。</p><p>代码只提供T-LESS数据集方法。</p><h2 id="05-GPV-Pose-Category-level-Object-Pose-Estimation-via-Geometry-guided-Point-wise-Voting">05.  GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：YanDi, Technical University of Munich<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/lolrudy/GPV_Pose">https://github.com/lolrudy/GPV_Pose</a></p></blockquote><h3 id="1-目标问题-12">1 目标问题</h3><p>主要解决了现有方法在处理未见过的物体实例时存在的不确定性和不稳定性的问题</p><h3 id="2-方法-12">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230317093148.png" alt=""></p><ol><li>给定一副RGB-D图像，先使用如Maks-RCNN等方法将物体从深度图中分割出来。</li><li>然后从深度三维点云中抽取1028个点，并将它们输入到GPV-Pose位姿估计网络中。</li><li>由于3DGC对于点云的移动和缩放不敏感，所以以3DGC为主干提取全局和每个点的特征，凭借附加了三个并行分支，用于姿态预测，对称性，和逐点包围盒。</li></ol><p>注：</p><ul><li>3DGC方法首先将输入点云转换为一个k近邻图（kNN graph），其中每个点与其最近的k个邻居相连。使用多层图卷积（Graph Convolution）来提取每个点的局部特征，并使用最大池化（Max Pooling）来提取全局特征。3DGC方法将全局和逐点特征拼接起来，形成一个混合特征向量，用于后续的姿态估计、对称感知重建和点投票模块。</li></ul><h3 id="3-思考-12">3 思考</h3><p><strong>（1）创新点</strong></p><ul><li>引入了一种解耦的置信度驱动旋转表示，允许几何感知恢复相关旋转矩阵</li><li>提出了一种基于点投票的位移估计模块，利用几何约束来生成可靠和精确的位移预测</li><li>在一个端到端可训练的网络中整合这两个模块，并通过一个多任务损失函数进行优化</li></ul><p><strong>（2）与DenseFusion相比</strong></p><p>它们都使用了3D图卷积网络（3DGC）来从输入点云中提取每个点的局部特征，并将其与全局特征拼接起来，形成一个混合特征向量。</p><p>GPV-Pose使用了一种解耦的置信度驱动的旋转表示，可以通过几何关系恢复旋转矩阵，而DenseFusion使用了一种直接预测四元数的方法。</p><p><strong>使用NOCS数据集。</strong></p><h2 id="06-DGECN-A-Depth-Guided-Edge-Convolutional-Network-for-End-to-End-6D-Pose-Estimation">06. DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Tuo Cao, School of Computer Science, Wuhan University, Wuhan, Hubei, China<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/maplect/DGECN_CVPR2022">https://github.com/maplect/DGECN_CVPR2022</a></p></blockquote><h3 id="1-目标问题-13">1 目标问题</h3><p>从单目RGB图像中进行位姿估计。</p><h3 id="2-方法-13">2 方法</h3><p><strong>（1）深度细化网络DRN</strong></p><p>两个不同的深度估计网络分别输出深度图DA和DB，计算两个深度图之间的差异，并将差异超过阈值的区域定义为不确定区域。</p><p><strong>（2）特征提取</strong></p><ul><li>深度估计：将彩色图像作为输入，并执行深度图预测</li><li>对象分割：利用分割的掩码，将深度图转换为3D点云，并利用3D特征提取器来提取几何特征</li></ul><p><strong>（3）2D关键点定位</strong></p><p>采用最远点采样（FPS）算法来选择物体表面上的关键点。</p><p><strong>（4）从2D-3D对应关系学习6D位姿</strong></p><p>使用动态图PnP（DG-PnP）算法，通过边缘卷积构建一个图结构，利用2D-3D对应关系中的拓扑信息来直接学习6D姿态</p><h3 id="3-思考-13">3 思考</h3><p><strong>（1）创新点</strong></p><ul><li>用一个深度引导网络同时预测分割和深度图，并用一个深度优化网络（DRN）提高深度图的质量</li><li>根据分割和深度图建立2D-3D对应关系，即将图像上的关键点与3D模型上的点匹配</li><li>提出一个动态图PnP（DG-PnP）算法，通过边缘卷积构建一个图结构，利用2D-3D对应关系中的拓扑信息来直接学习6D姿态</li></ul><p><strong>（2）实用性</strong></p><p>从单目RGB图像进行位姿估计，通过网络回归深度图。</p><h2 id="07-Templates-for-3D-Object-Pose-Estimation-Revisited-Generalization-to-New-Objects-and-Robustness-to-Occlusions">07. Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions)</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Van Nguyen Nguyen, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/nv-nguyen/template-pose">https://github.com/nv-nguyen/template-pose</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h3 id="1-目标问题-14">1 目标问题</h3><p>提出了一种方法，只需要物体的CAD模型，就可以将输入对象匹配到一组模板，即使在部分遮挡的情况下也可以估计3D姿态。</p><h3 id="2-方法-14">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230322150858.png" alt=""></p><p>训练时，使用由真实图像和合成模板组成的对，来计算局部特征，预测两幅图像的相似性。</p><p>然后对于未看到的图像，计算其局部特征，将图像与模板数据库匹配来检索对象姿态。</p><h3 id="3-思考-14">3 思考</h3><p>编码本思想，实用性较强，可尝试。</p><h2 id="08-Coupled-Iterative-Refinement-for-6D-Multi-Object-Pose-Estimation">08. Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Lahav Lipson, Princeton University<br><strong>关键词</strong>：位姿估计；迭代优化<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">https://github.com/princeton-vl/Coupled-Iterative-Refinement</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h3 id="1-目标问题-15">1 目标问题</h3><p>给定一组已知的RGBD输入，检测每个对象的6D姿态。</p><h3 id="2-方法-15">2 方法</h3><p>算法复杂，迭代优化方法。</p><h3 id="3-思考-15">3 思考</h3><p>代码效果最好，但是代码较为复杂。</p>]]></content>
    
    
    <summary type="html">检索阅读近三年CVPR6D位姿估计相关论文并进行记录</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】DenseFusion被引论文丨无代码</title>
    <link href="https://www.mahaofei.com/post/d3bc13be.html"/>
    <id>https://www.mahaofei.com/post/d3bc13be.html</id>
    <published>2023-03-14T07:23:31.000Z</published>
    <updated>2023-03-14T07:23:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1>01. 6D Pose Estimation for Bin-Picking based on Improved Mask R-CNN and DenseFusion</h1><blockquote><p><strong>期刊 / 会议</strong>：26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)<br><strong>作者 / 机构</strong>：Hesheng Wang, 上海交通大学<br><strong>关键词</strong>：位姿估计, 实例分割, MaskRCNN, DenseFusion<br><strong>时间</strong>：2021<br><strong>代码</strong>：无</p></blockquote><h2 id="1-目标问题">1 目标问题</h2><p>将实例分割算法和位姿估计算法应用到工业机器人抓取中。</p><h2 id="2-方法">2 方法</h2><p>使用两级神经网络，将输入的RGB-D图像回归出6D位姿。</p><p><img src="https://img.mahaofei.com/img/20230314153950.png" alt=""></p><p><strong>（1）数据集生成</strong></p><p>由于基于学习的算法需要大量的已经标注的数据集，因此本文给出了一种工业零件的虚拟数据集的生成过程。使用Blender进行物理模拟，调整渲染参数，生成多种高质量的RGB图像及分割掩码和6D姿态标签。</p><p><strong>（2）实例分割</strong></p><p>使用ResNeXt与MaskRCNN完成目标检测与实例分割。</p><p><strong>（3）姿态估计</strong></p><p>基于DenseFusion来预测6D位姿，为了提高性能，增加了NonLocal模块，(图中的绿色块)，使得网络可以从提取的特征中学习空间结构特征，并且可以有效地在点特征之间建立连接。（好像没什么用）</p><p><img src="https://img.mahaofei.com/img/1678779913.png" alt=""></p><h2 id="3-思考">3 思考</h2><p>虚拟数据集可辅助提高训练结果的鲁棒性，可尝试。</p><h1>02. A Lightweight Two-End Feature Fusion Network for Object 6D Pose Estimation</h1><blockquote><p><strong>期刊 / 会议</strong>：Machines<br><strong>作者 / 机构</strong>：Ligang Zuo, 北京科技大学<br><strong>关键词</strong>：位姿估计, 特征融合<br><strong>时间</strong>：2022<br><strong>代码</strong>：无代码</p></blockquote><h2 id="1-目标问题-2">1 目标问题</h2><p>提出一种轻量化的位姿估计模型，用于部署在移动设备上。</p><h2 id="2-方法-2">2 方法</h2><ol><li>使用PointnoProblemNet网络提取点云特征</li><li>将点云特征与图像特征进行像素级融合</li><li>利用CNN进行特征提取</li><li>对每个特征进行位姿估计，选择置信度最高的作为最终结果<br>（这不就是DenseFusion的思路吗，只不过把PointNet换成了PointnoProblem）</li></ol><p>使用深度可分离卷积代替标准卷积，即将ResNet特征提取部分换为MobileNetv2，来减少模型参数的数量。</p><h2 id="3-思考-2">3 思考</h2><p>可以考虑尝试使用MobileNetv2特征提取网络，因为所使用的也是移动设备。</p>]]></content>
    
    
    <summary type="html">检索了DenseFusion的被引论文，但是因为太多都没有开源代码，看了两篇就转向只关注顶会论文了。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="DenseFusion" scheme="https://www.mahaofei.com/tags/DenseFusion/"/>
    
  </entry>
  
  <entry>
    <title>Windows设置共享文件夹给Ubuntu或其它设备</title>
    <link href="https://www.mahaofei.com/post/76e8a448.html"/>
    <id>https://www.mahaofei.com/post/76e8a448.html</id>
    <published>2023-03-11T04:11:46.000Z</published>
    <updated>2023-03-11T04:11:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Windows系统设置</h1><p>在需要的位置新建一个文件夹，然后【右键-共享】，将其设置为共享文件夹。</p><p><img src="https://img.mahaofei.com/img/202303111048877.png" alt=""></p><p>然后打开【设置-网络和Internet-高级网络设置-高级共享设置】，设置为如下的形式。<strong>主要是公用网络的两个都要打开，所有网络的共享打开，密码保护关闭</strong></p><p><img src="https://img.mahaofei.com/img/202303111048312.png" alt=""></p><h1>二、Ubuntu系统设置</h1><h2 id="2-1-临时挂载">2.1 临时挂载</h2><p>挂载方式如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t cifs //192.168.3.67/home/Share ~/Share -o username=&#x27;admin&#x27;,password=&#x27;123456&#x27;,dir_mode=0777,file_mode=0777,vers=2.0</span><br></pre></td></tr></table></figure><h2 id="2-2-自动挂载">2.2 自动挂载</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/fstab</span><br></pre></td></tr></table></figure><p>在最后面按照下面的格式添加</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//192.168.1.143/home/Share ~/share cifs defaults,username=admin,password=123456,dir_mode=0777,file_mode=0777,vers=2.0 0 2</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">局域网内，共享win电脑上的某个文件夹，使局域网内其它系统可以直接访问</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Windows工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Windows%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu使用OneDrive</title>
    <link href="https://www.mahaofei.com/post/ba007624.html"/>
    <id>https://www.mahaofei.com/post/ba007624.html</id>
    <published>2023-03-10T07:31:33.000Z</published>
    <updated>2023-03-10T07:31:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1>安装</h1><p><strong>（1）安装依赖</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt install build-essential </span><br><span class="line">apt install libcurl4-openssl-dev -y</span><br><span class="line">apt install libsqlite3-dev -y</span><br><span class="line">apt install pkg-config -y</span><br><span class="line">apt install libnotify-dev -y</span><br><span class="line">curl -fsS https://dlang.org/install.sh | bash -s dmd</span><br></pre></td></tr></table></figure><p>激活DMD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/dlang/dmd-2.082.0/activate</span><br></pre></td></tr></table></figure><p><strong>（2）安装onedrive客户端</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/abraunegg/onedrive.git</span><br><span class="line">cd onedrive</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><h1>使用</h1><p>输入以下命令登录onedrive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onedrive</span><br></pre></td></tr></table></figure><p>下载config文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/.config/onedrive</span><br><span class="line">wget https://raw.githubusercontent.com/abraunegg/onedrive/master/config -O ~/.config/onedrive/config</span><br><span class="line">nano ~/.config/onedrive/config</span><br></pre></td></tr></table></figure><p>打开config文件中的下面几行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sync_dir = &quot;~/disk/ubuntu/onedrive&quot;</span><br><span class="line">monitor_interval = &quot;60&quot;</span><br></pre></td></tr></table></figure><h1>同步</h1><p>第一次同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onedrive --synchronize</span><br></pre></td></tr></table></figure><p>实时同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onedrive --monitor</span><br></pre></td></tr></table></figure><blockquote><p>参考链接</p><ol><li><a href="https://github.com/abraunegg/onedrive">abraunegg/onedrive</a></li><li><a href="https://www.moerats.com/archives/740/">Rat’s. 适用于Linux的OneDrive客户端，支持VPS和OneDrive之间实时同步/备份</a></li></ol></blockquote><h1>替代方法</h1><p>此方法不太推荐，同步速度较慢，而且一旦取消同步本地文件都会清空。</p><h2 id="1-安装OneDriver">1. 安装OneDriver</h2><p>参考项目：<a href="https://github.com/jstaf/onedriver">https://github.com/jstaf/onedriver</a></p><p>根据作者的说明，对于Ubuntu系统，可以直接下载deb文件安装，下载链接为：<a href="https://software.opensuse.org/download.html?project=home%3Ajstaf&amp;package=onedriver">https://software.opensuse.org/download.html?project=home%3Ajstaf&amp;package=onedriver</a></p><p>选择Ubuntu，找到自己的Ubuntu版本，以及amd64/arm64，下载deb安装包并安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i onedriver_0.13.0-1_amd64.deb</span><br></pre></td></tr></table></figure><h1>2. OneDriver使用</h1><p>点击左上角<code>+</code>，选择本地同步文件夹。</p><p>然后输入用户名密码登录自己的OneDrive网盘。</p><p>点击右面的√，勾选系统登录时启动OneDriver。</p>]]></content>
    
    
    <summary type="html">之前一直使用坚果云做多设备同步，但是坚果云每个月只有1G流量，不够用。因此改到OneDrive，本文记录在Ubuntu上进行OneDrive的配置过程。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【浏览器插件】iTab新建标签页</title>
    <link href="https://www.mahaofei.com/post/999804d5.html"/>
    <id>https://www.mahaofei.com/post/999804d5.html</id>
    <published>2023-03-10T00:28:23.000Z</published>
    <updated>2023-03-10T00:28:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1>介绍</h1><p>本人之前很长一段时间都是实用的默认主页，并且只保留一个搜索框使主页尽可能简洁。</p><p>但是后来随着要用的网页越来越多，很多网站记不住经常需要搜索才能找到。</p><p>收藏夹又因为网站太多，我的收藏夹都是文件夹套文件夹套网址。</p><p>正巧看到了这个新建标签页，感觉还不错，有兴趣的可以体验一下，有点像之前用的infinite和青柠，但个人觉得这个更好一些。</p><h1>主页</h1><p>主页侧面是不同的分区，每个分区内可以添加很多不同的图标。</p><p><img src="https://img.mahaofei.com/img/20230310082742.png" alt=""></p><p>有官方给的图标，也可以自定义添加。</p><p><img src="https://img.mahaofei.com/img/20230310082814.png" alt=""></p><h1>其它</h1><p>而且itab支持网页版主页，也就是说，手机edge等无法安装浏览器扩展的应用，可以在设置里将主页改成<a href="https://go.itab.link/">https://go.itab.link/</a>就可以实现手机电脑使用同样的主页了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;介绍&lt;/h1&gt;
&lt;p&gt;本人之前很长一段时间都是实用的默认主页，并且只保留一个搜索框使主页尽可能简洁。&lt;/p&gt;
&lt;p&gt;但是后来随着要用的网页越来越多，很多网站记不住经常需要搜索才能找到。&lt;/p&gt;
&lt;p&gt;收藏夹又因为网站太多，我的收藏夹都是文件夹套文件夹套网址。&lt;/p&gt;
&lt;p</summary>
      
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="浏览器插件" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8F%92%E4%BB%B6/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【统计学习方法笔记04】朴素贝叶斯法</title>
    <link href="https://www.mahaofei.com/post/7bc6ebb.html"/>
    <id>https://www.mahaofei.com/post/7bc6ebb.html</id>
    <published>2023-03-04T11:14:19.000Z</published>
    <updated>2023-03-04T11:14:19.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、贝叶斯定理</h1><h2 id="1-1-条件概率">1.1 条件概率</h2><p>条件概率属于概率论的内容，指的是已知在情况A的条件下，求发生事件B的概率，即P(B|A)，计算方法如下。</p><p>$$P(B|A)=\frac{P(AB)}{P(A)}$$</p><h2 id="1-2-贝叶斯定理">1.2 贝叶斯定理</h2><p>相对于条件概率，贝叶斯定理是其逆过程。已知发生事件B，求事件发生的情况A的概率，即P(A|B)，计算方法如下，其中分母是全概率公式。</p><p>$$P(A|B)=\frac{P(AB)}{P(B)}=\frac{P(A|B)\cdot P(B)}{P(B|A)\cdot P(A)+P(B|\hat A)\cdot P(\hat A)}$$</p><p>同理推广到分类问题，已知存在K类$c_1,c_2\dots c_k$，给定一个新的实例$x=(x^{(1)},x^{(2)}\dots x^{(n)})$，求该实例点归属于$c_i$类的可能。</p><p>$$P(Y=c_i|X=x)=\frac{P(X=x|Y=c_i)\cdot P(Y=c_i)}{P(X=x)}=\frac{P(X=x|Y=c_i)\cdot P(Y=c_i)}{\sum^K_{i=1}P(X=x|Y=c_i)\cdot P(Y=c_i)}$$</p><h2 id="1-3-朴素贝叶斯">1.3 朴素贝叶斯</h2><p>朴素贝叶斯相较于贝叶斯，多了<strong>实例特征之间相互独立</strong>这个条件，这样更便于计算</p><p>即$P(X=x|Y=c_i)=\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)$</p><p>因此</p><p>$$P(Y=c_i|X=x)=\frac{P(X=x|Y=c_i)\cdot P(Y=c_i)}{\sum^K_{c_i}P(Y=c_i)\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)}$$</p><p>$$P(Y=c_i|X=x)=\frac{P(Y=c_i)\cdot \prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)}{\sum^K_{c_i}P(Y=c_i)\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)}$$</p><p>可以看出$x$属于任何分类$c_i$的概率，其分母都是一样的，因此实际计算时只需要比较分子即可，即</p><p>$$argmax P(Y=c_i)\cdot \prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)$$</p><p>通过训练数据集，我们可以得到联合概率分布。</p><h1>二、后验概率最大化准则</h1><h1>三、极大似然估计</h1>]]></content>
    
    
    <summary type="html">统计学习方法的学习笔记</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="深度学习基础" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="统计学习方法" scheme="https://www.mahaofei.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Terminator终端终结者常用快捷键</title>
    <link href="https://www.mahaofei.com/post/79fdfcf6.html"/>
    <id>https://www.mahaofei.com/post/79fdfcf6.html</id>
    <published>2023-03-04T00:52:28.000Z</published>
    <updated>2023-03-04T00:52:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1>安装</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install terminator</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230304085853.png" alt=""></p><h1>快捷键</h1><p><strong>常用快捷键</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+Shift+O                    //水平分割终端</span><br><span class="line">Ctrl+Shift+E                    //垂直分割终端（有按键冲突）</span><br><span class="line">Ctrl+Shift+Right                //在垂直分割的终端中将分割条向右移动</span><br><span class="line">Ctrl+Shift+Left                 //在垂直分割的终端中将分割条向左移动</span><br><span class="line">Ctrl+Shift+Up                   //在水平分割的终端中将分割条向上移动</span><br><span class="line">Ctrl+Shift+Down                 //在水平分割的终端中将分割条向下移动</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Ctrl+Shift+T                    //打开一个新的标签</span><br><span class="line">Ctrl+Shift+W                    //关闭当前标签</span><br><span class="line">Ctrl+PageDown                   //移动到下一个标签</span><br><span class="line">Ctrl+PageUp                     //移动到上一个标签</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Ctrl+Shift+G                    //重置终端状态并clear屏幕</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Super+t                         //绑定当前标签的所有终端，向一个终端输入的内容会自动输入到其他终端</span><br><span class="line">Super+Shift+T                   //解除绑定</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="（1）同一个窗口">（1）同一个窗口</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Alt+Up                          //★移动到上面的终端</span><br><span class="line">Alt+Down                        //★移动到下面的终端</span><br><span class="line">Alt+Left                        //★移动到左边的终端</span><br><span class="line">Alt+Right                       //★移动到右边的终端</span><br><span class="line">Ctrl+Shift+O                    //★水平分割终端</span><br><span class="line">Ctrl+Shift+E                    //垂直分割终端（有按键冲突）</span><br><span class="line">Ctrl+Shift+Right                //在垂直分割的终端中将分割条向右移动</span><br><span class="line">Ctrl+Shift+Left                 //在垂直分割的终端中将分割条向左移动</span><br><span class="line">Ctrl+Shift+Up                   //在水平分割的终端中将分割条向上移动</span><br><span class="line">Ctrl+Shift+Down                 //在水平分割的终端中将分割条向下移动</span><br><span class="line">Ctrl+Shift+S                    //隐藏/显示滚动条</span><br><span class="line">Ctrl+Shift+F                    //搜索</span><br><span class="line">Ctrl+Shift+C                    //★复制选中的内容到剪贴板</span><br><span class="line">Ctrl+Shift+V                    //★粘贴剪贴板的内容到此处</span><br><span class="line">Ctrl+Shift+W                    //★关闭当前终端</span><br><span class="line">Ctrl+Shift+Q                    //退出当前窗口，当前窗口的所有终端都将被关闭</span><br><span class="line">Ctrl+Shift+X                    //★最大化显示当前终端</span><br><span class="line">Ctrl+Shift+Z                    //最大化显示当前终端并使字体放大</span><br><span class="line">Ctrl+Shift+N or Ctrl+Tab        //移动到下一个终端</span><br><span class="line">Ctrl+Shift+P or Ctrl+Shift+Tab  //Crtl+Shift+Tab 移动到之前的一个终端</span><br></pre></td></tr></table></figure><h2 id="（2）不同窗口之间">（2）不同窗口之间</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">F11                             //全屏开关</span><br><span class="line">Ctrl+Shift+T                    //打开一个新的标签</span><br><span class="line">Ctrl+PageDown                   //移动到下一个标签</span><br><span class="line">Ctrl+PageUp                     //移动到上一个标签</span><br><span class="line">Ctrl+Shift+PageDown             //将当前标签与其后一个标签交换位置</span><br><span class="line">Ctrl+Shift+PageUp               //将当前标签与其前一个标签交换位置</span><br><span class="line">Ctrl+Plus (+)                   //增大字体</span><br><span class="line">Ctrl+Minus (-)                  //减小字体</span><br><span class="line">Ctrl+Zero (0)                   //恢复字体到原始大小</span><br><span class="line">Ctrl+Shift+R                    //重置终端状态</span><br><span class="line">Ctrl+Shift+G                    //重置终端状态并clear屏幕</span><br><span class="line">Super+g                         //绑定所有的终端，以便向一个输入能够输入到所有的终端</span><br><span class="line">Super+Shift+G                   //解除绑定</span><br><span class="line">Super+t                         //绑定当前标签的所有终端，向一个终端输入的内容会自动输入到其他终端</span><br><span class="line">Super+Shift+T                   //解除绑定</span><br><span class="line">Ctrl+Shift+I                    //打开一个窗口，新窗口与原来的窗口使用同一个进程</span><br><span class="line">Super+i                         //打开一个新窗口，新窗口与原来的窗口使用不同的进程</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">一个比较好用的终端工具。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
</feed>
