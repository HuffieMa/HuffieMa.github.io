<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>马浩飞丨博客</title>
  
  <subtitle>进步无止境</subtitle>
  <link href="https://www.mahaofei.com/atom.xml" rel="self"/>
  
  <link href="https://www.mahaofei.com/"/>
  <updated>2023-12-24T05:54:05.000Z</updated>
  <id>https://www.mahaofei.com/</id>
  
  <author>
    <name>马浩飞</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【课程笔记】Stanford CS25 V2 - Robotics and Imitation Learning</title>
    <link href="https://www.mahaofei.com/post/1aebe4e4.html"/>
    <id>https://www.mahaofei.com/post/1aebe4e4.html</id>
    <published>2023-12-24T05:54:05.000Z</published>
    <updated>2023-12-24T05:54:05.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Video Link: <a href="https://www.youtube.com/watch?v=ct4tdyyNDY4">https://www.youtube.com/watch?v=ct4tdyyNDY4</a></p></blockquote><p>过去两年，由于大语言模型、语音技术、视觉技术的发展，机器人技术的研究思路发生了180度的转变。</p><h1>机器人基础模型</h1><p>目前在大量数据上进行大规模模型训练时，通常由两个比较重要的属性：</p><ul><li>涌现：当非常简单地事情在小范围发挥作用时，当扩大范围，它的表现会更好，例如更多的数据、更大的模型。</li><li>均质化：模型可以组合许多下游任务来实现泛化能力</li></ul><p>当然我们可能会思考一个问题，为什么目前还没有机器人领域的基础模型。不像音频、语言、图像等领域，这些领域已经出现了较为通用的基础模型，机器人技术的基础模型还没有人研究出来。</p><p>那么如果想要实现机器人的基础模型，可以如何做：</p><p><strong>（1）设计强化学习算法</strong></p><ol><li>利用高性能的架构，例如self-attention</li><li>利用比例法则，不仅要扩大模型大小，还有扩展计算，还需要扩大数据集语料库以及标记数量</li><li>数据集大小比数据质量更重要</li></ol><p><strong>（2）互联网规模的模型扩散</strong></p><ol><li>生成式模型在语言、编程、视觉、音频等领域已经展现出了涌现的能力，并且一次次超出了我们的预期，这个趋势在可见的未来中还会一直保持下去。</li><li>涌现和加速意味着这些模型可以“自己”变的更好</li></ol><p><strong>（3）从在线机器人学习转移到离线学习</strong></p><ol><li>目前的大模型都是在巨大规模的离线数据集上训练完成的</li></ol><h1>Google Brain 相关工作</h1><h2 id="过去工作">过去工作</h2><p><strong>（1）2016 - 机器人工厂</strong></p><ol><li><strong>强化学习</strong>：Google建立了一个包含7个Kuka机械臂的机器人工厂，7x24小时执行抓取动作，进行强化学习训练。</li><li><strong>Qt-Opt</strong>：Q-学习算法，接受视觉输入的同时进行连续控制。</li><li><strong>RL-CycleGan</strong>：将房展环境的图像转换为真实的图像，让机器人在显示世界中更好地完成任务。</li></ol><p><strong>（2）2020 - 厨房环境</strong></p><ol><li><strong>BC-Z</strong>：多任务模仿学习</li><li><strong>AW-Opt</strong>：将强化学习与模仿学习引导相结合</li></ol><p><strong>（3）2022</strong></p><p>遇到了一些问题，例如在一些场景任务中，成功率已经稳定到了50~70%，一些方法需要非常特定的数据分布，如果策略没有训练过当前的数据，那么任务很可能失败。为了解决这个问题，Google 进行了以下工作</p><ol><li>多任务模仿学习</li><li>使用大规模数据集</li></ol><h2 id="近期工作">近期工作</h2><p><strong>（1）RT-1</strong></p><p>主要聚焦于如何扩展模仿学习。</p><ol><li>使用一年半时间从13个机器人中收集了100k的演示数据，包括700个任务</li><li>使用BC-Z进行训练</li><li>由于基于Transformer的算法在处理图像时，现在还不足以支撑机器人学习中的高频率、实时性要求，因此希望数据集能够理解语言模型。</li></ol><p>从较高的层面来看，RT-1是一个机器人Transformer，它接受机器人摄像头的视觉输入，以及自然语言指令，仅仅使用transformer进行解码，分离目标物体类别，使用预训练的EfficientNet主干网络得到离散化动作。</p><p><strong>（2）SayCan</strong></p><p>机器人的技能能够是有限的，大语言模型也是受限制的，他不知道机器人的状态，不知道周围环境。</p><p>因此这项工作主要实现让语言模型说机器人的语言。</p><p><strong>（3）Inner-Monologue</strong></p><p>将环境的动态环境反馈加入到闭环中，也使用语言的API来传达环境中有什么。</p>]]></content>
    
    
    <summary type="html">Stanford CS25 V2中的一节课，由Google Brain工程师主讲的机器人学习主题课程。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器人" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="课程笔记" scheme="https://www.mahaofei.com/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Windows与Ubuntu双系统绑定同一个蓝牙设备（无需重新配对）</title>
    <link href="https://www.mahaofei.com/post/e0777253.html"/>
    <id>https://www.mahaofei.com/post/e0777253.html</id>
    <published>2023-12-13T13:25:52.000Z</published>
    <updated>2023-12-13T13:25:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1、Ubuntu 下配对蓝牙设备</h1><p>会生成一个配置文件。</p><h1>2、Windows 下再配对蓝牙设备</h1><p>读取蓝牙配对信息，Windows下的蓝牙配对信息存储在注册表中，并且此信息需要使用特殊方法才能查看。</p><p>下载 <a href="https://docs.microsoft.com/en-us/sysinternals/downloads/psexec">PsTools</a>，将PSTools.zip 中的 PsExec.exe 或 PsExec64.exe 解压出来，并以管理员权限运行 cmd，<code>cd</code> 到<code>PsExec64.exe</code> 所在目录，使用下面的命令启动regedit.exe</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psexec64.exe -si regedit</span><br></pre></td></tr></table></figure><p>找到下面的蓝牙配对信息</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\BTHPORT\Parameters\Keys\【本机蓝牙 MAC】</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202312132218479.png" alt="image.png"></p><p>可以看到其中有类型为<code>REG_BINARY</code>的值，每个值对应一个设备，将其名称和数据记下来，或者直接导出，我导出后用记事本打开如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Windows Registry Editor Version 5.00</span><br><span class="line"></span><br><span class="line">[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\BTHPORT\Parameters\Keys\d4d853556b34]</span><br><span class="line">&quot;CentralIRK&quot;=hex:38,a5,32,96,6c,bf,c2,31,7b,e9,43,e0,b9,cc,9d,d6</span><br><span class="line">&quot;6468761a6934&quot;=hex:6b,bf,d8,14,d8,aa,2f,d8,f7,68,73,a7,83,64,6e,8f</span><br></pre></td></tr></table></figure><h1>3、修改Linux下的蓝牙配对信息</h1><p>Linux 下蓝牙设备的配对信息存储在 <code>/var/lib/bluetooth/【本机蓝牙 MAC】</code>目录下，例如 <code>/var/lib/bluetooth/D4:D8:53:55:6B:34</code>，MAC 地址中的字母全为大写，且含冒号分隔符。</p><p>进入该目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br><span class="line">cd /var/lib/bluetooth/D4\:D8\:53\:55\:6B\:34/</span><br></pre></td></tr></table></figure><p>可以看到系统已配对的蓝牙设备：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@Victus-Ubuntu:/var/lib/bluetooth/D4:D8:53:55:6B:34# ll</span><br><span class="line">total 44</span><br><span class="line">drwx------ 7 root root  4096 12月 13 22:19 ./</span><br><span class="line">drwxr-xr-x 3 root root  4096 5月  13  2023 ../</span><br><span class="line">drwx------ 2 root root  4096 12月 13 22:19 64:68:76:1A:69:34/</span><br><span class="line">drwx------ 2 root root  4096 12月 13 22:19 A4:C1:38:AB:43:5C/</span><br><span class="line">drwx------ 2 root root 16384 12月 13 22:20 cache/</span><br><span class="line">drwx------ 2 root root  4096 12月 13 22:19 EC:B3:D5:3C:5F:92/</span><br><span class="line">drwx------ 2 root root  4096 12月 13 22:20 F9:E7:70:AD:8C:64/</span><br><span class="line">-rw------- 1 root root    50 12月 13 22:19 settings</span><br></pre></td></tr></table></figure><p>根据我们导出的reg文件，可以知道进入我们要配置的蓝牙鼠标的目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd 64\:68\:76\:1A\:69\:34/</span><br></pre></td></tr></table></figure><p>我们需要修改的就是这个目录下的 info 文件，以我的为例，文件如下（这里已经改好了，只需要将Key换成windows下导出的注册表里面的key，具体就是win下的逗号删除小写变大写）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[General]</span><br><span class="line">Name=EDIFIER W820NB 双金标版</span><br><span class="line">Class=0x240404</span><br><span class="line">SupportedTechnologies=BR/EDR;</span><br><span class="line">Trusted=true</span><br><span class="line">Blocked=false</span><br><span class="line">Services=00001101-0000-1000-8000-00805f9b34fb;0000110b-0000-1000-8000-00805f9b34fb;0000110c-0000-1000-8000-00805f9b34fb;0000110d-0000-1000-8000-00805f9b34fb;0000110e-0000-1000-8000-00805f9b34fb;0000111e-0000-1000-8000-00805f9b34fb;66666666-6666-6666-6666-666666666666;edf00000-edfe-dfed-fedf-edfedfedfedf;</span><br><span class="line"></span><br><span class="line">[LinkKey]</span><br><span class="line">Key=6BBFD814D8AA2FD8F76873A783646E8F</span><br><span class="line">Type=4</span><br><span class="line">PINLength=0</span><br></pre></td></tr></table></figure><p>然后重启电脑，可以直接连接蓝牙。</p>]]></content>
    
    
    <summary type="html">本人安装了Ubuntu与Windows双系统，平时使用蓝牙鼠标与耳机，但是每次切换系统都需要重新配对设备，因此想要实现双系统配对。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="双系统" scheme="https://www.mahaofei.com/tags/%E5%8F%8C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】机器人动作、轨迹、过程模仿</title>
    <link href="https://www.mahaofei.com/post/5e168247.html"/>
    <id>https://www.mahaofei.com/post/5e168247.html</id>
    <published>2023-12-12T07:36:37.000Z</published>
    <updated>2023-12-12T07:36:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale</h1><blockquote><p><strong>标题</strong>：AW-Opt：通过大规模模仿和强化学习机器人技能<br><strong>作者团队</strong>：Google<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://awopt.github.io/">https://awopt.github.io/</a></p></blockquote><h2 id="1-1-目标问题-2">1.1 目标问题</h2><p>强化学习可以实现目标任务，但是需要大量自主数据收集。模仿学习只能学习和演示一样程度的动作。</p><p>本文探索如何最好的结合两种方法，并进行扩展。实现大规模机器人学习。</p><h2 id="1-2-方法-2">1.2 方法</h2><p><strong>（1）任务场景与基础算法</strong></p><p>本文以基于视觉的垃圾分类任务为例，机器人从不同对象类型（可回收物、不可回收对象）等垃圾箱中拾取特定类型的对象。</p><p>本文的算法为了研究IL+RL的扩展性，在QT-Opt和AWAC两种IL+RL算法的基础上进行研究。</p><p><strong>（2）正样本过滤</strong></p><p>两种基础算法性能不够好的原因可能有以下两点：</p><ul><li>探索过程中增加了大量的失败事件、掩盖了最初的成功演示</li><li>算法在学习有效的Q函数，对actor进行更新之前，会删除预训练初始化。</li></ul><p>为了解决这个问题，本文进行了以下两个修改：</p><ul><li>为critic使用优先缓冲区，其中一半来自于成功事件奖励</li><li>对actor使用正向过滤，仅对通过过滤器的样本进行更新</li></ul><p><strong>（3）混合 actor-critic 探索</strong></p><p>QT-Opt方法没有明确的actor，由于任务使用交叉熵利用critic来优化动作，因此可以视为隐式策略。</p><p>AWAC是一种actor-critic算法，通过对actor的动作采样来实现，而这种方式在训练初期，主要是critic进行学习，来确定哪些动作是好的。这种方式对于复杂的任务学习存在限制。</p><p>本文为了解决以上问题，结合两种算法，并比较了四种策略：仅actor探索、隐式critic策略、episode级随机切换策略（80%critic策略、20%actor）、step级随机切换策略、</p><h1>2 DexMV: Imitation Learning for Dexterous Manipulation from Human Videos</h1><blockquote><p><strong>标题</strong>：DexMV：模仿学习，从人类视频中进行灵巧操作<br><strong>作者团队</strong>：University of California San Diego<br><strong>期刊会议</strong>：ECCV<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://yzqin.github.io/dexmv/">https://yzqin.github.io/dexmv/</a></p></blockquote><h2 id="2-1-目标问题">2.1 目标问题</h2><p>由于多指机器人拥有高自由度关节、非线性驱动，因此需要大量的强化学习训练数据，而机器人数据收集困难，且仅使用仿真数据训练机器人运动也很不自然，能否利用人类与真实世界的交互经验来引导机器人？</p><h2 id="2-2-方法">2.2 方法</h2><p>设计了一个基于模仿学习的平台</p><ul><li>多指机器人复杂灵巧操作的模拟系统</li><li>记录人手执行相同任务的大规模演示（从视频中提取手部3D位置和物体姿态）</li><li>演示翻译方法：将人体动作转换为机器人演示</li></ul><p><img src="https://img.mahaofei.com/img/202312131522496.png" alt="image.png"></p><h3 id="2-2-1-姿态估计">2.2.1 姿态估计</h3><p><strong>物体位姿估计</strong>：使用在YCB数据集上训练的PVN3D实现物体6D位姿的估计。</p><p><strong>人手姿态估计</strong>：使用MANO模型表示人手关节，进行手部检测和实例分割，利用现有的模型估计手部关节。</p><h3 id="2-2-2-演示数据转换">2.2.2 演示数据转换</h3><p>常见的模仿学习算法使用==机器人状态和动作==作为训练数据，而不是人手姿态。并且人手和灵巧手的运动学模型也不同。</p><p><strong>（1）手部运动重定向</strong></p><p>给定视频中估计的人手姿态序列，将其重定向为机器人关节角度序列。改过程可视为优化问题。</p><p>在大多数操作任务中，人类和机器人都是指尖接触物体，因此保留手掌根的指尖的空间向量。</p><p>但是这样可能会导致手指弯曲信息丢失，导致手指穿透物体，因此考虑同时优化手掌根到中指骨的向量。</p><p><strong>（2）机器人动作估计</strong></p><p>手部运动重定向提供了手部姿态到机器人关节角度的转换，但是关节扭矩是位置的，因此通过逆动力学函数，将关节角度拟合到连续的关节轨迹函数中，计算扭矩。在这个过程中，需要保证q’‘’(t)加加速度尽可能小。</p><p><strong>（3）时间对齐</strong></p><p>录制的视频帧率为30Hz，模拟运行的频率为120Hz，因此在训练之前，需要进行时间对齐，以模拟频率对机器人动作q(t)进行采样。</p><h3 id="2-2-3-模仿学习">2.2.3 模仿学习</h3><p>本文使用转换后的演示进行模仿学习，不使用行为克隆方法，而是使用模仿学习算法，并将演示合并到强化学习中。</p><p>本文考虑使用 $&lt;S, A, P, R, \gamma&gt;$ 的马尔科夫决策链，其中：</p><ul><li>$S$: 状态空间</li><li>$A$: 动作空间</li><li>$P(s_{t+1}|s_t, a_t)$: 是状态转移函数，在$t+1$步，给定动作$a_t$时，下一个状态$s_{t+1}$的概率密度</li><li>$R(S, a)$: 奖励函数</li><li>$\gamma$: 折扣因子</li></ul><p>强化学习的目的是最大化策略$\pi(a|s)$下的预期奖励。给定演示轨迹${(s_i, a_i)}^n_{i=1}$，使用该轨迹和奖励，优化策略$\pi$。</p><p>本文使用<strong>生成对抗模仿学习GAIL</strong>，这是一种使用state-action动作密度匹配来学习策列的SOTA IL方法。通过最大限度的减小演示和动作的距离函数，实现动作模仿。</p><p>本文使用<strong>演示增强策略梯度</strong>进行强化学习。</p><h2 id="2-3-总结">2.3 总结</h2><ol><li>将真实世界人手姿态转换为机器人动作，加上物体位姿估计，作为演示数据用于后续模仿学习与强化学习。</li><li>使用生成对抗模仿学习GAIL</li><li>使用演示增强策略梯度进行强化学习</li></ol><h1>3 Learning and Retrieval from Prior Data for Skill-based Imitation Learning</h1><blockquote><p><strong>标题</strong>：基于技能的模仿学习的先验数据的学习和检索<br><strong>作者团队</strong>：The University of Texas at Austin<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://ut-austin-rpl.github.io/sailor">https://ut-austin-rpl.github.io/sailor</a></p></blockquote><h2 id="3-1-目标问题">3.1 目标问题</h2><p>模仿学习由于需要进行监督学习和较弱的泛化能力，因此可扩展性有限。</p><p>本文研究如何使用其他任务的先验数据，来稳定高效地学习新任务。</p><h2 id="3-2-方法">3.2 方法</h2><p>本文提出了一种基于技能的模仿学习框架，从先前的数据中提取运动技能，并随后调用这些学到的技能的目标函数策略。</p><p><strong>（1）学习可预测的技能表示</strong></p><p>通过使用变分自动编码器VAE编码小段轨迹来学习技能表示。为了提高技能的可预测性，将小段轨迹使用LSTM编码器编码为潜在技能的高斯分布，解码器也是一个LSTM网络。</p><p>对于每个时间步，将潜在特征z和给定的观察o，解码为动作a。</p><p><strong>（2）基于检索的策略学习</strong></p><p>为了提高任务策略的学习效果，从先前的数据集中检索与目标任务相关的数据。</p><p>策略学习阶段，使用LSTM策略通过观察历史的潜在技能，预测输出接下来要执行的技能z，这种方式可以利用丰富多样的先验交互知识，将其融入到策略中，从而在新环境中更有效的执行任务。</p><h2 id="3-3-总结">3.3 总结</h2><p>使用变分自动编码器VAE处理子轨迹，形成一致的潜在技能表示。</p><p>使用LSTM编码解码技能表示，根据状态预测需要输出的技能。</p><h1>4 VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors</h1><blockquote><p><strong>标题</strong>：VIOLA：使用对象提议先验进行基于视觉的操作的模仿学习<br><strong>作者团队</strong>：The University of Texas at Austin<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://ut-austin-rpl.github.io/VIOLA">https://ut-austin-rpl.github.io/VIOLA</a></p></blockquote><h2 id="4-1-目标问题">4.1 目标问题</h2><p>一种先进的模仿学习算法。</p><h2 id="4-2-方法">4.2 方法</h2><p>本文实现了一种基于视觉的机器人操作任务模仿学习算法：</p><ul><li>使用预训练的视觉模型生成通用的对象表示；</li><li>采用基于Transformer的策略，来推理这些表示，根据视觉预测动作；</li></ul><p><strong>（1）构建面向对象的表示</strong></p><ol><li>对象识别：使用预训练的RPN区域建议网络在工作空间图像上生成对象建议，并选择执行度最高的前K个建议。</li><li>对象特征表示：包含区域特征和上下文特征<ol><li>区域特征：为每个建议区域设计视觉特征和位置特征，使用ROI Align从工作空间图像的ResNet18编码的特征途中提取特征；</li><li>上下文特征，包括全局特征、手眼相机图像特征和机器人状态的本体感知特征组成；</li></ol></li><li>时间组合，将过去H+1步的特征和时间编码组合，构成面向对象的表示，来获得对象状态的时间依赖性和动态变化。</li></ol><p><strong>（2）基于Transformer的策略</strong></p><p>使用多个Transformer编码器，处理一系列特征向量。</p><p>该策略网络将面向对象的区域特征和上下文特征作为输入token，并加入了一个动作token，通过动作监督学习，能够关注任务相关的区域。</p><p>最后利用两层全连接层和高斯混合模型输出动作。</p><h2 id="4-3-总结">4.3 总结</h2><ol><li>构建面向对象的特征（图像区域特征+机器人状态特征）</li><li>使用Transformer进行特征编码与预测</li><li>使用高斯混合模型输出动作。</li></ol><h1>5 SEIL: Simulation-augmented Equivariant Imitation Learning</h1><blockquote><p><strong>标题</strong>：SEIL: Simulation-augmented Equivariant Imitation Learning<br><strong>作者团队</strong>：Northeastern University<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://saulbatman.github.io/project/seil/">https://saulbatman.github.io/project/seil/</a></p></blockquote><h2 id="5-1-目标问题">5.1 目标问题</h2><p>机器人模仿学习中，样本获取非常困难，因为需要与现实世界进行交互。为了解决这个问题，本文在图像数据增强的基础上进行模仿学习。</p><h2 id="5-2-方法">5.2 方法</h2><p><strong>（1）数据增强</strong></p><p>本文实现了一种专家数据增强方法Transition Simulation。通过将观察到的点云投影到模拟的机械臂姿态中，生成新的观察图像，从而模拟专家的state-action来增加数据多样性。</p><p><strong>（2）等变行为克隆</strong></p><p>利用机器人操作过程中的O(2)对称性（所有平面旋转和反射对称性），利用Steerable CNNs将策略自动泛化到不同的O(2)状态中。</p><h2 id="5-3-总结">5.3 总结</h2><p>专家数据增强+等变行为克隆</p><h1>6 Waypoint-Based Imitation Learning for Robotic Manipulation</h1><blockquote><p><strong>标题</strong>：基于路标点的机器人操作模仿学习<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></p></blockquote><h2 id="6-1-目标问题">6.1 目标问题</h2><p>模仿学习容易出现错误，特别是在执行复杂操作时。</p><p>使用路标点可以减少模仿学习中的错误，但是一般路标点需要认为监督，本文则是提出了一种自动生成路标点的方法。提高了模仿学习的成功率。</p><h2 id="6-2-方法">6.2 方法</h2><p>本文设计了一种自动路标点提取方法，这是一个预处理模块，因此很容易添加到其它行为克隆算法中。本文举例了两种先进的模仿学习算法：扩散策略和基于Transformer的动作。</p><p><strong>（1）重建损失</strong></p><p>定义了一种损失：给定路标点后重建轨迹的质量，通过最小化原始轨迹与根据路标点重建的轨迹之间的偏差，来使得路标点能尽可能代替真实轨迹。</p><p><strong>（2）路径点选择动态规划</strong></p><p>使用一种简单而动态规划算法，用于选择最小数量的路径点，并保证重建的误差满足要求。</p><h2 id="6-3-总结">6.3 总结</h2><p>一种自动路标点提取算法，用于对行为克隆数据进行预处理。</p>]]></content>
    
    
    <summary type="html">基于传统的强化学习方法大多实现某项任务，而无法实现动作的模仿，调研相关论文实现机器人动作轨迹的模仿。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【实验准备】MuJoCo导入自定义机器人</title>
    <link href="https://www.mahaofei.com/post/f67206dd.html"/>
    <id>https://www.mahaofei.com/post/f67206dd.html</id>
    <published>2023-12-04T01:59:28.000Z</published>
    <updated>2023-12-04T01:59:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本实验所配置的模型已开源至 <a href="https://github.com/HaofeiMa/E05_Robotiq-2f-85">Github: HaofeiMa/E05_Robotiq-2f-85</a></p></blockquote><h1>1 模型准备</h1><h2 id="1-1-机器人Solidworks模型转URDF-2">1.1 机器人Solidworks模型转URDF</h2><p>本仿真实验使用<a href="https://www.hansrobot.com/service/download/3dmoxing?pagenum=3">大族E05机器人</a>和<a href="https://robotiq.com/products/2f85-140-adaptive-robot-gripper">Robotiq 2f-85夹爪</a></p><p>该部分参考<a href="https://www.bilibili.com/video/BV1Tx411o7rH">以下视频</a></p><iframe src="//player.bilibili.com/player.html?aid=56651666&bvid=BV1Tx411o7rH&cid=98972250&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="height:100%;width:100%; aspect-ratio: 16 / 9;"> </iframe><p>首先下载机器人模型，并转换为URDF。</p><p><strong>（1）安装sw_urdf_exporter插件</strong></p><p>下载sw_urdf_exporter插件：<a href="http://wiki.ros.org/sw_urdf_exporter">http://wiki.ros.org/sw_urdf_exporter</a>，注意下载最新的就行，最新的也支持以前版本的Solidworks。</p><p>关闭 Solidworks。</p><p>运行 <a href="https://github.com/ros/solidworks_urdf_exporter/releases">sw2urdfSetup.exe</a>，自行安装即可。</p><p><strong>（2）为机器人添加基准轴（旋转轴）</strong></p><p><img src="https://img.mahaofei.com/img/202312041343070.png" alt="image.png"></p><p>以此选择六个圆柱面，确定六个旋转轴方向。</p><p><img src="https://img.mahaofei.com/img/202312041401387.png" alt="image.png"></p><p><strong>（3）导出URDF</strong></p><p>查看是否有【工具-最下面File-Export as URDF】，如果有的话，直接点击打开，如果没有，则打开【工具-插件】，在最下面打开Sw2URDF插件的两个√。</p><p>按照以下过程，创建base_link和link1-6</p><p><img src="https://img.mahaofei.com/img/202312041434677.png" alt="image.png"></p><p>然后点击Preview and Export</p><p><img src="https://img.mahaofei.com/img/202312041439265.png" alt="image.png"></p><p>然后点击Next和Export URDF and Mesh，它会将我们的URDF模型以功能包的形式保存到设置的位置。</p><p><strong>注意创建完成后，一定要检查最后一个坐标系是否是在机器人末端连接法兰的中心，因为后续添加夹爪需要这个坐标系，如果不是，需要自己手动调整坐标系的位置，重新生成URDF</strong></p><h2 id="1-2-ROS中查看模型-2">1.2 ROS中查看模型</h2><p>创建一个工作空间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p catkin_robot/src</span><br><span class="line">cd catkin_robot/src</span><br><span class="line">catkin_init_workspace</span><br></pre></td></tr></table></figure><p>将功能包复制到<code>src</code>目录下</p><p>编译工作空间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ../..</span><br><span class="line">catkin_make</span><br></pre></td></tr></table></figure><p>运行测试程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source devel/setup.bash</span><br><span class="line">roslaunch e05 display.launch</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202312041606763.png" alt="image.png"></p><h2 id="1-3-添加-Robotiq-2f-85-夹爪-2">1.3 添加 Robotiq 2f-85 夹爪</h2><p><strong>（1）准备夹爪环境</strong></p><p>进入工作空间的src目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd src</span><br><span class="line">git clone https://github.com/ros-industrial/robotiq.git</span><br></pre></td></tr></table></figure><p>在自己的机械臂的功能包的urdf文件夹中，新建一个<code>common.gazebo.xacro</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd src/e05/urdf</span><br><span class="line">gedit common.gazebo.xacro</span><br></pre></td></tr></table></figure><p>添加以下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">robot</span> <span class="attr">xmlns:xacro</span>=<span class="string">&quot;http://wiki.ros.org/xacro&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">gazebo</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span> <span class="attr">name</span>=<span class="string">&quot;ros_control&quot;</span> <span class="attr">filename</span>=<span class="string">&quot;libgazebo_ros_control.so&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">gazebo</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">robot</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>为了方便表示，我在<code>e05.urdf</code>最后添加了一个<code>ee_link</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">name</span>=<span class="string">&quot;ee_link&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">visual</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">rpy</span>=<span class="string">&quot;0 0 0&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">sphere</span> <span class="attr">radius</span>=<span class="string">&quot;0.01&quot;</span> /&gt;</span> <span class="comment">&lt;!-- You can use any simple geometry like a sphere for visualization --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">material</span> <span class="attr">name</span>=<span class="string">&quot;&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">color</span> <span class="attr">rgba</span>=<span class="string">&quot;1 0 0 1&quot;</span> /&gt;</span> <span class="comment">&lt;!-- Choose a color for visualization --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">material</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">link</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">&quot;ee_joint&quot;</span> <span class="attr">type</span>=<span class="string">&quot;fixed&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">rpy</span>=<span class="string">&quot;0 0 0&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">parent</span> <span class="attr">link</span>=<span class="string">&quot;link6&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">child</span> <span class="attr">link</span>=<span class="string">&quot;ee_link&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">joint</span>&gt;</span></span><br></pre></td></tr></table></figure><p>再新建一个<code>xacro</code>文件，（例如我的机械臂功能包名字为e05）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit e05.xacro</span><br></pre></td></tr></table></figure><p>添加如下内容，注意修改<code>Gazebo支持</code>和<code>E05机械臂</code>部分自己的机械臂功能包名称，以及<code>夹爪与机械臂连接</code>部分的第一行的parent，我这里连接在了link6也就是末端上。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">robot</span> <span class="attr">xmlns:xacro</span>=<span class="string">&quot;http://www.ros.org/wiki/xacro&quot;</span> <span class="attr">name</span>=<span class="string">&quot;e05&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:arg</span> <span class="attr">name</span>=<span class="string">&quot;transmission_hw_interface&quot;</span> <span class="attr">default</span>=<span class="string">&quot;hardware_interface/PositionJointInterface&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- E05机械臂 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:include</span> <span class="attr">filename</span>=<span class="string">&quot;$(find e05)/urdf/e05.urdf&quot;</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Gazebo 支持 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:include</span> <span class="attr">filename</span>=<span class="string">&quot;$(find e05)/urdf/common.gazebo.xacro&quot;</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 加载gazebo中需要使用的模型 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- macros for transmission --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:macro</span> <span class="attr">name</span>=<span class="string">&quot;transmission_block&quot;</span> <span class="attr">params</span>=<span class="string">&quot;joint_name&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">transmission</span> <span class="attr">name</span>=<span class="string">&quot;tran1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">type</span>&gt;</span>transmission_interface/SimpleTransmission<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">&quot;$&#123;joint_name&#125;&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">hardwareInterface</span>&gt;</span>hardware_interface/PositionJointInterface<span class="tag">&lt;/<span class="name">hardwareInterface</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">joint</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">actuator</span> <span class="attr">name</span>=<span class="string">&quot;motor1&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">hardwareInterface</span>&gt;</span>hardware_interface/PositionJointInterface<span class="tag">&lt;/<span class="name">hardwareInterface</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mechanicalReduction</span>&gt;</span>1<span class="tag">&lt;/<span class="name">mechanicalReduction</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">actuator</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">transmission</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">xacro:macro</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Transmissions for ros control --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:transmission_block</span> <span class="attr">joint_name</span>=<span class="string">&quot;joint1&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:transmission_block</span> <span class="attr">joint_name</span>=<span class="string">&quot;joint2&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:transmission_block</span> <span class="attr">joint_name</span>=<span class="string">&quot;joint3&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:transmission_block</span> <span class="attr">joint_name</span>=<span class="string">&quot;joint4&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:transmission_block</span> <span class="attr">joint_name</span>=<span class="string">&quot;joint5&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:transmission_block</span> <span class="attr">joint_name</span>=<span class="string">&quot;joint6&quot;</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- &lt;xacro:include filename=&quot;$(find e05)/urdf/e05.gazebo.xacro&quot; /&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 机器人固定在世界坐标系下 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">name</span>=<span class="string">&quot;world&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">&quot;world_joint&quot;</span> <span class="attr">type</span>=<span class="string">&quot;fixed&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">parent</span> <span class="attr">link</span>=<span class="string">&quot;world&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span> <span class="attr">link</span> = <span class="string">&quot;base_link&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">&quot;0.0 0.0 0&quot;</span> <span class="attr">rpy</span>=<span class="string">&quot;0.0 0.0 0.0&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">joint</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Robotiq 2F-85夹爪 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:include</span> <span class="attr">filename</span>=<span class="string">&quot;$(find robotiq_2f_85_gripper_visualization)/urdf/robotiq_arg2f_85_macro.xacro&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:include</span> <span class="attr">filename</span>=<span class="string">&quot;$(find robotiq_85_description)/urdf/robotiq_85_gripper.urdf.xacro&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">gazebo</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span> <span class="attr">name</span>=<span class="string">&quot;gazebo_grasp_fix&quot;</span> <span class="attr">filename</span>=<span class="string">&quot;libgazebo_grasp_fix.so&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">arm</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- &lt;arm_name&gt;应该是一个单独的名字，不能和别的任何关节同名 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">arm_name</span>&gt;</span>ur5_gripper<span class="tag">&lt;/<span class="name">arm_name</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- &lt;palm_link&gt;是和手指相连的关节 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">palm_link</span>&gt;</span>link6<span class="tag">&lt;/<span class="name">palm_link</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- &lt;gripper_link&gt;是会检测碰撞的关节 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">gripper_link</span>&gt;</span>gripper_finger1_finger_tip_link<span class="tag">&lt;/<span class="name">gripper_link</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">gripper_link</span>&gt;</span>gripper_finger2_finger_tip_link<span class="tag">&lt;/<span class="name">gripper_link</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">gripper_link</span>&gt;</span>gripper_finger2_knuckle_link<span class="tag">&lt;/<span class="name">gripper_link</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">gripper_link</span>&gt;</span>gripper_finger1_knuckle_link<span class="tag">&lt;/<span class="name">gripper_link</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">gripper_link</span>&gt;</span>gripper_finger1_inner_knuckle_link<span class="tag">&lt;/<span class="name">gripper_link</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">gripper_link</span>&gt;</span>gripper_finger2_inner_knuckle_link<span class="tag">&lt;/<span class="name">gripper_link</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">arm</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">forces_angle_tolerance</span>&gt;</span>150<span class="tag">&lt;/<span class="name">forces_angle_tolerance</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 检测频率 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">update_rate</span>&gt;</span>130<span class="tag">&lt;/<span class="name">update_rate</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 检测为抓取状态的接触次数阈值 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">grip_count_threshold</span>&gt;</span>2<span class="tag">&lt;/<span class="name">grip_count_threshold</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">max_grip_count</span>&gt;</span>8<span class="tag">&lt;/<span class="name">max_grip_count</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 释放时的容忍度，超过这个就会把物体放下。数值越大，需要把夹爪打开更大才能释放物体 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">release_tolerance</span>&gt;</span>0.005<span class="tag">&lt;/<span class="name">release_tolerance</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">disable_collisions_on_attach</span>&gt;</span>true<span class="tag">&lt;/<span class="name">disable_collisions_on_attach</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">contact_topic</span>&gt;</span>__default_topic__<span class="tag">&lt;/<span class="name">contact_topic</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">gazebo</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 将夹爪实例化，并设置夹爪和机械臂的关系（连接在tool0上） --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xacro:robotiq_85_gripper</span> <span class="attr">prefix</span>=<span class="string">&quot;&quot;</span> <span class="attr">parent</span>=<span class="string">&quot;ee_link&quot;</span> &gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">rpy</span>=<span class="string">&quot;0 $&#123;-pi/2&#125; 0&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">xacro:robotiq_85_gripper</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 相机实例化，然后设置仿真位置 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- &lt;xacro:include filename=&quot;$(find realsense_ros_gazebo)/xacro/depthcam.xacro&quot;/&gt;</span></span><br><span class="line"><span class="comment">    &lt;xacro:realsense_d435 sensor_name=&quot;d435&quot; parent_link=&quot;tool0&quot; rate=&quot;10&quot;&gt;</span></span><br><span class="line"><span class="comment">        &lt;origin rpy=&quot;0 $&#123;-pi/2&#125; 0 &quot; xyz=&quot;-0.1 0 0&quot;/&gt;</span></span><br><span class="line"><span class="comment">    &lt;/xacro:realsense_d435&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">robot</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改原来的launch文件，这里我为了后续方便，将原来的<code>display.launch</code>重命名了<code>display_e05_with_gripper.launch</code>，这里主要修改<code>&lt;param name=&quot;robot_description&quot; command=&quot;$(find xacro)/xacro '$(find e05)/urdf/e05.xacro'&quot; /&gt;  </code>这一行，添加刚刚创建的xacro文件。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">launch</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;model&quot;</span> /&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- &lt;param name=&quot;robot_description&quot; textfile=&quot;$(find e05)/urdf/e05.xacro&quot; /&gt; --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">&quot;robot_description&quot;</span> <span class="attr">command</span>=<span class="string">&quot;$(find xacro)/xacro &#x27;$(find e05)/urdf/e05.xacro&#x27;&quot;</span> /&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">name</span>=<span class="string">&quot;joint_state_publisher_gui&quot;</span> <span class="attr">pkg</span>=<span class="string">&quot;joint_state_publisher_gui&quot;</span> <span class="attr">type</span>=<span class="string">&quot;joint_state_publisher_gui&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">name</span>=<span class="string">&quot;robot_state_publisher&quot;</span> <span class="attr">pkg</span>=<span class="string">&quot;robot_state_publisher&quot;</span> <span class="attr">type</span>=<span class="string">&quot;robot_state_publisher&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">name</span>=<span class="string">&quot;rviz&quot;</span> <span class="attr">pkg</span>=<span class="string">&quot;rviz&quot;</span> <span class="attr">type</span>=<span class="string">&quot;rviz&quot;</span> <span class="attr">args</span>=<span class="string">&quot;-d $(find e05)/urdf.rviz&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">launch</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>（2）Rviz中查看机器人</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd catkin_motion</span><br><span class="line">catkin_make</span><br><span class="line">source devel/setup.bash</span><br><span class="line">roslaunch e05 display_e05_with_gripper.launch</span><br></pre></td></tr></table></figure><p>左下角Add，添加机器人模型RobotModel。</p><p>左侧Fixed Frame选择base_link，即可看到机器人了。</p><p><img src="https://img.mahaofei.com/img/202312051357909.png" alt="image.png"></p><p><strong>（3）Gazebo中查看机器人</strong></p><p>创建一个<code>gazebo_e05_with_gripper.launch</code>文件，内容参考如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">launch</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;gui&quot;</span> <span class="attr">default</span>=<span class="string">&quot;true&quot;</span> <span class="attr">doc</span>=<span class="string">&quot;Starts gazebo gui&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;paused&quot;</span> <span class="attr">default</span>=<span class="string">&quot;false&quot;</span> <span class="attr">doc</span>=<span class="string">&quot;Starts gazebo in paused mode&quot;</span> /&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 启动仿真环境 后续有环境修改，可以替换此处的worlds/empty.world，改为例如&quot;$(find ur_gazebo)/worlds/table_custom.world&quot; --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">include</span> <span class="attr">file</span>=<span class="string">&quot;$(find gazebo_ros)/launch/empty_world.launch&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;world_name&quot;</span> <span class="attr">default</span>=<span class="string">&quot;$(find gazebo_ros)/launch/empty_world.world&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;paused&quot;</span> <span class="attr">value</span>=<span class="string">&quot;$(arg paused)&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;gui&quot;</span> <span class="attr">value</span>=<span class="string">&quot;$(arg gui)&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 加载TF --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">name</span>=<span class="string">&quot;tf_footprint_base&quot;</span> <span class="attr">pkg</span>=<span class="string">&quot;tf&quot;</span> <span class="attr">type</span>=<span class="string">&quot;static_transform_publisher&quot;</span> <span class="attr">args</span>=<span class="string">&quot;0 0 0 0 0 0 base_link base_footprint 40&quot;</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 启动机器人 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">include</span> <span class="attr">file</span>=<span class="string">&quot;$(find e05)/launch/display_e05_with_gripper.launch&quot;</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 将 robot_description 发送到 gazebo 中生成机器人 --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- &lt;node name=&quot;spawn_model&quot; pkg=&quot;gazebo_ros&quot; type=&quot;spawn_model&quot; args=&quot;-file $(find e05)/urdf/e05.urdf -urdf -model e05&quot; output=&quot;screen&quot; /&gt; --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">name</span>=<span class="string">&quot;spawn_gazebo_model&quot;</span> <span class="attr">pkg</span>=<span class="string">&quot;gazebo_ros&quot;</span> <span class="attr">type</span>=<span class="string">&quot;spawn_model&quot;</span> <span class="attr">args</span>=<span class="string">&quot;-urdf -param robot_description -model robot -z 0&quot;</span> <span class="attr">respawn</span>=<span class="string">&quot;false&quot;</span> <span class="attr">output</span>=<span class="string">&quot;screen&quot;</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">name</span>=<span class="string">&quot;fake_joint_calibration&quot;</span> <span class="attr">pkg</span>=<span class="string">&quot;rostopic&quot;</span> <span class="attr">type</span>=<span class="string">&quot;rostopic&quot;</span> <span class="attr">args</span>=<span class="string">&quot;pub /calibrated std_msgs/Bool true&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">launch</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此时启动<code>launch</code>文件，可以看到gazebo环境中机器人。</p><p><img src="https://img.mahaofei.com/img/202312051949512.png" alt="image.png"></p><hr><p><strong>下面的创建moveit驱动，在mujoco中不需要，请直接跳到 2 模型导入mujoco</strong></p><h2 id="1-4-创建MoveIt驱动-2">1.4 创建MoveIt驱动</h2><p>此部分参考<a href="https://blog.csdn.net/Tepmoe/article/details/119533792">该文章</a>。</p><p>启动moveit设置助手</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosrun moveit_setup_assistant moveit_setup_assistant</span><br></pre></td></tr></table></figure><p>选择Create New Moveit Configuration Package，选择自己的<code>e05.xacro</code>文件，点击<code>Load Files</code></p><p><img src="https://img.mahaofei.com/img/202312051459862.png" alt="image.png"></p><p>左侧第二个的<code>Self-Collisions</code>是检查碰撞，一定将所有有可能发生碰撞的都勾选上，不然后续会出错。</p><p><img src="https://img.mahaofei.com/img/202312051948359.png" alt="image.png"></p><p>第三个<code>Virtual Joints</code>一般也不需要。</p><p>第四个<code>Planning Groups</code>是最重要的，我们需要设置，点击<code>Add Group</code>，分别配置机器人和末端夹爪。</p><p>机械臂：</p><ul><li>Group Name一般填manipulator就行</li><li>运动学求解器，选择kdl</li><li>路径规划算法，默认选择RRT Star就行</li><li>点击<code>Add Kin. Chain</code>，Baselink选择 base_link，Tiplink选择ee_link</li></ul><p><img src="https://img.mahaofei.com/img/202312051507765.png" alt="image.png"></p><p>末端夹爪的Group Name填上gripper，其它的都不用选。</p><p>第五个<code>Robot Poses</code>可以添加一些常用位置，便于我们快速使机器人运动到这些位置，例如</p><ul><li>up：机器人初始的竖直向上</li><li>pick：机器人准备夹取</li><li>open：夹爪打开</li><li>close：夹爪关闭</li></ul><p><img src="https://img.mahaofei.com/img/202312051513180.png" alt="image.png"></p><p>第六个<code>End Effectors</code>，按下图设置就行</p><p><img src="https://img.mahaofei.com/img/202312051533929.png" alt="image.png"></p><p>点击<code>Controllers</code>，点击左上角自动生成</p><p><img src="https://img.mahaofei.com/img/202312051534681.png" alt="image.png"></p><p>点击倒数第二个<code>Author Information</code>，填写名字和邮箱，不一定是真实的，但是不填无法生成功能包</p><p>最后<code>Generate Package</code>就可以了（在src目录下新建一个<code>e05_moveit</code>文件夹，选择此文件夹生成）</p><p>测试rviz是否能控制机器人</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">catkin_make</span><br><span class="line">source devel/setup.bash</span><br><span class="line">roslaunch e05_moveit demo.launch</span><br></pre></td></tr></table></figure><p>测试gazebo是否能联动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roslaunch e05_moveit demo_gazebo.launch</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202312052212812.png" alt="image.png"></p><p>测试时遇到了<strong>Rviz中的机械臂可以正常做规划和执行，但是Gazebo中机械臂没有反应</strong>的问题，解决方法参考<a href="https://blog.csdn.net/qq_50598558/article/details/114702163">此文章</a></p><h1>2 模型导入Mujoco</h1><h2 id="2-1-修改URDF">2.1 修改URDF</h2><p>在现有模型的xacro或urdf中的开头，添加下面的tag</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mujoco</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">compiler</span> </span></span><br><span class="line"><span class="tag"><span class="attr">meshdir</span>=<span class="string">&quot;../meshes_mujoco/&quot;</span> </span></span><br><span class="line"><span class="tag"><span class="attr">balanceinertia</span>=<span class="string">&quot;true&quot;</span> </span></span><br><span class="line"><span class="tag"><span class="attr">discardvisual</span>=<span class="string">&quot;false&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mujoco</span>&gt;</span></span><br></pre></td></tr></table></figure><p>创建一个文件夹<code>e05/meshes_mujoco</code>，将所有模型的stl文件放到这个文件夹下。</p><p>从xacro文件生成urdf文件的命令（如果有urdf文件则不需要此步）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosrun xacro xacro --inorder e05.xacro &gt; e05.urdf</span><br></pre></td></tr></table></figure><p>检查urdf文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_urdf e05.urdf</span><br></pre></td></tr></table></figure><p>在RViz中可视化：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roslaunch e05 display.launch model:=path/to/your/urdf/file</span><br></pre></td></tr></table></figure><h2 id="2-2-生成基本模型">2.2 生成基本模型</h2><p>在确认URDF模型没有问题后，进入MuJoCo的可执行文件夹内执行命令进行转换</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.mujoco/mujoco210/bin</span><br></pre></td></tr></table></figure><p>官方说明可以转换成三种模型<code>.mjb/.txt/.xml</code>，我们一般用xml的格式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./compile /path/to/model.urdf /path/to/model.xml</span><br></pre></td></tr></table></figure><p>测试生成的基本模型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./simulate /path/to/model.xml</span><br></pre></td></tr></table></figure><p>可以看到我们的机器人出现在了仿真环境中，虽然会发现我们的模型直接瘫倒，但是这是因为没有添加actuator等配置。</p><p><img src="https://img.mahaofei.com/img/202312060913373.png" alt="image.png"></p><p>由于夹爪出现问题，一直无法使用，因此替换了<a href="https://github.com/vikashplus/robotiq_sim">vikashplus/robotiq_sim</a>的夹爪。</p><p>添加完成后的<code>e05_mujoco.xml</code>代码如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mujoco</span> <span class="attr">model</span>=<span class="string">&quot;e05&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">compiler</span> <span class="attr">angle</span>=<span class="string">&quot;radian&quot;</span> <span class="attr">meshdir</span>=<span class="string">&quot;../meshes_mujoco/&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">size</span> <span class="attr">njmax</span>=<span class="string">&quot;500&quot;</span> <span class="attr">nconmax</span>=<span class="string">&quot;100&quot;</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 加载 E05 机械臂模型与基本配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">asset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;base_link&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/base_link.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;link1&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/link1.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;link2&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/link2.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;link3&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/link3.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;link4&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/link4.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;link5&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/link5.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">name</span>=<span class="string">&quot;link6&quot;</span> <span class="attr">file</span>=<span class="string">&quot;e05/link6.STL&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">texture</span> <span class="attr">type</span>=<span class="string">&quot;skybox&quot;</span> <span class="attr">builtin</span>=<span class="string">&quot;gradient&quot;</span> <span class="attr">rgb1</span>=<span class="string">&quot;0.3 0.5 0.7&quot;</span> <span class="attr">rgb2</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">width</span>=<span class="string">&quot;512&quot;</span> <span class="attr">height</span>=<span class="string">&quot;3072&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;texture builtin=&quot;flat&quot; height=&quot;1278&quot; mark=&quot;cross&quot; markrgb=&quot;1 1 1&quot; name=&quot;texgeom&quot; random=&quot;0.01&quot; rgb1=&quot;0.8 0.6 0.4&quot; rgb2=&quot;0.8 0.6 0.4&quot; type=&quot;cube&quot; width=&quot;127&quot;/&gt; --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;material name=&quot;geom&quot; texture=&quot;texgeom&quot; texuniform=&quot;true&quot;/&gt; --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">texture</span> <span class="attr">type</span>=<span class="string">&quot;2d&quot;</span> <span class="attr">name</span>=<span class="string">&quot;groundplane&quot;</span> <span class="attr">builtin</span>=<span class="string">&quot;checker&quot;</span> <span class="attr">mark</span>=<span class="string">&quot;edge&quot;</span> <span class="attr">rgb1</span>=<span class="string">&quot;0.2 0.3 0.4&quot;</span> <span class="attr">rgb2</span>=<span class="string">&quot;0.1 0.2 0.3&quot;</span></span></span><br><span class="line"><span class="tag">      <span class="attr">markrgb</span>=<span class="string">&quot;0.8 0.8 0.8&quot;</span> <span class="attr">width</span>=<span class="string">&quot;300&quot;</span> <span class="attr">height</span>=<span class="string">&quot;300&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">material</span> <span class="attr">name</span>=<span class="string">&quot;groundplane&quot;</span> <span class="attr">texture</span>=<span class="string">&quot;groundplane&quot;</span> <span class="attr">texuniform</span>=<span class="string">&quot;true&quot;</span> <span class="attr">texrepeat</span>=<span class="string">&quot;5 5&quot;</span> <span class="attr">reflectance</span>=<span class="string">&quot;0.2&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">asset</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">default</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">default</span> <span class="attr">class</span>=<span class="string">&quot;E05&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">joint</span> <span class="attr">damping</span>=<span class="string">&#x27;200&#x27;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">default</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">default</span> <span class="attr">class</span>=<span class="string">&quot;E05e&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">joint</span> <span class="attr">damping</span>=<span class="string">&#x27;100&#x27;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">default</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">default</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 加载 Robotiq-2f-85 夹爪与基本配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span> <span class="attr">file</span>=<span class="string">&quot;robotiq-2f-85-assets.xml&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 机器人本体 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">worldbody</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 仿真环境设置定义 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;body pos=&quot;-0.16 0.90 0.02&quot; euler=&quot;4.71238898 0 3.14159265&quot;&gt;</span></span><br><span class="line"><span class="comment">            &lt;body euler=&quot;0 -0.82 0&quot;&gt;</span></span><br><span class="line"><span class="comment">                &lt;camera name=&quot;workbench_camera&quot; euler=&quot;0 0 0&quot; fovy=&quot;60&quot; pos=&quot;0 0 0.5&quot; /&gt;</span></span><br><span class="line"><span class="comment">            &lt;/body&gt;</span></span><br><span class="line"><span class="comment">        &lt;/body&gt; --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">light</span> <span class="attr">diffuse</span>=<span class="string">&quot;0.6 0.6 0.6&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 3&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">light</span> <span class="attr">diffuse</span>=<span class="string">&quot;0.6 0.6 0.6&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 -0.3 3&quot;</span> <span class="attr">dir</span>=<span class="string">&quot;0 0.2 -0.8&quot;</span> <span class="attr">directional</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;light cutoff=&quot;100&quot; diffuse=&quot;0.5 0.5 0.5&quot; dir=&quot;-0 0 -1.3&quot; directional=&quot;true&quot; exponent=&quot;1&quot; pos=&quot;0 0 1.3&quot; specular=&quot;.1 .1 .1&quot;/&gt; --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">geom</span> <span class="attr">name</span>=<span class="string">&quot;floor&quot;</span> <span class="attr">size</span>=<span class="string">&quot;0 0 0.05&quot;</span> <span class="attr">type</span>=<span class="string">&quot;plane&quot;</span> <span class="attr">material</span>=<span class="string">&quot;groundplane&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 整个运动链 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.501961 0.501961 0.501961 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;base_link&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.501961 0.501961 0.501961 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;base_link&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">body</span> <span class="attr">name</span>=<span class="string">&quot;link1&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0.0735&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">inertial</span> <span class="attr">pos</span>=<span class="string">&quot;-0.0218175 -1.34618e-05 0.0953928&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.696024 -0.147787 -0.14991 0.686467&quot;</span> <span class="attr">mass</span>=<span class="string">&quot;2.58559&quot;</span> <span class="attr">diaginertia</span>=<span class="string">&quot;0.0143616 0.0141793 0.00506835&quot;</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">joint</span> <span class="attr">class</span>=<span class="string">&quot;E05&quot;</span> <span class="attr">name</span>=<span class="string">&quot;joint1&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 1&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.772549 0.752941 0.733333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link1&quot;</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.772549 0.752941 0.733333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link1&quot;</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">body</span> <span class="attr">name</span>=<span class="string">&quot;link2&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0.1465&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.5 -0.5 -0.5 0.5&quot;</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">inertial</span> <span class="attr">pos</span>=<span class="string">&quot;4.42659e-06 -0.15814 0.130501&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.491565 0.508215 -0.508335 0.491606&quot;</span> <span class="attr">mass</span>=<span class="string">&quot;1.21311&quot;</span> <span class="attr">diaginertia</span>=<span class="string">&quot;0.0159807 0.0154676 0.00149746&quot;</span> /&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">joint</span> <span class="attr">class</span>=<span class="string">&quot;E05&quot;</span> <span class="attr">name</span>=<span class="string">&quot;joint2&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 -1&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> /&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.792157 0.819608 0.933333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link2&quot;</span> /&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.792157 0.819608 0.933333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link2&quot;</span> /&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">body</span> <span class="attr">name</span>=<span class="string">&quot;link3&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 -0.38 0&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.707107 0 0 0.707107&quot;</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">inertial</span> <span class="attr">pos</span>=<span class="string">&quot;-0.0414786 1.14213e-05 0.0194164&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.601987 0.371892 0.373044 0.600122&quot;</span> <span class="attr">mass</span>=<span class="string">&quot;1.53239&quot;</span> <span class="attr">diaginertia</span>=<span class="string">&quot;0.00608826 0.00587729 0.00215529&quot;</span> /&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">joint</span> <span class="attr">class</span>=<span class="string">&quot;E05&quot;</span> <span class="attr">name</span>=<span class="string">&quot;joint3&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 -1&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> /&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.866667 0.866667 0.890196 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link3&quot;</span> /&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.866667 0.866667 0.890196 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link3&quot;</span> /&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">body</span> <span class="attr">name</span>=<span class="string">&quot;link4&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.5 -0.5 0.5 -0.5&quot;</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">inertial</span> <span class="attr">pos</span>=<span class="string">&quot;3.8238e-05 -0.0589656 -0.257081&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.988248 -0.152858 -0.000415939 -0.000860755&quot;</span> <span class="attr">mass</span>=<span class="string">&quot;0.475189&quot;</span> <span class="attr">diaginertia</span>=<span class="string">&quot;0.00538322 0.0052745 0.000778119&quot;</span> /&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">joint</span> <span class="attr">class</span>=<span class="string">&quot;E05e&quot;</span> <span class="attr">name</span>=<span class="string">&quot;joint4&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 1&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> /&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.772549 0.752941 0.733333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link4&quot;</span> /&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.772549 0.752941 0.733333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link4&quot;</span> /&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">body</span> <span class="attr">name</span>=<span class="string">&quot;link5&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 -0.42&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.707105 -0.707108 0 0&quot;</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">inertial</span> <span class="attr">pos</span>=<span class="string">&quot;1.42418e-06 0.0365906 -0.0159722&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.527641 0.849468 0.000113207 -7.90953e-05&quot;</span> <span class="attr">mass</span>=<span class="string">&quot;0.811919&quot;</span> <span class="attr">diaginertia</span>=<span class="string">&quot;0.00229898 0.00217056 0.000742684&quot;</span> /&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">joint</span> <span class="attr">class</span>=<span class="string">&quot;E05e&quot;</span> <span class="attr">name</span>=<span class="string">&quot;joint5&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 1&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> /&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.792157 0.819608 0.933333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link5&quot;</span> /&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.792157 0.819608 0.933333 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link5&quot;</span> /&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">body</span> <span class="attr">name</span>=<span class="string">&quot;link6&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0.155 0&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.707105 -0.707108 0 0&quot;</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">inertial</span> <span class="attr">pos</span>=<span class="string">&quot;-4.84703e-06 0.00109667 -0.0032406&quot;</span> <span class="attr">quat</span>=<span class="string">&quot;0.999926 0.0120786 2.3554e-05 -0.00120144&quot;</span> <span class="attr">mass</span>=<span class="string">&quot;0.75038&quot;</span> <span class="attr">diaginertia</span>=<span class="string">&quot;0.00129697 0.00122959 0.00051245&quot;</span> /&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">joint</span> <span class="attr">class</span>=<span class="string">&quot;E05e&quot;</span> <span class="attr">name</span>=<span class="string">&quot;joint6&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 -1&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> /&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.776471 0.756863 0.737255 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link6&quot;</span> /&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">geom</span> <span class="attr">type</span>=<span class="string">&quot;mesh&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;0.776471 0.756863 0.737255 1&quot;</span> <span class="attr">mesh</span>=<span class="string">&quot;link6&quot;</span> /&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">geom</span> <span class="attr">size</span>=<span class="string">&quot;0.01&quot;</span> <span class="attr">contype</span>=<span class="string">&quot;0&quot;</span> <span class="attr">conaffinity</span>=<span class="string">&quot;0&quot;</span> <span class="attr">group</span>=<span class="string">&quot;1&quot;</span> <span class="attr">rgba</span>=<span class="string">&quot;1 0 0 1&quot;</span> /&gt;</span></span><br><span class="line">                                </span><br><span class="line">                                <span class="comment">&lt;!-- 导入 Robotiq 2f-85 运动链 --&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">include</span> <span class="attr">file</span>=<span class="string">&quot;robotiq-2f-85-chain.xml&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">worldbody</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">actuator</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- E05 机械臂驱动 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">position</span> <span class="attr">name</span>=<span class="string">&quot;joint1&quot;</span> <span class="attr">ctrllimited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">ctrlrange</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> <span class="attr">joint</span>=<span class="string">&quot;joint1&quot;</span> <span class="attr">kp</span>=<span class="string">&quot;4000&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">position</span> <span class="attr">name</span>=<span class="string">&quot;joint2&quot;</span> <span class="attr">ctrllimited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">ctrlrange</span>=<span class="string">&quot;-1.57 1.57&quot;</span> <span class="attr">joint</span>=<span class="string">&quot;joint2&quot;</span> <span class="attr">kp</span>=<span class="string">&quot;3000&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">position</span> <span class="attr">name</span>=<span class="string">&quot;joint3&quot;</span> <span class="attr">ctrllimited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">ctrlrange</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> <span class="attr">joint</span>=<span class="string">&quot;joint3&quot;</span> <span class="attr">kp</span>=<span class="string">&quot;3000&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">position</span> <span class="attr">name</span>=<span class="string">&quot;joint4&quot;</span> <span class="attr">ctrllimited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">ctrlrange</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> <span class="attr">joint</span>=<span class="string">&quot;joint4&quot;</span> <span class="attr">kp</span>=<span class="string">&quot;2000&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">position</span> <span class="attr">name</span>=<span class="string">&quot;joint5&quot;</span> <span class="attr">ctrllimited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">ctrlrange</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> <span class="attr">joint</span>=<span class="string">&quot;joint5&quot;</span> <span class="attr">kp</span>=<span class="string">&quot;2000&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">position</span> <span class="attr">name</span>=<span class="string">&quot;joint6&quot;</span> <span class="attr">ctrllimited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">ctrlrange</span>=<span class="string">&quot;-3.14159 3.14159&quot;</span> <span class="attr">joint</span>=<span class="string">&quot;joint6&quot;</span> <span class="attr">kp</span>=<span class="string">&quot;2000&quot;</span>/&gt;</span>     </span><br><span class="line"><span class="tag">&lt;/<span class="name">actuator</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mujoco</span>&gt;</span></span><br></pre></td></tr></table></figure><p>添加完成后效果如下：</p><p><img src="https://img.mahaofei.com/img/202312061457266.png" alt="image.png"></p><p>整理文件结构，添加桌子、小方块，移动机器人位置和相机视角，最后得到下面的仿真环境。具体代码已开源至<a href="https://github.com/HaofeiMa/E05_Robotiq-2f-85">Github</a>:</p><p><img src="https://img.mahaofei.com/img/202312062141059.png" alt="image.png"></p><h1>3 Mujoco 常用命令</h1><h2 id="3-1-模型加载与初始化配置">3.1 模型加载与初始化配置</h2><p>模型加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = mujoco_py.load_model_from_path(<span class="string">&quot;path/to/.xml&quot;</span>)</span><br></pre></td></tr></table></figure><p>创建mujoco仿真实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sim = mujoco_py.MjSim(model)</span><br></pre></td></tr></table></figure><p>渲染设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建渲染器</span></span><br><span class="line">viewer = mujoco_py.MjViewer(sim)</span><br><span class="line"><span class="comment"># 设置模拟环境视角</span></span><br><span class="line">lookat = [<span class="number">1.27998563</span>, <span class="number">0.68635066</span>, <span class="number">0.55350562</span>]</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">viewer.cam.lookat[idx] = lookat[idx]</span><br><span class="line">viewer.cam.distance = <span class="number">1.4547035766991275</span></span><br><span class="line">viewer.cam.azimuth = <span class="number">134.95215311004816</span></span><br><span class="line">viewer.cam.elevation = -<span class="number">32.488038277512022</span></span><br></pre></td></tr></table></figure><p>初始姿态设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">initial_qpos = &#123;</span><br><span class="line"><span class="string">&#x27;joint1&#x27;</span>: np.pi/<span class="number">2</span>,</span><br><span class="line"><span class="string">&#x27;joint2&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">&#x27;joint3&#x27;</span>: np.pi/<span class="number">2</span>,</span><br><span class="line"><span class="string">&#x27;joint4&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">&#x27;joint5&#x27;</span>: np.pi/<span class="number">2</span>,</span><br><span class="line"><span class="string">&#x27;joint6&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">&#x27;robotiq_2f_85_right_driver_joint&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;object0:joint&#x27;</span>: [<span class="number">1</span>, <span class="number">0.45</span>, <span class="number">0.425</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, value <span class="keyword">in</span> initial_qpos.items():</span><br><span class="line">sim.data.set_joint_qpos(name, value)</span><br><span class="line">sim.forward()</span><br></pre></td></tr></table></figure><p>主函数调用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">sim.step()</span><br><span class="line">viewer.render()</span><br></pre></td></tr></table></figure><h2 id="3-2-基本信息查询与设置">3.2 基本信息查询与设置</h2><p><strong>（1）body</strong></p><p>打印<code>body</code>位置与姿态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">body_idx = sim.model.body_name2id(<span class="string">&quot;link6&quot;</span>)</span><br><span class="line"><span class="comment"># 打印 body 坐标</span></span><br><span class="line"><span class="built_in">print</span>(sim.data.body_xpos[body_idx])</span><br><span class="line"><span class="comment"># 打印 body 四元数</span></span><br><span class="line"><span class="built_in">print</span>(sim.data.body_xquat[body_idx])</span><br></pre></td></tr></table></figure><p><strong>（2）site</strong></p><p>打印<code>site</code>位置与姿态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sim.data.get_site_xpos(<span class="string">&#x27;site_name&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sim.data.get_site_xquat(<span class="string">&#x27;site_name&#x27;</span>))</span><br></pre></td></tr></table></figure><p><strong>（3）joint</strong></p><p>打印<code>joint</code>值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">joint_idx = model.joint_name2id(<span class="string">&quot;joint_name&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sim.data.qpos[model.jnt_qposadr[joint_idx]])</span><br></pre></td></tr></table></figure><p>设置<code>joint</code>值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sim.data.set_joint_qpos(<span class="string">&quot;joint_name&quot;</span>, value)</span><br></pre></td></tr></table></figure><p><strong>（4）actuator</strong></p><p>打印<code>actuator</code>状态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sim.data.ctrl)</span><br></pre></td></tr></table></figure><p>设置<code>actuator</code>值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sim.data.ctrl[actuator_index] = value</span><br></pre></td></tr></table></figure><p><strong>（5）mocap</strong></p><p>打印<code>mocap</code>位置和姿态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sim.data.mocap_pos)</span><br><span class="line"><span class="built_in">print</span>(sim.data.mocap_quat)</span><br></pre></td></tr></table></figure><p>设置<code>mocap</code>位置和姿态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一</span></span><br><span class="line">sim.data.mocap_pos[:] = np.array([x, y, z])</span><br><span class="line">sim.data.mocap_quat[:] = np.array([quat_1, quat_2, quat_3, quat_4])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line">sim.data.set_mocap_pos(<span class="string">&#x27;mocap_name&#x27;</span>, np.array([x, y, z]))</span><br><span class="line">sim.data.set_mocap_quat(<span class="string">&#x27;mocap_name&#x27;</span>, np.array([quat_1, quat_2, quat_3, quat_4]))</span><br></pre></td></tr></table></figure><blockquote><p>参考链接：</p><ol><li><a href="https://www.bilibili.com/video/BV1Tx411o7rH">SUES木鸢机甲工作室. SolidWorks模型导出urdf （古月居老师）. Bilibili</a></li><li><a href="https://zhuanlan.zhihu.com/p/99991106">Robot Learning. MuJoCo的机器人建模. 知乎</a></li><li><a href="https://roboti.us/forum/index.php?resources/">MuJoCo官方论坛</a></li><li><a href="http://www.mujoco.org/book/index.html">MoJoCo官方文档</a></li><li><a href="https://github.com/vikashplus/robotiq_sim">vikashplus/robotiq_sim.git</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">将自定义机器人模型导入进Mujoco中，为后续强化学习搭建仿真环境基础。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MuJoCo" scheme="https://www.mahaofei.com/tags/MuJoCo/"/>
    
  </entry>
  
  <entry>
    <title>【笔记工具】Markdown语法与Obsidian编辑器</title>
    <link href="https://www.mahaofei.com/post/37a3baf2.html"/>
    <id>https://www.mahaofei.com/post/37a3baf2.html</id>
    <published>2023-11-29T12:34:55.000Z</published>
    <updated>2023-11-29T12:34:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 Markdown</h1><h2 id="1-1-什么是-Markdown">1.1 什么是 Markdown</h2><p>Markdown 是一种是目前互联网上最流行的写作语言，它使用一些简单的符号来标记文本格式，其简洁的语法、优美的格式以及强大的软件支持深受广大网友的喜爱。</p><p>Markdown 使用易读易写的纯文本格式编写文档，然后转换成有效的 HTML 文档。它的学习门槛很低，你可能只需花几分钟就可以入门了。</p><p>目前基本上所有的博客网站、在线文档等都支持 markdown 格式，尤其广泛应用于程序员领域，例如 Github 中的所有说明文档一般都是 <code>.md</code> 格式。</p><h2 id="1-2-为什么使用-Markdown">1.2 为什么使用 Markdown</h2><p>Markdown 的出现主要是为了解决以下通点：</p><ul><li><strong>软件不兼容</strong>：例如不同版本的word、或者不同软件如office与wps，打开同一文档，可能有的正常，有的格式错误，有的乱码。</li><li><strong>排版浪费时间</strong>：功能越多，我们就容易想得越多，遇到的问题也会越多。对于专注于写作的场景，有时可能并不需要那么多选项格式，例如行高行距字体颜色等等。而是要关注内容。同时word中每次写完一段文字，都要再手动为它选择格式，多的这一步操作可能就会打断思路。而markdown可以让你仅通过键盘，在写文档的同时完成格式的设置。</li><li><strong>格式不兼容</strong>：复制同一段文字到另一个文档中，很有可能格式全乱了，同理复制到网页或从其他地方复制过来，格式也都是很麻烦的问题。</li></ul><p>为了解决这些问题，Markdown 实现了以下功能：</p><ul><li><strong>语法简单</strong>：只需要一些简单地标记符号，例如<code># * &gt; - [] () =</code></li><li><strong>兼容性强</strong>：所有的编辑器都可以打开 <code>.md</code> 文件，甚至记事本和vim都可以编辑</li><li><strong>导出方便</strong>：可以导出PDF、也可以嵌入HTML、Latex等格式</li><li><strong>专注内容</strong>：写作时双手无需脱离键盘，再也不用纠结排版问题</li></ul><h2 id="1-3-Markdown-怎么用">1.3 Markdown 怎么用</h2><p>虽然所有的编辑器都可以编辑 Markdown 文件，但是为了更加直观，并且让我们有更好地写作体验，这里推荐两款所见即所得的 Markdown 编辑神器：<a href="https://obsidian.md/">Obsidian</a>和<a href="https://typora.io/">Typora</a>。前者是我现在一直在用的笔记软件，作为我的知识库管理工具，后者在我写作初期使用过，编辑体验也非常不错。</p><p>下图为 Obsidian：</p><p><img src="https://img.mahaofei.com/img/202311292058718.png" alt="image.png"></p><p>下图为 Typora：</p><p><img src="https://img.mahaofei.com/img/202311292059046.png" alt="image.png"></p><p>打开自己的 Markdown 编辑器，新建一个文件，就可以开始自己的创作了。</p><h2 id="1-4-Markdown-基本语法">1.4 Markdown 基本语法</h2><p>以下是一些比较常用的 Markdown 语法，按照我的常用顺序排列：</p><h3 id="1-标题">1. 标题</h3><p>使用方式为：<code>#</code> + <code>空格</code> + <code>标题文字</code>，注意<code>#</code>号与<code>标题文字</code>之间的<code>空格</code>：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 一级标题</span></span><br><span class="line"><span class="section">## 二级标题</span></span><br><span class="line"><span class="section">### 三级标题</span></span><br><span class="line"><span class="section">#### 四级标题</span></span><br><span class="line"><span class="section">##### 五级标题</span></span><br><span class="line"><span class="section">###### 六级标题</span></span><br></pre></td></tr></table></figure><h1>一级标题</h1><h2 id="二级标题">二级标题</h2><h3 id="三级标题">三级标题</h3><h4 id="四级标题">四级标题</h4><h5 id="五级标题">五级标题</h5><h6 id="六级标题">六级标题</h6><h3 id="2-倾斜">2. 倾斜</h3><p>使用方式：<code>*被倾斜的文字*</code>或者<code>_被倾斜的文字_</code>，例如：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里是正常文字，<span class="emphasis">*这里是被倾斜的文字，但是不能换行，如果要对多段倾斜，需要每一段使用倾斜标记*</span>，这里又是正常文字</span><br><span class="line"></span><br><span class="line"><span class="emphasis">_这样也是倾斜的_</span></span><br></pre></td></tr></table></figure><p>这里是正常文字，<em>这里是被倾斜的文字，但是不能换行，如果要对多段倾斜，需要每一段使用倾斜标记</em>，这里又是正常文字</p><p><em>这样也是倾斜的</em></p><h3 id="3-加粗">3. 加粗</h3><p>使用方式：<code>**被加粗的文字**</code>或<code>__被加粗的文字__</code>，例如：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里是正常文字，<span class="strong">**这里是被加粗的文字，但是不能换行，如果要对多段加粗，需要每一段使用加粗标记**</span>，这里又是正常文字</span><br><span class="line"></span><br><span class="line"><span class="strong">__这样也是加粗的__</span></span><br></pre></td></tr></table></figure><p>这里是正常文字，<strong>这里是被加粗的文字，但是不能换行，如果要对多段加粗，需要每一段使用加粗标记</strong>，这里又是正常文字</p><p><strong>这样也是加粗的</strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">**<span class="emphasis">*当然倾斜和加粗可以一起使用，那样就是三个星号*</span>**</span></span><br><span class="line"></span><br><span class="line"><span class="strong">__<span class="emphasis">_也可以是三个下划线_</span>__</span></span><br></pre></td></tr></table></figure><p><em><strong>当然倾斜和加粗可以一起使用，那样就是三个星号</strong></em></p><p><em><strong>也可以是三个下划线</strong></em></p><h3 id="4-高亮">4. 高亮</h3><p>使用方式如下</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里是正常文字，==这里是被高亮的文字，但是不能换行，如果要对多段高亮，需要每一段使用高亮标记==，这里又是正常文字</span><br></pre></td></tr></table></figure><p>这里是正常文字，==这里是被加粗的文字，但是不能换行，如果要对多段加粗，需要每一段使用加粗标记==，这里又是正常文字</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">**<span class="emphasis">*==当然倾斜和加粗可以一起使用，就像这样==*</span>**</span></span><br><span class="line"></span><br><span class="line">==<span class="strong">**<span class="emphasis">*也可以是这样*</span>**</span>==</span><br><span class="line"></span><br><span class="line">==<span class="strong">__<span class="emphasis">_也可以是下划线_</span>__</span>==</span><br></pre></td></tr></table></figure><p>==<em><strong>当然倾斜和加粗可以一起使用，那样就是三个星号</strong></em>==</p><p>==<em><strong>也可以是这样</strong></em>==</p><p>==<em><strong>也可以是下划线</strong></em>==</p><h3 id="5-列表">5. 列表</h3><p><strong>无序列表</strong></p><p>使用<code>+ -</code>标记无序列表，使用方式为<code>+/-</code>+<code>空格</code>+<code>文字</code>，注意<code>+/-</code>号与<code>文字</code>之间的<code>空格</code>。</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 这是无序列表的第一项</span><br><span class="line"><span class="bullet">-</span> 这是第二项</span><br><span class="line"><span class="bullet">-</span> 这是第三项</span><br></pre></td></tr></table></figure><ul><li>这是无序列表的第一项</li><li>这是第二项</li><li>这是第三项</li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">+</span> 这是无序列表的第一项</span><br><span class="line"><span class="bullet">+</span> 这是第二项</span><br><span class="line"><span class="bullet">+</span> 这是第三项</span><br></pre></td></tr></table></figure><ul><li>这是无序列表的第一项</li><li>这是第二项</li><li>这是第三项</li></ul><p><strong>有序列表</strong></p><p>使用方式为<code>数字</code>+<code>.</code>+<code>空格</code>+<code>文字</code>。</p><p>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 第一项</span><br><span class="line">2. 第二项</span><br><span class="line">3. 第三项</span><br></pre></td></tr></table></figure><ol><li>第一项</li><li>第二项</li><li>第三项</li></ol><h3 id="6-引用">6. 引用</h3><p>使用<code>&gt;</code>标记引用的内容，使用方式为<code>&gt;</code> + <code>空格</code> + <code>引用文字</code>，注意<code>&gt;</code>号与<code>文字</code>之间的<code>空格</code>。</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt; 这是被引用的内容</span></span><br></pre></td></tr></table></figure><blockquote><p>这是被引用的内容</p></blockquote><h3 id="7-代码">7. 代码</h3><p>代码分为行内代码和代码块。</p><p><strong>行内代码</strong>，使用方式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里是正常文字`code`文字继续</span><br></pre></td></tr></table></figure><p>这里是正常文字<code>code</code>文字继续</p><p><strong>代码块</strong>，使用方式如下：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">```代码格式（c, c++, python, html, java, css, yaml, shell ...)</span></span><br><span class="line"><span class="code">代码内容</span></span><br><span class="line"><span class="code">```</span>ㅤㅤ</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">```python</span></span><br><span class="line"><span class="code">import numpy as np</span></span><br><span class="line"><span class="code">np.array([1, 2, 3])</span></span><br><span class="line"><span class="code">```</span>ㅤㅤ</span><br></pre></td></tr></table></figure><p>效果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><h3 id="8-链接">8. 链接</h3><p>链接的使用格式如下：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">链接显示文字</span>](<span class="link">链接地址</span>)</span><br><span class="line"></span><br><span class="line">[<span class="string">马浩飞丨博客</span>](<span class="link">https://www.mahaofei.com</span>)</span><br></pre></td></tr></table></figure><p><a href="https://www.mahaofei.com">马浩飞丨博客</a></p><h3 id="9-图片">9. 图片</h3><p>图片的使用格式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">![](图片地址)</span><br><span class="line">![](https://img.mahaofei.com/img/202311292058718.png)</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202311292058718.png" alt=""></p><h3 id="10-分割线">10. 分割线</h3><p>分割线使用三个以上的<code>*</code>或<code>-</code>或<code>_</code>组成，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">分割线之上</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">或者使用</span><br><span class="line"></span><br><span class="line">***</span><br><span class="line"></span><br><span class="line">再或者</span><br><span class="line"></span><br><span class="line">___</span><br><span class="line"></span><br><span class="line">就这些</span><br></pre></td></tr></table></figure><p>分割线之上</p><hr><p>或者使用</p><hr><p>再或者</p><hr><p>就这些</p><h3 id="11-表格">11. 表格</h3><p>表格的使用可能稍微有些繁琐，如果是在Obsidian中，可以有些插件简化表格的创建方式。</p><p>常规使用方式如下，注意中间的<code>|-|-|-|</code>是不能删的，表格有几列就写几个<code>|-|</code>：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">|表头1|表头2|表头3|</span><br><span class="line">|-|-|-|</span><br><span class="line">|表格内容1|表格内容2|表格内容3|</span><br><span class="line">|表格内容4|表格内容5|表格内容6|</span><br></pre></td></tr></table></figure><table><thead><tr><th>表头1</th><th>表头2</th><th>表头3</th></tr></thead><tbody><tr><td>表格内容1</td><td>表格内容2</td><td>表格内容3</td></tr><tr><td>表格内容4</td><td>表格内容5</td><td>表格内容6</td></tr></tbody></table><blockquote><p>哦对，补充一下，为了表示直观，markdown使用空行分割两个段落</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一段</span><br><span class="line"></span><br><span class="line">第二段</span><br></pre></td></tr></table></figure></blockquote><h1>2 Obsidian</h1><h2 id="2-1-为什么使用-Obsidian">2.1 为什么使用 Obsidian</h2><p><strong>Obsidian 很适合用来管理多层级多文件，因此很适合用来构建知识库</strong></p><p>最初我使用 Obsidian 是因为我写的文章越来越多，当时使用 Typora 管理起来感觉不是很方便，而且那时正逢 Typora 更新至1.0后开始收费，而 Obsidian 开始展露头角，因此我尝试使用了 Obsidian，从此在我的所有设备的dock栏上，一定会有一个 Obsidian，只要我在使用电脑，Obsidian一定是处于打开状态的。</p><p>下图是我现在的 Obsidian 图</p><ul><li>左边是文件列表、历史打开文件</li><li>右边是任务列表和日历（也可以切换为文档目录）</li></ul><p><img src="https://img.mahaofei.com/img/202311292140615.png" alt="image.png"></p><p>我自己的博客内容全部使用Obsidian存储，同时记录我的一些笔记和平时遇到的各种Bug，这样遇到相似问题时，直接使用全局搜索，就可以快速找到解决办法，亲测已经帮我解决了好多次问题，节约了大把时间。</p><p><strong>个人不放心云笔记，Obsidian 是纯本地化文件编辑与管理</strong></p><p>由于各种原因，我不想将我自己记录的一些内容全部放到云笔记上，一旦宕机或跑路，会是一件非常麻烦的问题，例如有道云笔记、印象笔记等等，OneNote等我也用过，但是迁移性太差，如果想换别的笔记软件，之前记录的东西根本不能导出。</p><p>而Obsidian则是使用的本地化文件管理，左侧的文件列表就是本地中真实存在的一个个文件夹和<code>.md</code>文件。在多设备同步方面，推荐坚果云或者OneDrive实现多设备的实时同步。</p><p><strong>Obsidian 的功能非常丰富，是一个笔记软件，又不止是一个笔记软件</strong></p><p>对于我来说，我使用 Obsidian 完成了很多功能，当然最主要的还是记笔记，包括学习笔记、Bug 笔记、组会记录等等。</p><p>同时使用<code>custom frames</code>插件能够将网页嵌入到 Obsidian 中，我将滴答清单加入了进来，这样就实现了上图右侧的任务管理功能。</p><p>此外我还写了一个自动化脚本，根据我的笔记库的内容修改，每天定时更新博客内容。</p><p><strong>Obsidian 颜值非常高，有各种各样主题，而且几乎所有的样式格式都可以自定义修改</strong></p><p>在外观这方面，我也折腾了一段时间，最终还是使用 <code>Blue Topaz</code>主题，辅以<code>Style Settings</code> 插件中的一些个性化设置。总的来说没有修改太多。</p><p>网上也有一些开源的高度修改的库模板，下载下来就可以直接用，但是我找了一圈，发现自己还是更喜欢简约直接的，各位可以根据自己的审美找找看。</p><h2 id="2-2-Obsidian-的特色功能">2.2 Obsidian 的特色功能</h2><p>这里只介绍大致功能，具体使用方法，请参考 <a href="https://publish.obsidian.md/help-zh/%E7%94%B1%E6%AD%A4%E5%BC%80%E5%A7%8B">Obsidian 官方教程</a>，或者下载Obsidian软件后，第一次打开会有示例库，我当时就是阅读那个学习的。</p><p><strong>双向链接</strong></p><p>不确定 Obsidian 是不是第一个双链笔记软件，但它也是最早的一批了。</p><p>双向链接，顾名思义就是双向引用的链接，例如我在A文章中使用<code>[[B文章的标题或者小标题]]</code>，就可以实现文章和文章之间的链接，有点类似于网页链接，但是它是在笔记软件内跳转的。</p><p>在被引用的笔记中，你也可以看到这篇笔记被哪些其它笔记引用了。例如可能某篇<code>[[Ubuntu系统Buglist]]</code>中的内容就可能被我在写其它实验笔记的时候引用很多次。</p><p>这样所有的笔记相互链接，就会形成一个庞大的关系图谱，Obsidian中也可以查看这个关系图谱。</p><p>此外，双向链接还可以使用<code>![[文件名]]</code>或<code>![[文件名#小节标题]]</code>这样的格式，在引用的同时，将那篇笔记的全部或某一节插入到当前文章中，例如：</p><p><img src="https://img.mahaofei.com/img/202311292210694.png" alt="image.png"></p><p><strong>模板</strong></p><p>Obsidian 提供了模板的功能。</p><p>我创建了一个文件夹保存所有的模板。当我新建笔记后，可以直接插入现有的模板，例如我经常使用的论文笔记模板</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># X 论文名称</span></span><br><span class="line"></span><br><span class="line"><span class="quote">&gt; <span class="strong">**标题**</span>：</span></span><br><span class="line"><span class="quote">&gt; <span class="strong">**作者团队**</span>：</span></span><br><span class="line"><span class="quote">&gt; <span class="strong">**期刊会议**</span>：</span></span><br><span class="line"><span class="quote">&gt; <span class="strong">**时间**</span>：</span></span><br><span class="line"><span class="quote">&gt; <span class="strong">**代码**</span>：</span></span><br><span class="line"></span><br><span class="line"><span class="section">## X.1 目标问题</span></span><br><span class="line"></span><br><span class="line"><span class="section">## X.2 方法</span></span><br><span class="line"></span><br><span class="line"><span class="section">## X.3 思考</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202311292215167.png" alt="image.png"></p><p><strong>工作区保存与快速恢复</strong></p><p>有的时候我可能会同时打开好几个笔记，比如实验笔记、论文笔记、Buglist等等，我会把他们分屏分开放在不同的位置，例如这样：</p><p><img src="https://img.mahaofei.com/img/202311292218056.png" alt="image.png"></p><p>但是此时如果我需要去做另一件事，比如开会，我就需要把当前的这些笔记都关掉，打开会议记录，但是下次想要在恢复这个布局又比较麻烦。</p><p>这时候就可以点击左侧的<code>管理工作空间布局按钮</code>，保存工作空间布局，然后就可以放心关闭了，下次使用点击加载工作空间布局就可以一键恢复之前的布局。</p><p><strong>丰富的插件系统</strong></p><p>Obsidian 提供了丰富的插件系统，有些可以增加 Obsidian 的功能，有些则是美化外观，有些可以提高编辑体验。</p><p>具体哪些插件我用过比较好用的，后续会持续在本文下面更新。</p><p><strong>录音</strong></p><p>写这篇文章，在查资料的时候突然发现Obsidian出了录音功能，在【设置-核心插件】里打开即可使用，打开后会在主页面最左侧添加一个图标，按下即可开始录音。</p><p>录音完成后，会保存在当前笔记文件中。</p><p><img src="https://img.mahaofei.com/img/202311292224701.png" alt="image.png"></p><h2 id="2-3-Obsidian-插件推荐">2.3 Obsidian 插件推荐</h2><p>所有的插件都可以在插件市场中直接搜索下载</p><p>后续有时间再更新各个插件的介绍和使用详情</p><h3 id="1-提升编辑体验">1. 提升编辑体验</h3><p><strong>Media Extended</strong></p><blockquote><p>Github: <a href="https://github.com/aidenlx/media-extended">https://github.com/aidenlx/media-extended</a></p></blockquote><p>Media Extended增强了媒体播放功能，包括链接到在线视频，速度控制，循环播放，字幕支持。</p><p>以及创建时间戳链接笔记，从在线视频、本地视频、录音中获取时间戳等等。</p><p><img src="https://www.youtube.com/watch?v=yBCD-Hfpd8U" alt=""></p><p><strong>Outliner</strong></p><blockquote><p>Github: <a href="https://github.com/vslinko/obsidian-outliner">https://github.com/vslinko/obsidian-outliner</a></p></blockquote><p>增强列表样式</p><ul><li>可以上移下移缩进列表</li><li>增加列表垂直缩进线</li><li>增加折叠展开功能</li></ul><p><strong>Advanced Tables</strong></p><blockquote><p>Github: <a href="https://github.com/tgrosinger/advanced-tables-obsidian">https://github.com/tgrosinger/advanced-tables-obsidian</a></p></blockquote><p>优化创建表格的语法：输入一个<code>|</code>，然后输入第一个标题，接着按下<code>tab</code>，继续输入标题直到创建所有标题，然后按下<code>Enter</code>转到第一行输入内容，以此类推。</p><p>当光标位于表格中时：</p><table><thead><tr><th>快捷键</th><th>动作</th></tr></thead><tbody><tr><td><code>Tab</code></td><td>下一个单元格</td></tr><tr><td><code>Shift</code> + <code>Tab</code></td><td>上一个单元格</td></tr><tr><td><code>Enter</code></td><td>下一行</td></tr><tr><td><code>Ctrl</code> + <code>Shift</code> + <code>D</code></td><td>打开表格控件侧边栏</td></tr></tbody></table><p><img src="https://img.mahaofei.com/img/202311301613362.gif" alt=""></p><p><strong>Table Enhancer</strong></p><blockquote><p>Github: <a href="https://github.com/Stardusten/ob-table-enhancer">https://github.com/Stardusten/ob-table-enhancer</a></p></blockquote><p>不需要复杂的代码，使用可视化的方式创建和编辑表格。</p><ul><li>使用右键菜单创建表格</li><li>直接点击表格内容实时编辑预览</li></ul><p><img src="https://img.mahaofei.com/img/202311301654424.gif" alt=""></p><p><img src="https://img.mahaofei.com/img/202311301655235.gif" alt=""></p><h3 id="2-增加功能">2. 增加功能</h3><p><strong>Calendar</strong></p><blockquote><p>Github: <a href="https://github.com/liamcain/obsidian-calendar-plugin">https://github.com/liamcain/obsidian-calendar-plugin</a></p></blockquote><p>日历插件，打开后会在右边侧栏出现日历。</p><p>点击日历上的日期，可以跳转到当天的每日笔记中（每日笔记也可以设置模板）</p><p><img src="https://img.mahaofei.com/img/202311301615320.png" alt="image.png"></p><p><strong>Custom Frames</strong></p><blockquote><p>Github: <a href="https://github.com/Ellpeck/ObsidianCustomFrames">https://github.com/Ellpeck/ObsidianCustomFrames</a></p></blockquote><p>可以将任意的网页转换为obsidian窗格，非常不错。</p><p>例如我这里右侧的滴答清单：</p><p><img src="https://img.mahaofei.com/img/202311292140615.png" alt="image.png"></p><p><strong>Image auto upload Plugin</strong></p><blockquote><p>Github: <a href="https://github.com/renmu123/obsidian-image-auto-upload-plugin">https://github.com/renmu123/obsidian-image-auto-upload-plugin</a></p></blockquote><p>image auto upload插件搭配PicGo使用，能自动将ob中插入的图片上传到图床中，优化图片插件体验。</p><p>在这里我使用的是阿里云OSS存储我的图片，具体可以参考：<a href="https://www.mahaofei.com/post/d05a7f72.html">阿里云图床搭建方法<br></a>或者[[01_阿里云图床搭建方法]]</p><p><strong>Recent Files</strong></p><blockquote><p>Github: <a href="https://github.com/tgrosinger/recent-files-obsidian">https://github.com/tgrosinger/recent-files-obsidian</a></p></blockquote><p>非常直观的功能，就是在左侧文件列表增加最近打开的文件列表。</p><p><img src="https://img.mahaofei.com/img/202311301701536.png" alt="image.png"></p><p><strong>dataview</strong></p><p><strong>templater</strong></p><p><strong>admonition</strong></p><p><strong>quickadd</strong></p><p><strong>buttons</strong></p><p><strong>banners</strong></p><p><strong>workspaces plus</strong></p><p><strong>Hover Editor</strong></p><h3 id="3-个性化设置">3. 个性化设置</h3><p><strong>MySnippets</strong></p><blockquote><p>Github: <a href="https://github.com/chetachiezikeuzor/MySnippets-Plugin">https://github.com/chetachiezikeuzor/MySnippets-Plugin</a></p></blockquote><p>css片段管理插件，在右下角生成css管理菜单，点击后可方便的开关css片段。</p><p><img src="https://img.mahaofei.com/img/202312010757678.gif" alt=""></p><p><strong>Obsidian42 - BRAT</strong></p><blockquote><p>Github: <a href="https://github.com/TfTHacker/obsidian42-brat">https://github.com/TfTHacker/obsidian42-brat</a></p></blockquote><p>obsidian的插件下载大致可分为两种，第一种是从ob软件内的官方渠道下载，第二种则是通过Github等渠道下载后再手动安装。</p><p>Obsidian42 - BRAT 可以帮助你直接安装社区插件市场无法安装的插件。</p><p><strong>Style Settings</strong></p><blockquote><p>Github: <a href="https://github.com/mgmeyers/obsidian-style-settings">https://github.com/mgmeyers/obsidian-style-settings</a></p></blockquote><p>主题自定义设置插件，可以在现有主题的基础上，根据自己的需要进行魔改。</p><p><img src="https://img.mahaofei.com/img/202312010804210.png" alt="image.png"></p>]]></content>
    
    
    <summary type="html">介绍一种我自己一直在用的非常强大的笔记语法Markdown，以及基于markdown语法的编辑器Obsidian.</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】人机协作</title>
    <link href="https://www.mahaofei.com/post/a9bacf05.html"/>
    <id>https://www.mahaofei.com/post/a9bacf05.html</id>
    <published>2023-11-27T10:56:59.000Z</published>
    <updated>2023-11-27T10:56:59.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 Transfer Learning-enabled Action Recognition for Human-robot Collaborative Assembly</h1><blockquote><p><strong>标题</strong>：用于人机协作装配的迁移学习动作识别<br><strong>作者团队</strong>：香港理工大学（郑湃）<br><strong>期刊会议</strong>：CIRP<br><strong>时间</strong>：2021<br><strong>代码</strong>：</p></blockquote><h2 id="1-1-背景">1.1 背景</h2><p>随着现代制造业从大规模生产转向大量的个性化应用，工业机器人对自适应控制以及在共享工作空间中与人类无缝协作的需求不断增加。在柔性自动化的背景下，人机协作旨在将机器人的准确性和强度在人类的认知能力和灵活性相结合。</p><blockquote><p>With modern manufacturing shifting from mass production to mass personalization, industrial robots have been of rising demands for adaptive control and seamless cooperation with human operators in a shared workspace. In the context of flexible automation, human-robot collaboration (HRC) aims to integrate the accuracy and strength of robots with cognitive ability and flexibility of humans in the execution loop.</p></blockquote><p>实现这一目标的一个重要基础，就是机器人动态规划对人类活动和意图的安全的响应。因此人类的动作识别作为先决条件，在高效HRC中起着至关重要的作用。</p><blockquote><p>One major pillar to achieve this is that robots dynamically plan safe reactions responded to human activities and intentions.</p></blockquote><p>这可以在面向个性化定制的制造中带来更高的效率。</p><blockquote><p>which can result in higher overall productivity in customization-oriented manufacturing.</p></blockquote><h2 id="1-2-目标问题">1.2 目标问题</h2><p>人机协作(Human-robot collaboration, HRC)对于当今制造业的高柔性装配趋势至关重要。人类动作识别作为HRC的先决条件，使工业机器人能够理解人类意图并自适应的执行规划。</p><blockquote><p>Human-robot collaboration (HRC) is critical to today’s tendency towards high-flexible assembly in manufacturing. Human action recognition, as one of the core prerequisites for HRC, enables industrial robots to understand human intentions and to execute planning adaptively.</p></blockquote><p>目前的基于深度学习的动作识别方法严重依赖大量的注释数据，这在实际中并不有效且不现实。</p><p>因此本文提出了一种基于迁移学习的动作识别方法，帮助HRC装配。并引入了机器人规划决策机制。</p><h2 id="1-3-方法">1.3 方法</h2><p>本文提出了一种基于迁移学习的动作预测方法，以实现高效的HRC装配。系统包括三部分：（1）数据感知和预处理；（2）从采样视频中提取知识和动作识别；（3）机器人根据学到的语义知识做出决策和反应。</p><p><strong>（1）基于迁移学习的人类动作识别</strong></p><p>使用Kinect获取机器人的动作，通过Openpose工具箱获得人体姿态（该工具箱可以在连续视频中预测操作者的身体关节）。</p><p>本文提出的基于迁移学习的ST-GCN框架，包括三个模块：特征提取器、动作分类器和域自适应模块。</p><p><strong>（2）面向任务的自适应HRC装配</strong></p><p>预测的人类活动通过语义图转换为HRC装配中的机器人规划决策。通过将人体轨迹和深度相机的手眼标定，机器人可以获得真实的物理世界坐标，从而使机器人能够移动到精确的位置。因此机器人可以动态的协调人类并根据人类的子任务自适应的改变动作，最终实现面向任务的HRC装配。</p><h1>2 Vision-based holistic scene understanding towards proactive human–robot collaboration</h1><blockquote><p><strong>标题</strong>：基于视觉的整体场景理解，实现主动的人机协作<br><strong>作者团队</strong>：香港理工大学（郑湃）<br><strong>期刊会议</strong>：Robotics and Computer-Integrated Manufacturing<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><p>近年来，人机协作因其潜在的生产效率提高和大规模自定义能力而在制造领域引起了许多关注。 HRC将机器人的力量和准确性与人类的灵活性和创造力相结合，使人类操作员和机器人能够在共享工作空间中无缝工作并执行共享任务。</p><blockquote><p>Human–robot collaboration (HRC) has attracted many interests in recent years in the field of manufacturing because of the potential production efficiency improvement and mass personalization capability. The strength and accuracy of robots along with the flexibility and creativity of humans are combined in an HRC team, allowing human operators and robots to work seamlessly in a shared workspace with shared tasks.</p></blockquote><p>近年来，由于能够充分利用人类的灵活性和机器人精度的优势，人机协作已经成为制造业大规模个性化的一个有前途的方法。</p><blockquote><p>Recently human–robot collaboration (HRC) has emerged as a promising paradigm for mass personalization in manufacturing owing to the potential to fully exploit the strength of human flexibility and robot precision.</p></blockquote><p>为了实现更好的协作，机器人应该能够实现整体感知和解析工作场景的信息，从而主动规划并采取相应行动。目前HRC的相关工作虽然关注了人类的认知，但是缺乏对工作场景的其它关键要素的整体考虑。</p><p>为了解决这个问题，本文考虑物体、人类和环境的认识以及视觉推理，以收集视觉信息，并将其编译为语义，用于后续机器人的决策与协作。</p><h1>3 Dynamic Scene Graph for Mutual-Cognition Generation in Proactive Human-Robot Collaboration</h1><blockquote><p><strong>标题</strong>：主动人机协作中相互认知生成的动态场景图<br><strong>作者团队</strong>：香港理工大学（郑湃）<br><strong>期刊会议</strong>：CIRP Conference on Manufacturing Systems<br><strong>时间</strong>：<br><strong>代码</strong>：</p></blockquote><h2 id="3-1-背景">3.1 背景</h2><p>在先进制造业向工业5.0转型和在工业化过程中，人类在生产过程中发挥着核心作用。一方面，大规模个性化生产趋势对现代工厂的柔性制造提出了越来越高的要求，而这些要求目前还无法实现，只能依靠人的手动敏捷操作。此外，为了实现工业5.0的可持续性和弹性原则，产品的再利用和回收过程需要高水平的灵活性和自适应性的自动化技术。基于这种情况，人机协作引起了人们对灵活自动化任务的兴趣，该任务结合了人类和机器人的互补能力以提高生产力。</p><blockquote><p>Among the advanced manufacturing transition to Industry 5.0 and reindustrialization, human operators play a central role in the production process. For one side, the mass personalized production tendency raises ever-increasing flexible manufacturing requirements for modern factories, which remain unattained and rely on human manually agile operations. Besides, to achieve sustainability and resilience principles of Industry 5.0, the re-use, re-purpose and recycle processes of products demand high-level flexible and adaptable automation technologies. Motivated by this situation, human-robot collaboration (HRC) has elicited particular interest in flexiable automation tasks, which combines human and robotic complementing competencies for improved productivity.</p></blockquote><p>针对HRC系统，人们进行了大量的研究，以期在工业环境中实现人类技能和机器人操作的相互作用。例如，对人类的动作识别、工件的6-DOF位姿估计等。以及基于这些感知结果的机器人控制、 人类安全机制等等。</p><p>但是除了以上研究，HRC对于上下文感知的能力仍然停留在对周围环境的非语义感知。</p><h2 id="3-2-目标问题">3.2 目标问题</h2><p>人机协作在敏捷、灵活和以人为中心的制造向大规模个性化的转型中发挥着至关重要的作用。</p><p>现存问题：</p><ul><li>在当今的HRC任务中，无论是人类还是机器人都需要遵循命令和指示来进行协作活动，而不是主动、相互参与。</li><li>HRC的非语义感知阻碍了HRC系统中的主动规划和认知能力</li></ul><p>解决方法：<br>提出了一种基于动态场景图的方法，用于主动HRC应用中的相互认知生成。</p><ul><li>利用空间注意金字塔网络，检测工业环境中的对象（工件、机器人手臂、人手）。</li><li>利用链接预测模块构建HRC场景图，利用注意力图卷积网络来捕获工业零件、人类操作员和机器人操作之间的关系，并将人机协作处理结构连接推理为图嵌入，链接到人类操作和机器人主动指令的相互规划器。</li><li>在电池拆卸任务中进行了评估。</li></ul><h1>4 A visual reasoning-based approach for mutual-cognitive human-robot collaboration</h1><blockquote><p><strong>标题</strong>：基于视觉推理的人机交互认知协作方法<br><strong>作者团队</strong>：香港理工大学（郑湃）<br><strong>期刊会议</strong>：CIRP Annals-Manufacturing Technology<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="4-1-背景">4.1 背景</h2><p>在现代工厂中，许多复杂的机械产品的个性化生产，既依赖于机器人的精确操控，又依赖于人类的敏捷操作。在此背景下，人机协作HRC利用了人类的高灵活性和机器人的高效率和可靠性，引起了工业界和学术界的极大兴趣。人类和机器人具有互补的操作目标和能力，并在共享工作空间中协作执行制造任务，迄今为止，已经出现了大量的HRC解决方案。</p><blockquote><p>In modern factories, personalized production of many complicated mechanical products relies on both robots’ precision manipulation and human operators’ agile operations. In this context, human-robot collaboration (HRC) has attracted much interest from the industry and academia, which leverages humans’ high flexibility and robots’ high efficiency and reliability. Human and robotic agents have complementary operation goals and capabilities, and collaboratively conduct manufacturing tasks in a shared workspace. To date, numerous research efforts on HRC solutions have emerged.</p></blockquote><h2 id="4-2-目标问题">4.2 目标问题</h2><p>人机协作允许人类和机器人之间的无缝通信和协作，以在共享工作空间中完成灵活的制造任务。</p><p>现有的HRC系统缺乏机器人和人类认知的有效整合。</p><ul><li>现有的HRC系统上下文感知能力侧重于对环境的感知，而不是对任务过程的类人理解</li><li>现有的HRC系统直接将结果传递到反应控制中，很少考虑知识学习来进行主动路径规划</li><li>机器人执行和人类操作的规划器通常是预定义的，缺乏任务完成过程中的动态调整能力。</li></ul><p>本文提出了一种基于视觉推理的相互认知HRC方法：</p><ul><li>建立HRC知识图谱</li><li>视觉传感器将整体制造场景感知为时间图，通过图嵌入推断出具有相似指令的协作模式。</li><li>将相互认知决策融入到增强现实执行中。</li></ul><h1>5 Towards Mutual-Cognitive Human-Robot Collaboration: A Zero-shot Visual Reasoning Method</h1><blockquote><p><strong>标题</strong>：迈向相互认知的人机协作：零样本视觉推理方法<br><strong>作者团队</strong>：香港理工大学（郑湃）<br><strong>期刊会议</strong>：CASE<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="5-1-背景">5.1 背景</h2><p>工业5.0代表了制造业转型为以人为本、可持续和弹性的原则。迈向以人为本的智能制造，人机协作揭示了改善人类工作条件和确保一致质量的优势。HRC系统结合了人类灵活能力和机器人的自动化能力，对于制造任务，人类能够敏捷操作，而机器人同时执行重复且精确的操作，所有这些都朝着一个共同的目标努力。</p><blockquote><p>Industry 5.0 represents the principles of human-centricity, sustainability and resilience for manufacturing transformation. Towards human-centric smart manufacturing, Human-Robot Collaboration (HRC) sheds light on the benefits of improving human working conditions and ensuring consistent quality. HRC systems combine human’s flexible capabilities and robot automatic capabilities. For a manufacturing task, human operators take agile operations, while the robot concurrently executes repetitive and precise manipulation, all working towards a common goal.</p></blockquote><p>HRC的成功取决于多种因素，包括周围环境的感知、安全的机器人控制和双向通信。在此背景下，先进的计算机视觉技术促进了HRC系统在工业环境中的应用。例如，在HRC系统中跟踪人体运动以避免机器人碰撞并确保人体安全。与此同时，人们还探索了人类的行为、手势和声音，以实现无缝的人机通信。 HRC 场景中的感知信息在人类和机器人代理之间传输，以便在共享制造目标内进行有效协作。</p><blockquote><p>The success of HRC relies on various factors, including the perception of surrounding environments, safe robot control, and bidirectional communication. In this context, the advanced computer vision techniques facilitate applications of HRC systems in industrial settings. For example, human motions were tracked in HRC systems to avoid robot collision and ensure human safety. Meanwhile, human actions, gestures, and voices were explored to allow for seamless human-robot communication. The perceived information in HRC scenarios is transmitted among human and robotic agents for effective co-working within a shared manufacturing goal.</p></blockquote><p>然而，现有的HRC研究工作主要集中在感知层面，未能学习人机操作意图的语义知识并制定认知任务规划策略。在当前的 HRC 系统中，机器人可以跟随人类手势来反应性地规划运动并协助工人，而很少考虑 HRC 任务结构知识和随时间变化的团队合作目标。如果没有对任务的整体理解，HRC系统就无法减轻人类手动操作并增强机器人自适应辅助。此外，对于相似但不同的任务，HRC系统需要学习新的知识表示来规划合理的人类和机器人操作，这限制了HRC在实际情况中的应用。</p><blockquote><p>Nevertheless, existing research efforts on HRC focus on perception level, which fails to learn semantic knowledge of human-robot operation intentions and make cognitive task planning strategies. In current HRC systems, a robot can follow human gestures to reactively plan motions and assist the worker, while seldom considering HRC task structure knowledge and time-changing teamwork goals. Without a holistic understanding of tasks, the HRC system cannot relieve human manual operations and enhance robot adaptive assistance. Besides, for similar but different tasks, the HRC system needs to learn new knowledge representations to plan reasonable human and robotic operations, which limits the HRC applications in real cases.</p></blockquote><h2 id="5-2-目标问题">5.2 目标问题</h2><p>人机协作HRC在当今工业5.0所规定的以人为本的智能制造中显示出了广泛的应用潜力。</p><p>为了实现安全高效的协作，人们探索了多种视觉感知方法。使机器人能够感知周围环境并实现无碰撞的操作规划。</p><p>现存问题：</p><ul><li>目前的视觉感知方法只能传达机器人和人类之间的基本信息，缺乏语义知识（机器人遇到类似但未见过的情况，则无法顺利执行）</li></ul><p>解决方法：</p><ul><li>本文提出了一种基于相互认知的HRC架构，基于现场情况和任务结构的知识表示进行学习，规划人类和机器人的操作。</li><li>引入零样本视觉推理方法，从感知结果中得出机器人策略。</li><li>对老化电动汽车电池协同拆卸任务进行测试。</li></ul>]]></content>
    
    
    <summary type="html">阅读人机协作相关论文的笔记</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="人机协作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E4%BA%BA%E6%9C%BA%E5%8D%8F%E4%BD%9C/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="机器人" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="人机协作" scheme="https://www.mahaofei.com/tags/%E4%BA%BA%E6%9C%BA%E5%8D%8F%E4%BD%9C/"/>
    
    <category term="HRC" scheme="https://www.mahaofei.com/tags/HRC/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】Reskill基于技能的适应性动作空间学习</title>
    <link href="https://www.mahaofei.com/post/be8c51e1.html"/>
    <id>https://www.mahaofei.com/post/be8c51e1.html</id>
    <published>2023-11-25T08:00:39.000Z</published>
    <updated>2023-11-25T08:00:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><blockquote><p><strong>标题</strong>：剩余技能策略：学习基于技能的适应性行动空间，用于机器人强化学习<br><strong>作者团队</strong>：昆士兰科技大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://krishanrana.github.io/reskill">https://krishanrana.github.io/reskill</a></p></blockquote><h2 id="1-1-目标问题-8">1.1 目标问题</h2><p>基于技能的学习已经成为加速机器人学习的方法，技能从专家演示中提取，是短序列的单步操作（平移、抓取、抬起等动作），这些技能嵌入到潜在空间中，构成上层 RL 策略的行动空间。但是这种方式存在一些问题：</p><ul><li>对所有技能进行随机抽样探索，效率极低，因为其中只有一小部分技能与当前执行的任务相关，并且这些相关的技能通常不会聚集在技能空间的同一邻域内。</li><li>该方法假设技能是最优的，并且下层的任务来自于技能空间的相同分布，因此学习的通用性和变化适应性有限，例如从移动方块中学习技能，则无法应对障碍物、物体变化、不同摩擦等情况。</li></ul><p>为解决上述问题，本文提出了以下创新方法，称为残差技能策略（Residual Skill Policies，ReSkill）：</p><ul><li>状态条件技能先验：对相关技能进行采样来引导探索</li><li>底层残差策略：通过对技能进行细粒度的技能适应，实现任务变化的适应</li></ul><h2 id="1-2-方法-7">1.2 方法</h2><p>总的来说，该方法将经典控制器产生的演示轨迹分解为与任务无关的技能，并将其嵌入到连续到技能空间中，利用技能空间实现真正的通用学习，上层智能体能够从技能空间中访问但不动作，降低了对数据集详细程度的要求。</p><ul><li>从现有控制器中提取技能</li><li>学习技能嵌入和先验技能</li><li>训练一个分层强化学习策略，在技能空间中使用底层残差适应性策略。</li></ul><p><img src="https://img.mahaofei.com/img/202311161019261.png" alt="image.png"></p><p><strong>（1）数据收集</strong></p><p>本文通过手动控制收集演示数据（基本操作任务，如推物体、抓物体），虽然任务简单，但轨迹包含复杂的技能，可以重新组合解决复杂的任务。</p><p>轨迹是由 state-action 成对组成的，本文从中随机切片 $H$ 长度的片段进行无监督技能提取，利用提取的动作 a 和状态 s 学习下一小节中的 state-action。</p><p>其中状态 s 包括关节角度、关节速度、夹具位置、物体位置，动作是连续的 4D 向量，包括末端位置和速度。</p><p><strong>（2）学习强化学习的状态条件技能空间</strong></p><ul><li>将提取的技能嵌入到潜在空间中：使用变分自动编码器 VAE 将技能 $a$ 嵌入到潜在空间中，VAE 包括编码器和解码器，编码器将完整的 state-action 序列编码为 $z$，解码器根据当前状态 $s_t$ 和技能编码 $z$ 重建动作。</li><li>在探索过程中采样的技能状态条件先验：学习潜在技能空间上的条件概率密度。传统的高斯密度不能处理多模态信息，本文使用 real NVP 方法，实值非体积保留变换。学习从 $Z\times S-&gt;G$ 的映射，该映射就可以从简单分布 G 变换到技能空间 Z，因此 f 就是技能先验。</li></ul><p><img src="https://img.mahaofei.com/img/202311251603089.png" alt=""></p><blockquote><p><strong>变分自编码器</strong>，是一种深度生成模型<br><strong>传统</strong>：传统的自编码器包括编码器和解码器两部分，经过反复训练，输入数据被编码成一个编码向量，编码向量的每一个维度表示学习到的数据的特征，解码器尝试从编码向量中解码原始输入<br><strong>缺陷</strong>：传统的方法，使用单个值表示输入在某个潜在特征的表现。但实际上，将潜在特征表示为可能的取值范围会更合理。<br><strong>改进</strong>：因此变分自编码器就是使用取值的概率分布，代替原来的单值表示特征。<br><strong>优势</strong>：每个潜在特征表示为概率分布，解码时从潜在状态分布中随机采样，生成一个编码向量作为解码器的输入。实现了连续且平滑的潜在空间表示（潜在空间中彼此相邻的值重构出的结果相似）<br>参考理解:<a href="https://zhuanlan.zhihu.com/p/64485020">https://zhuanlan.zhihu.com/p/64485020</a></p></blockquote><p><strong>（3）状态条件技能空间中的强化学习</strong></p><p>一旦训练完成，解码器和技能先验权重就会被冻结，并合并到 RL 框架中。高级强化学习策略 $\pi$ 是一个神经网络，将状态映射到技能先验变化中的向量 g，在转换为潜在技能 Z。</p><p>然后解码器根据技能范围 H 的当前状态顺序重构动作。同时有一个底层残差策略，调整解码后的技能。</p><h2 id="1-3-总结">1.3 总结</h2><p>该方法是一种基于技能的强化学习方法。</p><ol><li>数据收集：使用最基本的控制器生成一些基本任务轨迹（移动、抓取），然后将这些轨迹分割成固定长度的序列，每一小段包括动作和对应的状态。</li><li>学习技能空间，使用变分自编码器将技能编码到潜在空间中；使用realNVP将技能潜在空间+机器人状态空间映射到简单分布空间（高斯分布），这样可以直接根据状态采样技能，称为技能先验。</li><li>强化学习：使用一个高层策略网络，根据当前的状态生成一个向量，根据技能先验（与当前状态有关的技能）中选择一个技能，利用技能解码器解码成机器人动作。</li></ol><h1>二、代码复现</h1><h2 id="2-1-环境搭建-3">2.1 环境搭建</h2><p><strong>（1）安装<code>mujoco</code>：</strong></p><p>下载<code>mujoco</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz</span><br></pre></td></tr></table></figure><p>创建一个隐藏文件夹，尽量不要修改此路经</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.mujoco</span><br></pre></td></tr></table></figure><p>将<code>mujoco</code>库解压到上面的文件夹中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco</span><br></pre></td></tr></table></figure><p>编辑环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/.bashrc</span><br></pre></td></tr></table></figure><p>在文件最后添加下面的语句，注意修改自己的用户名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Mujoco environment</span></span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/用户名/.mujoco/mujoco210/bin</span><br><span class="line">export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so</span><br></pre></td></tr></table></figure><p>刷新环境变量，重启terminal或执行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>测试mujoco</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.mujoco/mujoco210/bin</span><br><span class="line">./simulate ../model/arm26.xml</span><br></pre></td></tr></table></figure><p>能看到一个mujoco界面启动，并看到一个二自由度机械臂，说明安装成功。<code>../model/</code>下也有很多其它的模型示例，感兴趣可以看看。</p><p><strong>（2）python环境构建</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/krishanrana/reskill.git</span><br><span class="line">cd reskill</span><br><span class="line">conda env create -f environment.yml</span><br><span class="line">conda activate reskill_new</span><br><span class="line">pip install -e .</span><br><span class="line">cd reskill</span><br></pre></td></tr></table></figure><h2 id="2-2-数据收集">2.2 数据收集</h2><p>使用下面的脚本收集数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python data/collect_demos.py --num_trajectories 40000 --subseq_len 10 --task block</span><br></pre></td></tr></table></figure><p>其中<code>task</code>可以设置为<code>block</code>或<code>hook</code>。</p><h2 id="2-3-训练-2">2.3 训练</h2><p>训练技能模块：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_skill_modules.py --config_file block/config.yaml --dataset_name fetch_block_40000</span><br></pre></td></tr></table></figure><p>可视化训练完成的技能模块的性能：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python utils/test_skill_modules.py --dataset_name fetch_block_40000 --task block --use_skill_prior True</span><br></pre></td></tr></table></figure><p>训练reskill代理：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_reskill_agent.py --config_file table_cleanup/config.yaml --dataset_name fetch_block_40000</span><br></pre></td></tr></table></figure><p>可视化训练完成的reskill代理：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python utils/test_reskill_agent.py --dataset_name fetch_block_40000 --env_name FetchSlipperyPush-v0</span><br></pre></td></tr></table></figure><h2 id="2-4-日志记录">2.4 日志记录</h2><p>使用<a href="https://wandb.ai/">W&amp;B</a>，第一次train时输入自己的api即可。</p><h1>三、代码理解</h1><h2 id="3-1-基本定义-2">3.1 基本定义</h2><p><strong>（1）机器人动作</strong></p><p>算法中机器人通过下面的方法进行定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_set_action</span>(<span class="params">self, action</span>):</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;设置动作，在模拟环境中执行动作&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 处理输入动作</span></span><br><span class="line"><span class="keyword">assert</span> action.shape == (<span class="number">4</span>,) <span class="comment"># 确保输入的动作形状是（4，）</span></span><br><span class="line">action = action.copy()      <span class="comment"># ensure that we don&#x27;t change the action outside of this scope</span></span><br><span class="line">pos_ctrl, gripper_ctrl = action[:<span class="number">3</span>], action[<span class="number">3</span>]  <span class="comment"># 将动作差分成位置控制[:3]和夹爪控制[3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对输入动作值进行处理，末端位置进行缩放，旋转固定，夹爪根据条件是否设0</span></span><br><span class="line">pos_ctrl *= <span class="number">0.05</span>  <span class="comment"># 限制位置变化的最大值</span></span><br><span class="line">rot_ctrl = [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>]  <span class="comment"># 固定末端执行器的旋转，使用四元数表示</span></span><br><span class="line"><span class="comment">#rot_ctrl = [ 0.5, -0.5, 0.5, 0.5 ]  # 90 deg rotation of the original end effector, expressed as a quaternion</span></span><br><span class="line">gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])   <span class="comment"># 夹爪复制成两个</span></span><br><span class="line"><span class="keyword">assert</span> gripper_ctrl.shape == (<span class="number">2</span>,)</span><br><span class="line"><span class="keyword">if</span> self.block_gripper:  <span class="comment"># 如果block_gripper，则将手指位置设置为0</span></span><br><span class="line">gripper_ctrl = np.zeros_like(gripper_ctrl)</span><br><span class="line"><span class="comment"># 将经过修改后的位置控制、固定的末端执行器旋转和处理后的夹爪控制连接成一个新的动作数组 action</span></span><br><span class="line">action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply action to simulation. 将动作应用到仿真环境中</span></span><br><span class="line">utils.ctrl_set_action(self.sim, action)</span><br><span class="line">utils.mocap_set_action(self.sim, action)</span><br></pre></td></tr></table></figure><p>可以看出在本算法中，机器人的动作被定义为了长度为4的数组，四个值分别代表机器人末端的控制位置和夹爪开合大小。（这里旋转被忽略了，因为任务是抓取方块到指定位置，所以算法直接设置末端永远竖直向下）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">action = [x, y, z, gripper]</span><br></pre></td></tr></table></figure><p>实际上gym中机器人的<code>action</code>使用长度为9的数组进行控制，分别代表末端的空间位置3个变量，末端的空间姿态四元数，夹爪两个平行板的动作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">action = [x, y, z, quat1, quat2, quat3, quat4, gripper_l, gripper_r]</span><br></pre></td></tr></table></figure><p><strong>（2）观测状态</strong></p><p>观测状态通过如下代码获取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_obs</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;获得环境的观察&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 位置</span></span><br><span class="line">    grip_pos = self.sim.data.get_site_xpos(<span class="string">&#x27;robot0:grip&#x27;</span>)   <span class="comment"># 获取机器人手爪的位置</span></span><br><span class="line">    dt = self.sim.nsubsteps * self.sim.model.opt.timestep</span><br><span class="line">    grip_velp = self.sim.data.get_site_xvelp(<span class="string">&#x27;robot0:grip&#x27;</span>) * dt    <span class="comment"># 计算手爪的线速度</span></span><br><span class="line">    robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)  <span class="comment"># 使用辅助函数获取机器人的位置和速度</span></span><br><span class="line"></span><br><span class="line">    gripper_state = robot_qpos[-<span class="number">2</span>:]     <span class="comment"># 提取了夹爪的状态和速度</span></span><br><span class="line">    gripper_vel = robot_qvel[-<span class="number">2</span>:] * dt  <span class="comment"># change to a scalar if the gripper is made symmetric</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将夹爪的位置、状态、夹爪线速度、速度连接起来，形成初始观察</span></span><br><span class="line">    obs = np.concatenate([</span><br><span class="line">        grip_pos,</span><br><span class="line">        gripper_state,</span><br><span class="line">        grip_velp,</span><br><span class="line">        gripper_vel,</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储已经到达的目标</span></span><br><span class="line">    achieved_goal = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有的方块</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_blocks):</span><br><span class="line">    <span class="comment"># for i in range(1):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取方块们的位置、姿态、速度、相对位置、相对速度</span></span><br><span class="line">        object_i_pos = self.sim.data.get_site_xpos(self.object_names[i])</span><br><span class="line">        <span class="comment"># rotations</span></span><br><span class="line">        object_i_rot = rotations.mat2euler(self.sim.data.get_site_xmat(self.object_names[i]))</span><br><span class="line">        <span class="comment"># velocities</span></span><br><span class="line">        object_i_velp = self.sim.data.get_site_xvelp(self.object_names[i]) * dt</span><br><span class="line">        object_i_velr = self.sim.data.get_site_xvelr(self.object_names[i]) * dt</span><br><span class="line">        <span class="comment"># gripper state</span></span><br><span class="line">        object_i_rel_pos = object_i_pos - grip_pos</span><br><span class="line">        object_i_velp -= grip_velp</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 连接到观察值中</span></span><br><span class="line">        obs = np.concatenate([</span><br><span class="line">            obs,</span><br><span class="line">            object_i_pos.ravel(),</span><br><span class="line">            object_i_rel_pos.ravel(),</span><br><span class="line">            <span class="comment">#object_i_rot.ravel(),</span></span><br><span class="line">            object_i_velp.ravel(),</span><br><span class="line">            <span class="comment">#object_i_velr.ravel()</span></span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is current location of the blocks</span></span><br><span class="line">        <span class="comment"># 方块们的当前位置</span></span><br><span class="line">        achieved_goal = np.concatenate([</span><br><span class="line">            achieved_goal, object_i_pos.copy()</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    achieved_goal = np.concatenate([achieved_goal, grip_pos.copy()])</span><br><span class="line"></span><br><span class="line">    achieved_goal = np.squeeze(achieved_goal)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.use_force_sensor:</span><br><span class="line">        self.sim.data.get_sensor(<span class="string">&#x27;force_sensor&#x27;</span>) </span><br><span class="line">        force_reading = self.sim.data.sensordata <span class="comment"># Read force sensor reading from tray</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        force_reading = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;observation&#x27;</span>: obs.copy(),</span><br><span class="line">        <span class="string">&#x27;achieved_goal&#x27;</span>: achieved_goal.copy(),</span><br><span class="line">        <span class="string">&#x27;desired_goal&#x27;</span>: self.goal.copy(),</span><br><span class="line">        <span class="string">&#x27;force_sensor&#x27;</span>: force_reading.copy()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>可以看出观测状态是一个字典：</p><table><thead><tr><th style="text-align:left">键</th><th style="text-align:left">值</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">observation</td><td style="text-align:left">[grip_x, grip_y, grip_z,</td><td style="text-align:left">grip_pos，末端位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">gripper_left, gripper_right,</td><td style="text-align:left">gripper_state，夹爪两侧状态</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">grip_vx, grip_vy, grip_vz,</td><td style="text-align:left">grip_vel，末端速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">gripper_vl, gripper_vr,</td><td style="text-align:left">gripper_vel，夹爪两侧速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj1_x, obj1_y, obj1_z,</td><td style="text-align:left">obj_i_pos，方块i位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj1_rx, obj1_ry, obj1_rz,</td><td style="text-align:left">obj_i_rel_pos，方块i相对末端位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj1_vx, obj1_vy, obj1_vz,</td><td style="text-align:left">obj_i_velp, 方块i相对末端速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj2…</td><td style="text-align:left">方块2位置、相对位置、相对速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obji…]</td><td style="text-align:left">方块i位置、相对位置、相对速度</td></tr><tr><td style="text-align:left">achieved_goal</td><td style="text-align:left">[grip_x1, grip_y1, grip_z1,</br>grip_x2, grip_y2, grip_z2</br>…]</td><td style="text-align:left">每调用一次_get_obs()方法</br>将当前末端位置</br>添加到achieved_goal最后</td></tr><tr><td style="text-align:left">desired_goal</td><td style="text-align:left">[[x1, y1, z1], [x2, y2, z2], … ]</td><td style="text-align:left">多个目标位置</td></tr><tr><td style="text-align:left">force_sensor</td><td style="text-align:left">[f_x, f_y, f_z]</td><td style="text-align:left">force_reading，末端力，默认[0,0,0]</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">基于技能单元的强化学习方法，将任务分解为技能，建立技能与状态的映射关系，学习技能选择策略</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】Dex 演示引导强化学习与手术机器人任务自动化的高效探索</title>
    <link href="https://www.mahaofei.com/post/90275938.html"/>
    <id>https://www.mahaofei.com/post/90275938.html</id>
    <published>2023-11-21T06:00:57.000Z</published>
    <updated>2023-11-21T06:00:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><blockquote><p><strong>标题</strong>：Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot<br><strong>标题</strong>：演示引导强化学习与手术机器人任务自动化的高效探索<br><strong>作者团队</strong>：香港中文大学（刘云辉团队）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/med-air/DEX">https://github.com/med-air/DEX</a></p></blockquote><h2 id="1-1-目标问题">1.1 目标问题</h2><p>虽然基于强化学习的方法为手术自动化提供了可能的方案，但是通常需要大量收集数据才能进行学习。因此本文目的是提高从演示中探索学习的效率，有效地利用专家演示数据。</p><p>具体而言，目前的问题如下：</p><ul><li>使用强化学习，如果不给出演示数据而仅通过探索学习，需要收集大量的数据来解决任务；</li><li>使用演示数据的方法，例如赋予演示数据相对于机器人探索数据更高的优先级，效率仍然低下，设置额外奖励函数的方法不仅只能针对特定环境，且容易引起局部最优；</li><li>使用 actor-critic 框架，通过正则化 actor 损失来衡量机器人与专家之间的行为差异，但是这种方式效率较低（尤其在初期机器人与演示差距较大情况下），且没有考虑 critic 的正则化，容易导致高估问题。</li></ul><p>本文贡献：</p><ul><li>提出一种 actor-critic 框架，降低 critic 的高估问题，提高强化学习过程中类似专家的行动进行探索。</li><li>使用非参数引导传播，实现未观测状态的探索</li><li>在 SurRoL 手术机器人上实验验证，效果优秀，同时部署在 dVRK 上，同样表示出强大的潜力。</li></ul><blockquote><p>dVRK(da Vinci Research Kit，达芬奇手术机器人系统)</p></blockquote><h2 id="1-2-方法">1.2 方法</h2><p>DEX(Demonstration-guided EXploration)，演示引导探索。</p><p><strong>（0）问题定义</strong></p><p>将手术机器人动作学习考虑为一个 off-policy 的智能体，在由马尔可夫决策过程构建的环境中进行交互。</p><blockquote><p>off-policy，指智能体不使用当前的策略来决定行动，而是使用不同的策略来生成行为数据，从过去的经历中学到最优的行为决策方法。</p></blockquote><p>在 $t$ 时刻，机器人根据当前状态 $s_t$ 以及确定性策略 $\pi$ 执行行动，环境用 $r_t=r(s_t,a_t)$ 奖励智能体，然后状态转移 $s_{t+1}$。</p><p>循环此过程，每次智能体将经验 $(s+t,a_t,r_t,s_{t+1})$ 存入重放缓冲区 $D_A$。</p><p>同时设置一个演示缓冲区 $D_E$，用于存放专家策略 $\pi$ 经验。</p><p><img src="https://img.mahaofei.com/img/202311151037433.png" alt="image.png"></p><p>如图，该方法由两部分组成：</p><ul><li>基于 actor-critic 的策略学习模块（右下角），用于从演示数据中指导探索；</li><li>基于最近邻匹配和局部加权回归的非参数模块（左上角），用于将与当前状态相差过大的演示传播到为当前状态。</li></ul><p><strong>（1）专家引导的 actor-critic 框架</strong></p><p>现有的 actor-critic 方法通过最大化预期回报来学习最优策略，但是如果 Q 值估计不准确，会阻碍探索。本文通过利用智能体和专家策略之间的动作差距来增强环境奖励。</p><p>$$<br>\max_{\pi}\mathbb{E}<em>{\pi}\left[\sum</em>{t=0}^{\infty}\gamma^{t}(r_{t}-\alpha d(a_{t},a_{t}^{e}))\right],a_{t}^{e}:=\pi^{e}(s_{t}),<br>$$</p><p>其中 $\alpha$ 是探索系数，$d()$ 衡量智能体动作和专家动作之间的相似性距离度量。</p><p>基于此奖励，本文设计了正则化 Q 函数（critic），并最小化动作价值和状态价值的差距。</p><p><strong>（2）有限演示情况下的引导的传播</strong></p><p>智能体在初始学习阶段很容易探索演示未覆盖的区域，无法实现监督 actor 探索。</p><p>常规的解决思路有行为克隆，但是当状态相差较大时，策略与专家行动仍会有较大的不同。因此本文使用非参数回归模型，从有限的演示中将经验传播实现更稳定的引导。</p><p>首先从演示缓冲区采样一小批状态和动作，然后给定一个当前状态，在一小批状态中搜索，利用 k 近邻方法找到最接近的状态，然后使用指数和函数的局部加权回归方法近似专家策略。</p><p>$$<br>\hat{\pi}^e(s)=\frac{\sum_{i=1}^k\exp\left(-|s-s^{(i)}|<em>2\right)\cdot a^{(i)}}{\sum</em>{i=1}^k\exp\left(-|s-s^{(i)}|_2)\right)}.<br>$$</p><h1>二、算法复现</h1><h2 id="2-1-环境配置">2.1 环境配置</h2><p>clone代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/med-air/DEX.git</span><br><span class="line">cd DEX</span><br></pre></td></tr></table></figure><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n dex python=3.8</span><br><span class="line">conda activate dex</span><br></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -e SurRoL/# install surrol environments</span><br><span class="line">pip3 install -r requirements.txt</span><br><span class="line">pip3 install -e .</span><br></pre></td></tr></table></figure><p>在虚拟环境的<code>gym/envs/__init__.py</code>的第一行注册SurRoL任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">directory: anaconda3/envs/dex/lib/python3.8/site-packages/gym/envs/__init__.py</span></span><br><span class="line">import surrol.gym</span><br></pre></td></tr></table></figure><h2 id="2-2-数据采集">2.2 数据采集</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir SurRoL/surrol/data/demo</span><br><span class="line">python SurRoL/surrol/data/data_generation.py --env NeedlePick-v0 </span><br></pre></td></tr></table></figure><h2 id="2-3-训练">2.3 训练</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 train.py task=NeedlePick-v0 agent=dex use_wb=True</span><br></pre></td></tr></table></figure><p>作者提供的程序中也包括了其它强化学习与模仿学习算法，例如：</p><ul><li>DDPG：深度确定性策略梯度的强化学习</li><li>DDPGBC：深度确定性策略梯度的强化学习+行为克隆</li><li>SAC：最大熵无模型深度强化学习</li><li>SQIL：正则化行为克隆的模仿学习</li><li>COL：行为克隆与强化学习</li><li>AWAC：离线强化学习</li><li>AMP：对抗模仿学习</li></ul><h1>三、代码理解</h1><h2 id="3-1-基本定义">3.1 基本定义</h2><p><strong>（1）机器人状态</strong></p><p>通过以下函数获取机器人状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SurRoL.surrol.tasks.psm_env.PsmEnv._get_robot_state</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_robot_state</span>(<span class="params">self, idx: <span class="built_in">int</span></span>) -&gt; np.ndarray:</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;获取机器人的状态，返回机器人当前位姿、夹爪角度，两者拼接成一个数组（3位置+3欧拉角+1开合角度）&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># robot state: tip pose in the world coordinate</span></span><br><span class="line">psm = self.psm1 <span class="keyword">if</span> idx == <span class="number">0</span> <span class="keyword">else</span> self.psm2</span><br><span class="line">pose_world = psm.pose_rcm2world(psm.get_current_position(), <span class="string">&#x27;tuple&#x27;</span>)    <span class="comment"># 机器人在世界坐标系下的位姿</span></span><br><span class="line">jaw_angle = psm.get_current_jaw_position()  <span class="comment"># 夹爪角度</span></span><br><span class="line"><span class="keyword">return</span> np.concatenate([</span><br><span class="line">np.array(pose_world[<span class="number">0</span>]), np.array(p.getEulerFromQuaternion(pose_world[<span class="number">1</span>])), np.array(jaw_angle).ravel()</span><br><span class="line">])  <span class="comment"># 3 + 3 + 1 = 7</span></span><br></pre></td></tr></table></figure><p>因此机器人的状态为一个数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">robot_state = [x, y, z, roll, pitch, yaw, gripper]</span><br></pre></td></tr></table></figure><p><strong>（2）观测状态</strong></p><p>观测状态通过以下方法进行获取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SurRoL.surrol.tasks.psm_env.PsmEnv._get_obs</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_obs</span>(<span class="params">self</span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;获取当前环境状态信息（机器人当前位置、目标物体位置、机器人与目标物体的相对位置）&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 获取机器人当前状态</span></span><br><span class="line">robot_state = self._get_robot_state(idx=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> may need to modify</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查环境中是否有物体，如果有则获取目标物体的位置、姿态、相对于机器人的位置</span></span><br><span class="line"><span class="keyword">if</span> self.has_object:</span><br><span class="line">pos, _ = get_link_pose(self.obj_id, -<span class="number">1</span>)     <span class="comment"># 目标物体位置</span></span><br><span class="line">object_pos = np.array(pos)</span><br><span class="line">pos, orn = get_link_pose(self.obj_id, self.obj_link1)   <span class="comment"># 获取目标物体的特定link的位置和方向</span></span><br><span class="line">waypoint_pos = np.array(pos)    <span class="comment"># 路径点位置为目标物体link位置</span></span><br><span class="line"><span class="comment"># rotations</span></span><br><span class="line">waypoint_rot = np.array(p.getEulerFromQuaternion(orn))  <span class="comment">#路径点姿态为目标物体link姿态</span></span><br><span class="line"><span class="comment"># relative position state</span></span><br><span class="line">object_rel_pos = object_pos - robot_state[<span class="number">0</span>: <span class="number">3</span>] <span class="comment"># 相对位置为目标位置与机器人末端位置之差</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> can have a same-length state representation</span></span><br><span class="line">object_pos = waypoint_pos = waypoint_rot = object_rel_pos = np.zeros(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确定使用哪个位置作为目标位置</span></span><br><span class="line"><span class="keyword">if</span> self.has_object:</span><br><span class="line"><span class="comment"># object/waypoint position，使用物体位置object_pos，或物体link位置waypoint_pos</span></span><br><span class="line">achieved_goal = object_pos.copy() <span class="keyword">if</span> <span class="keyword">not</span> self._waypoint_goal <span class="keyword">else</span> waypoint_pos.copy()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># tip position，如果没有目标物体，则将机器人末端的位置作为目标位置</span></span><br><span class="line">achieved_goal = np.array(get_link_pose(self.psm1.body, self.psm1.TIP_LINK_INDEX)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">observation = np.concatenate([</span><br><span class="line">robot_state, object_pos.ravel(), object_rel_pos.ravel(),</span><br><span class="line">waypoint_pos.ravel(), waypoint_rot.ravel()  <span class="comment"># achieved_goal.copy(),</span></span><br><span class="line">])</span><br><span class="line">obs = &#123;</span><br><span class="line"><span class="string">&#x27;observation&#x27;</span>: observation.copy(),</span><br><span class="line"><span class="string">&#x27;achieved_goal&#x27;</span>: achieved_goal.copy(),</span><br><span class="line"><span class="string">&#x27;desired_goal&#x27;</span>: self.goal.copy()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> obs</span><br></pre></td></tr></table></figure><p>可以看出观测状态是一个字典，包含三个键：<code>observation</code>, <code>achieved_goal</code>, <code>desired_goal</code>。</p><ul><li>observation: 由robot_state, object_pos.ravel(), object_rel_pos.ravel(), waypoint_pos.ravel(), waypoint_rot.ravel()组成，</li></ul><table><thead><tr><th style="text-align:left">obs</th><th style="text-align:left">键值（三个都是一维数组）</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">observation</td><td style="text-align:left">[robot_x, robot_y, robot_z, robot_roll, robot_pitch, robot_yaw, gripper,</td><td style="text-align:left">机器人末端位姿和夹爪状态</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj_x, obj_y, obj_z,</td><td style="text-align:left">目标物体位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj_rel_x, obj_rel_y, obj_rel_z,</td><td style="text-align:left">目标物体与当前机械臂相对位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj_link1_x, obj_link1_y, obj_link1_z</td><td style="text-align:left">目标物体的link1位置作为waypoint位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj_link1_roll, obj_link1_pitch, obj_link1_yaw]</td><td style="text-align:left">目标物体的link1姿态作为waypoint姿态</td></tr><tr><td style="text-align:left">achieved_goal</td><td style="text-align:left">[x, y, z]</td><td style="text-align:left">waypoint作为实际位置，未设置link1则用目标物体位置作为实际位置，无物体则用机器人末端位置为实际位置</td></tr><tr><td style="text-align:left">desired_goal</td><td style="text-align:left">[x, y, z]</td><td style="text-align:left">机器人末端的目标位置</td></tr></tbody></table><p><strong>（3）机器人动作</strong></p><p>机器人动作通过以下方法进行执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SurRoL.surrol.tasks.psm_env.PsmEnv._set_action</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_set_action</span>(<span class="params">self, action: np.ndarray</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">delta_position (3), delta_theta (1) and open/close the gripper (1)</span></span><br><span class="line"><span class="string">in the world frame</span></span><br><span class="line"><span class="string">执行动作的过程（位置的变化，旋转变化，夹爪的开合）</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(action) == self.ACTION_SIZE, <span class="string">&quot;The action should have the save dim with the ACTION_SIZE&quot;</span> <span class="comment"># ACTION_SIZE = 5</span></span><br><span class="line"><span class="comment"># time0 = time.time()</span></span><br><span class="line">action = action.copy()  <span class="comment"># ensure that we don&#x27;t change the action outside of this scope</span></span><br><span class="line">action[:<span class="number">3</span>] *= <span class="number">0.01</span> * self.SCALING  <span class="comment"># position, limit maximum change in position</span></span><br><span class="line">pose_world = self.psm1.pose_rcm2world(self.psm1.get_current_position())</span><br><span class="line">workspace_limits = self.workspace_limits1</span><br><span class="line">pose_world[:<span class="number">3</span>, <span class="number">3</span>] = np.clip(pose_world[:<span class="number">3</span>, <span class="number">3</span>] + action[:<span class="number">3</span>],</span><br><span class="line">workspace_limits[:, <span class="number">0</span>] - [<span class="number">0.02</span>, <span class="number">0.02</span>, <span class="number">0.</span>],</span><br><span class="line">workspace_limits[:, <span class="number">1</span>] + [<span class="number">0.02</span>, <span class="number">0.02</span>, <span class="number">0.08</span>])  <span class="comment"># clip to ensure convergence</span></span><br><span class="line">rot = get_euler_from_matrix(pose_world[:<span class="number">3</span>, :<span class="number">3</span>])</span><br><span class="line"><span class="keyword">if</span> self.ACTION_MODE == <span class="string">&#x27;yaw&#x27;</span>:</span><br><span class="line">action[<span class="number">3</span>] *= np.deg2rad(<span class="number">30</span>)  <span class="comment"># yaw, limit maximum change in rotation</span></span><br><span class="line">rot = (self.psm1_eul[<span class="number">0</span>], self.psm1_eul[<span class="number">1</span>], wrap_angle(rot[<span class="number">2</span>] + action[<span class="number">3</span>]))  <span class="comment"># only change yaw</span></span><br><span class="line"><span class="keyword">elif</span> self.ACTION_MODE == <span class="string">&#x27;pitch&#x27;</span>:</span><br><span class="line">action[<span class="number">3</span>] *= np.deg2rad(<span class="number">15</span>)  <span class="comment"># pitch, limit maximum change in rotation</span></span><br><span class="line">pitch = np.clip(wrap_angle(rot[<span class="number">1</span>] + action[<span class="number">3</span>]), np.deg2rad(-<span class="number">90</span>), np.deg2rad(<span class="number">90</span>))</span><br><span class="line">rot = (self.psm1_eul[<span class="number">0</span>], pitch, self.psm1_eul[<span class="number">2</span>])  <span class="comment"># only change pitch</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">raise</span> NotImplementedError</span><br><span class="line">pose_world[:<span class="number">3</span>, :<span class="number">3</span>] = get_matrix_from_euler(rot)</span><br><span class="line">action_rcm = self.psm1.pose_world2rcm(pose_world)</span><br><span class="line"><span class="comment"># time1 = time.time()</span></span><br><span class="line">self.psm1.move(action_rcm)</span><br><span class="line"><span class="comment"># time2 = time.time()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># jaw</span></span><br><span class="line"><span class="keyword">if</span> self.block_gripper:</span><br><span class="line">action[<span class="number">4</span>] = -<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> action[<span class="number">4</span>] &lt; <span class="number">0</span>:</span><br><span class="line">self.psm1.close_jaw()</span><br><span class="line">self._activate(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">self.psm1.move_jaw(np.deg2rad(<span class="number">40</span>))  <span class="comment"># open jaw angle; can tune</span></span><br><span class="line">self._release(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>其中机器人动作是一个长度为5的数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">action[<span class="number">5</span>] = [delta_x, delta_y, delta_z, yaw/pitch, gripper]</span><br></pre></td></tr></table></figure><ul><li>其中<code>delta_xyz</code>代表机器人末端在xyz轴上的位移量；</li><li>yaw/pitch根据类变量ACTION_MODE定义使用哪种旋转；</li><li>夹爪为负数则关闭，如果为非负数则根据数值大小确定打开大小。</li></ul><h1>四、代码修改</h1><h2 id="4-1-环境补充">4.1 环境补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install hydra-core colorlog termcolor opencv-python perlin_noise</span><br><span class="line">pip install <span class="string">&quot;cython&lt;3&quot;</span></span><br></pre></td></tr></table></figure><h2 id="4-2-仿真环境设置">4.2 仿真环境设置</h2><p>状态 obs 定义</p><table><thead><tr><th style="text-align:left">键</th><th style="text-align:left">值</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">observation</td><td style="text-align:left">[grip_x, grip_y, grip_z,</td><td style="text-align:left">grip_pos，末端位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">gripper_left, gripper_right,</td><td style="text-align:left">gripper_state，夹爪两侧状态</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">grip_vx, grip_vy, grip_vz,</td><td style="text-align:left">grip_vel，末端速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">gripper_vl, gripper_vr,</td><td style="text-align:left">gripper_vel，夹爪两侧速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj1_x, obj1_y, obj1_z,</td><td style="text-align:left">obj_i_pos，方块i位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj1_rx, obj1_ry, obj1_rz,</td><td style="text-align:left">obj_i_rel_pos，方块i相对末端位置</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj1_vx, obj1_vy, obj1_vz,</td><td style="text-align:left">obj_i_velp, 方块i相对末端速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obj2…</td><td style="text-align:left">方块2位置、相对位置、相对速度</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">obji…]</td><td style="text-align:left">方块i位置、相对位置、相对速度</td></tr><tr><td style="text-align:left">achieved_goal</td><td style="text-align:left">[grip_x1, grip_y1, grip_z1,</br>grip_x2, grip_y2, grip_z2</br>…]</td><td style="text-align:left">每调用一次_get_obs()方法</br>将当前末端位置</br>添加到achieved_goal最后</td></tr><tr><td style="text-align:left">desired_goal</td><td style="text-align:left">[[x1, y1, z1], [x2, y2, z2], … ]</td><td style="text-align:left">多个目标位置</td></tr><tr><td style="text-align:left">force_sensor</td><td style="text-align:left">[f_x, f_y, f_z]</td><td style="text-align:left">force_reading，末端力，默认[0,0,0]</td></tr></tbody></table><h2 id="4-3-测试">4.3 测试</h2><ul><li>AWAC：离线强化学习<ul><li>SuccessReward：夹爪只能到达物体上方，无法抓取物体</li><li>DistanceReward：夹爪只能到达物体上方，无法抓取物体</li></ul></li><li>DDPGBC：深度确定性策略梯度+模仿学习<ul><li>DistanceReward：夹爪只能通过各种方式移动物体到指定位置，无法抓取</li></ul></li></ul>]]></content>
    
    
    <summary type="html">香港中文大学刘云辉团队基于DDPG+BC改进的手术机器人强化学习方法</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>强化学习与模仿学习Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/1494b0dc.html"/>
    <id>https://www.mahaofei.com/post/1494b0dc.html</id>
    <published>2023-11-14T02:15:37.000Z</published>
    <updated>2023-11-14T02:15:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 环境搭建问题</h1><h2 id="1-1-mujoco-相关">1.1 mujoco 相关</h2><h3 id="1-mujoco-py-安装后编译错误-Error-compiling-Cython-file">(1) mujoco-py 安装后编译错误 Error compiling Cython file</h3><p><strong>问题详情：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">performance hint: /home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/mujoco_py/cymj.pyx:67:5: Exception check on &#x27;c_warning_callback&#x27; will always require the GIL to be acquired.</span><br><span class="line">Possible solutions:</span><br><span class="line">1. Declare the function as &#x27;noexcept&#x27; if you control the definition and you&#x27;re sure you don&#x27;t want the function to raise exceptions.</span><br><span class="line">2. Use an &#x27;int&#x27; return type on the function to allow an error code to be returned.</span><br><span class="line">performance hint: /home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/mujoco_py/cymj.pyx:104:5: Exception check on &#x27;c_error_callback&#x27; will always require the GIL to be acquired.</span><br><span class="line">Possible solutions:</span><br><span class="line">1. Declare the function as &#x27;noexcept&#x27; if you control the definition and you&#x27;re sure you don&#x27;t want the function to raise exceptions.</span><br><span class="line">2. Use an &#x27;int&#x27; return type on the function to allow an error code to be returned.</span><br><span class="line"></span><br><span class="line">Error compiling Cython file:</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">...</span><br><span class="line">    See c_warning_callback, which is the C wrapper to the user defined function</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    global py_warning_callback</span><br><span class="line">    global mju_user_warning</span><br><span class="line">    py_warning_callback = warn</span><br><span class="line">    mju_user_warning = c_warning_callback</span><br><span class="line">                       ^</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">/home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/mujoco_py/cymj.pyx:92:23: Cannot assign type &#x27;void (const char *) except * nogil&#x27; to &#x27;void (*)(const char *) noexcept nogil&#x27;. Exception values are incompatible. Suggest adding &#x27;noexcept&#x27; to type &#x27;void (const char *) except * nogil&#x27;.</span><br><span class="line"></span><br><span class="line">Error compiling Cython file:</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">...</span><br><span class="line">    See c_warning_callback, which is the C wrapper to the user defined function</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    global py_error_callback</span><br><span class="line">    global mju_user_error</span><br><span class="line">    py_error_callback = err_callback</span><br><span class="line">    mju_user_error = c_error_callback</span><br><span class="line">                     ^</span><br><span class="line">------------------------------------------------------------</span><br></pre></td></tr></table></figure><p><strong>解决方法：</strong></p><blockquote><p>参考：<a href="https://stackoverflow.com/questions/76985054/import-mujoco-py-is-giving-me-compiling-errors">stackoverflow</a> 与 <a href="https://github.com/openai/mujoco-py/issues/773#issuecomment-1639684035">github issue</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install &quot;cython&lt;3&quot;</span><br></pre></td></tr></table></figure><h3 id="2-ERROR-GLEW-initalization-error-Missing-GL-version">(2) ERROR: GLEW initalization error: Missing GL version</h3><p><strong>问题详情：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=100) is deprecated. Use register(max_episode_steps=100) instead.</span><br><span class="line">  warnings.warn(&quot;register(timestep_limit=&#123;&#125;) is deprecated. Use register(max_episode_steps=&#123;&#125;) instead.&quot;.format(timestep_limit, timestep_limit))</span><br><span class="line">Creating window glfw</span><br><span class="line">ERROR: GLEW initalization error: Missing GL version</span><br><span class="line"></span><br><span class="line">Press Enter to exit ...Killed</span><br></pre></td></tr></table></figure><p><strong>解决方法：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so</span><br></pre></td></tr></table></figure><h3 id="3-MujocoException-Got-MuJoCo-Warning-Nan-Inf-or-huge-value-in-QACC-at-DOF-X-The-simulation-is-unstable">(3) MujocoException: Got MuJoCo Warning: Nan, Inf or huge value in QACC at DOF X. The simulation is unstable.</h3><p><strong>问题原因：</strong></p><p>在使用<code>mujoco</code>进行强化学习训练的过程中，有可能会出先上面这种报错，从而导致训练提前终止。</p><p>这是因为在执行<code>env.step</code>过程中，环境中的模型（机器人自身或者机器人和其它物体）发生了模型穿透或者约束冲突，导致mujoco无法进行仿真。</p><p><strong>解决方法1：添加关节属性damping和armature（有效！）</strong></p><p>参考链接：<a href="https://github.com/google-deepmind/mujoco/issues/989">https://github.com/google-deepmind/mujoco/issues/989</a></p><p>在限制关节转角后，法线算法能训练的epoch数变多了，但是到后面还是会报错。</p><p>经过检查法线我的夹爪上的关节都没有设置阻尼，添加<code>damping</code>和<code>armature</code>，注意需要给所有的<code>joint</code>都添加这两个属性，包括机器人和各种物体。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">&quot;robotiq_2f_85_right_driver_joint&quot;</span> <span class="attr">range</span>=<span class="string">&quot;0 0.834&quot;</span> <span class="attr">damping</span>=<span class="string">&quot;0.1&quot;</span> <span class="attr">armature</span>=<span class="string">&#x27;0.01&#x27;</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>之后再进行训练，再也没有出现过错误。</p><p>注意：<code>damping</code>和<code>armature</code>这两个参数可能需要调节，尤其是夹爪等比较小的部件，如果这里的阻尼设置的太大，会出现关节无法运动的情况。</p><p><strong>其它方法2：限制关节范围（有一定效果）</strong></p><p>对于我的机械臂模型而言，由于我的目标任务是抓取，机械臂原本的活动范围设置的很大，在调试过程中也能够发现在很多情况下机械臂会与自身碰撞、或与桌面碰撞。</p><p>我的方法是：手动控制机械臂在<code>mujoco</code>环境中运动，使其到达任务空间的极限位置，使用下面的命令，打印出各<code>joint</code>在极限位置的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sim.data.qpos[model.jnt_qposadr])</span><br></pre></td></tr></table></figure><p>根据所有极限位置，确定各个关节的取值范围。</p><p>然后在模型的<code>.xml</code>文件中，修改<code>&lt;wholebody&gt;</code>中各个<code>&lt;joint&gt;</code>的<code>range</code>，例如：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 初始 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- &lt;joint name=&quot;joint1&quot; pos=&quot;0 0 0&quot; axis=&quot;0 0 1&quot; armature=&quot;1.5708&quot; limited=&quot;true&quot; range=&quot;-3.14159 3.14159&quot; damping=&#x27;200&#x27; /&gt; --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 测试关节范围(抓取) --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">&quot;joint1&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">axis</span>=<span class="string">&quot;0 0 1&quot;</span> <span class="attr">armature</span>=<span class="string">&quot;1.5708&quot;</span> <span class="attr">limited</span>=<span class="string">&quot;true&quot;</span> <span class="attr">range</span>=<span class="string">&quot;-1.57079 1.57079&quot;</span> <span class="attr">damping</span>=<span class="string">&#x27;200&#x27;</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>经过测试，只改变<code>joint</code>的<code>range</code>后，能迭代的次数变多了，但是还是没有解决问题。</p><p><strong>其它方法3：修改奖励函数（有一定效果）</strong></p><p>因为报错的原因是机器人的动作超出了工作空间或者发生了碰撞，那么说明学习算法没有学习到合适的动作，让机器人产生了错误的运动。</p><p>因此考虑修改奖励函数，加入超出工作空间的惩罚，例如对于抓取，如果夹爪末端位置xyz超出一定坐标，则给予惩罚，以及夹爪距离物体过远，也给予惩罚。</p><p>这种方式有一定效果。</p><p><strong>其它方法4：减小solrel（未尝试）</strong></p><p>参考链接：<a href="https://github.com/google-deepmind/mujoco/discussions/63">https://github.com/google-deepmind/mujoco/discussions/63</a></p><p><strong>其它方法5：改用RK4积分器（无效）</strong></p><p>参考链接：<a href="https://github.com/google-deepmind/mujoco/issues/168">https://github.com/google-deepmind/mujoco/issues/168</a></p><p><strong>其它方法6：检查环境设置（有效）</strong></p><p>检查环境的<code>reset</code>函数是否设置正确，经过检查，我发现我的代码问题在于重置了环境和机械臂的各个关节，但是没有重置<code>mocap</code>，导致如果上一次仿真最后机械臂末端漂了，这一次一开始机械臂就会漂的很远，随着仿真的进行就容易出现上面的错误。</p>]]></content>
    
    
    <summary type="html">强化学习环境搭建，项目运行过程中可能遇到的bug与解决方法记录</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】基于强化学习的机器人动作模仿</title>
    <link href="https://www.mahaofei.com/post/6fa4482c.html"/>
    <id>https://www.mahaofei.com/post/6fa4482c.html</id>
    <published>2023-11-09T07:36:15.000Z</published>
    <updated>2023-11-09T07:36:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 Reinforcement Learning with Videos: Combining Offline Observations with Interaction</h1><blockquote><p><strong>标题</strong>：视频强化学习：将离线观察与互动相结合<br><strong>作者团队</strong>：宾夕法尼亚大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/kschmeckpeper/rl_with_videos">https://github.com/kschmeckpeper/rl_with_videos</a></p></blockquote><h2 id="1-1-目标问题-6">1.1 目标问题</h2><p>应用强化学习使机器人学习技能，通常需要大量的机器人在线数据，但是机器人的数据收集非常麻烦困难，难以获得足够多的数据。</p><p>人类视频广泛且多样，因此考虑从人类经验中进行强化学习。但因为人类视频没有动作的标注，并且人类视频和机器人相机图像，具有巨大的图像差异和视角差异。具体问题如下：</p><ul><li>机器人必须能通过观察来更新策略，不需要任何的行动或者奖励；</li><li>人手与末端执行器视觉差异较大，自由度也不同，因此需要考虑动作空间、形态、视角、环境差异带来的变化；</li></ul><p>为了解决这些问题，本文提出了视频强化学习框架（Reinforcement Learning with Videos，RLV），使用人类数据经验和机器人数据学习策略和价值函数。</p><h2 id="1-2-方法-5">1.2 方法</h2><p><strong>（1）问题定义</strong></p><p>该论文将问题公式化为马尔可夫决策过程 MDP，定义成元组 $(S_{int},A_{int},P,R)$，其中 $S_{int}$ 是状态空间，$A_{int}$ 是动作空间，$P$ 是环境的动力学，$R$ 是奖励函数。</p><p>机器人首先被提供了人类的观测 ${(s_{obs},s_{int}')_{1:t}}$，这些观察被建模成另一组马尔可夫决策链，其具有不同的状态和动作空间，但是两者的动力学和奖励函数是相同的。</p><p><strong>（2）方法概述</strong></p><p>该论文所提出的方法如下图所示，包含两个重放池，一个是无动作的观测数据 $(s_{obs},s_{obs}‘)\in D_{obj}$，另一个是包含动作条件的交互数据 $(s_{int},a_{int},s_{int}’,r_{int})\in D_{int}$，交互数据在训练期间会更新，而观测数据仅仅是初始的观测数据集。</p><blockquote><p><strong>重放池 (reply pool)</strong>：存储了智能体过去经历过的（状态，动作，奖励，新状态）的数据结构，通过采样这个池中的数据进行训练，可以从过去的经验中学习更多的规律，提高决策能力。</p></blockquote><p><img src="https://img.mahaofei.com/img/202311091616113.png" alt="image.png"></p><ul><li>左图：从动作条件重放池中采样数据 $(s_{int},a_{int},s_{int}‘,r_{int})$，将观测状态分别编码成特征 $h_{int},h_{int}’$，训练一个可逆的模型，来从特征中预测动作 $a_{int}$。</li><li>中图：将这个可逆模型用于根据观测状态特征 $h_{obs},h_{obs}'$，预测离线视频中的缺失的机器人动作 $\hat a_{int}$，将轨迹中最后一步设置为很大的奖励，前面其它步骤都设置为很小的奖励。</li><li>右图：使用 adversarial domain confusion(ADS)来对齐特征，最后使用离线策略强化学习算法，对于数据 $((h_{int},h_{obs}),(a_{int},\hat a_{int}),(h_{int}‘,h_{obs}’),(r_{int},\hat r_{obs}))$ 进行训练。</li></ul><blockquote><p><strong>Adversarial Domain Confusion (ADC)</strong>：通过最小化源域和目标域之间的特征分布距离来实现跨域的迁移学习。</p></blockquote><p><strong>（3）动作预测</strong></p><p>本文通过监督学习训练了一个参数为 $\theta$ 的逆模型，根据一对不变的特征编码 $(h,h’)$ 计算机器人动作。由于机器人与人类视频环境相同，我们应该能够预测任一马尔可夫决策过程的数据的操作。</p><p>损失使用预测动作 $\hat a_{int}=f_{inv}(h_{int},h_{int}';\theta)$ 和真实动作的均方误差 $a_{int}$：</p><p>$$<br>L_a(a_{int},h_{int},h_{int}‘,\theta)=||a_{int}-f_{inv}(h_{int},h_{int}’;\theta)||^2<br>$$</p><p>本文使用逆模型预测人手视频中的动作数据，并用它们来训练强化学习算法。</p><p><strong>（4）奖励生成</strong></p><p>由于强化学习使用观测数据的一个障碍就是缺乏奖励，虽然可以通过上面训练的逆模型预测奖励和动作，但实际上效果可能不会很好。</p><p>本文使用了替代方案，将观测数据轨迹的最后一个时间步长分配一个大的恒定奖励，之前的每一个时间步长分配一个小的恒定奖励。</p><p>这种方式目的是保证观测数据在轨迹结束时达到目标状态。至于其中的不准确之处，可以通过机器人收集的交互数据训练来消除。</p><p><strong>（5）域自适应</strong></p><p>要使用观测数据 $(s_{obs},s_{obs})'$，需要将其映射到一个不变的量 $h$。</p><p>为了实现这个目的，本文训练了一种特征编码器 $f_{enc}$，来从观测状态 $s$ 中学习编码表示 $h=f_{enc}(s;\psi)$，这种编码器应该包含所有相关的信息，并对与观测的域来说是不变的。</p><p>本文还训练了一个鉴别器，用于区分观测数据中提取的特征 $h_{obs}$ 和机器人交互数据中提取的特征 $h_{int}$。</p><p>将特征编码器和鉴别器使用对抗性学习方法进行训练，过程中编码器试图最小化鉴别器对编码特征的域的正确分类能力，鉴别器试图最大化分类能力。</p><p>最终获得的编码器就是我们需要的，将观测数据和机器人交互数据映射到不变量 $h$ 的编码器。</p><p><strong>（6）联合优化</strong></p><p>将领域自适应损失和逆模型的损失进行联合优化。</p><h1>2 Learning Generalizable Robotic Reward Functions from “In-The-Wild” Human Videos</h1><blockquote><p><strong>标题</strong>：从“野外”人类视频中学习可推广的机器人奖励函数<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：Robotics: Science and Systems (RSS)<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://sites.google.com/view/dvd-human-videos">https://sites.google.com/view/dvd-human-videos</a></p></blockquote><h2 id="2-1-目标问题-3">2.1 目标问题</h2><p>要实现通用型机器人完成各类任务，关键是机器人能够知道任务成功和奖励的能力，该奖励函数还必须能够在不同环境、任务、对象中推广。</p><p>由于收集大规模机器人交互数据是一件十分复杂困难的问题，而人类视频中则包含了大量的不同环境中的任务信息。</p><p>本文提出了一种不可知域视频鉴别器(Domain-agnostic Video Discriminator, DVD)，通过训练鉴别器来分类两个视频是否执行相同的任务学习多任务奖励函数。并通过少量的机器人训练数据学习人类视频的广泛数据集进行推广。</p><p>要解决的问题：</p><ul><li>人类的 wild data 和机器人的观测空间有着巨大的域变换，不管是 agent 的形态、还是场景的外观。</li><li>人类的动作空间和机器人的动作空间不同，可能不能很好的实现动作的映射</li><li>人类视频很多情况下是低质量的、有噪声的，还有着复杂的背景或视角</li></ul><p>解决思路：</p><ul><li>训练一个分类器预测两个视频是否完成的是同一个任务，也就是不可知域视频鉴别器（DVD）</li><li>训练完成后，DVD 能够将人类视频作为演示，机器人的行为作为另一个视频，输出一个分数，衡量任务成功的奖励。</li></ul><h2 id="2-2-方法-3">2.2 方法</h2><p><strong>（1）Domain-Agnostic Video Discriminators</strong></p><ul><li>一个预训练视频编码器将视频 $d_i$ 编码为特征 $h_i$</li><li>一个全连接神经网络，预测两个视频是否完成同样的任务<ul><li>损失函数设置见原文</li><li>奖励函数通过训练分类器来获得</li></ul></li></ul><p>本文的关键是训练一个分类器来学习 $R_\theta$，该分类器两个视频作为输入，判断两个视频是否属于同一个任务。视频可以来自于人类数据集或机器人数据集。</p><p>首先对视频进行采样，设两个视频为 $d_i$ 和 $d_j$，采样一批视频 $(d_i,d_i’,d_j)$ 其中 $d_i$ 和 $d_i’$ 是完成相同的任务，$d_j$ 是完成不同的个任务，最小化平均交叉熵损失训练 $R_\theta$，损失函数见原文，最终得到奖励函数如下：</p><p>$$<br>R_\theta(d_i,d_j)=f_{sin}(f_{enc}(d_i),f_{enc}(d_j);\theta)<br>$$</p><p>其中 $h=f_{enc}$ 是一个预训练的视频编码器，$f_{sin}(h_i,h_j;\theta)$ 是一个参数为 $\theta$ 的全连接神经网络，用来预测两个视频编码特征 $h_i,h_j$ 是否完成同样的任务。</p><p><strong>（2）使用 DVD 执行任务</strong></p><p>使用视觉模型预测控制(Visual Model Predictive Control, VMPC)实现。</p><p><img src="https://img.mahaofei.com/img/202311101115799.png" alt="image.png"></p><ol><li>使用 SV2P 模型训练动作条件视频预测模型 $\rho$</li><li>使用交叉熵和该动作模型 $\rho$ 选择与人类演示最相似的动作<ol><li>对输入图像，从动作分布中采样多个动作序列，并使用动作模型 $\rho$ 预测相应的未来轨迹</li><li>将每个预测轨迹和人类演示视频，输入DVD，得到任务相似性分数</li><li>执行与演示图象具有最高相似性的动作轨迹</li></ol></li></ol><h1>3 PLAS: Latent Action Space for Offline Reinforcement Learning</h1><blockquote><p><strong>标题</strong>：PLAS：离线强化学习的潜在行动空间<br><strong>作者团队</strong>：卡耐基梅隆大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2021<br><strong>代码</strong>： <a href="https://github.com/sfujim/BCQ">https://github.com/sfujim/BCQ</a></p></blockquote><h2 id="3-1-目标问题-2">3.1 目标问题</h2><p>离线强化学习可以从固定的数据集中学习策略。</p><p>在机器人中，数据收集十分麻烦且有一定的危险性，现有的方法从离线数据集中进行学习，性能十分受限。</p><p>本文提出了潜在动作空间中的策略(Policy in the Latent Action Space, PLAS)。</p><h2 id="3-2-方法-2">3.2 方法</h2><p><strong>（0）原理基础-离线 RL</strong></p><p>给定一个固定的离线数据集 $D={(s_t,a_t,r_t,s_{t+1})_i}$，难点在于该数据集没有覆盖马尔科夫决策过程 MDP 的整个状态空间和动作空间。</p><p>离线 RL 目的就是学习能使奖励最大化的策略，而策略受到我们对马尔可夫决策过程的了解，马尔可夫决策过程则是从有限的数据集中推理得到的。</p><p>但如果考虑离线 RL 的目标是最大化 MDP 在有限数据集下的累计回报，也能作为近似替代。并且在近似 Q 函数的时候会存在推理误差，</p><p><img src="https://img.mahaofei.com/img/202311122207187.png" alt="image.png"></p><p>给定一个状态，潜在策略输出一个潜在动作，使用解码器将其解码为动作空间输出。（可以添加扰动层来增加泛化能力）</p><p><strong>（1）潜在动作空间中的策略（Policy in Latent Action Space, PLAS）</strong></p><p>给定离线数据集，本文使用条件变分自动编码器（Conditional Variational Autoencoder, CVAE）对策略进行建模。为了使策略约束在数据集的范围内，考虑使用确定性策略，从状态映射到潜在动作，再用解码器得到实际动作。</p><p><strong>（2）泛化</strong></p><p>潜在策略再数据集范围内能够提供约束，但是在训练的时候，本文允许了从分布外的行为的发生，即添加了一个扰动层，设置了一个超参数限制扰动层的动作输出残差。</p><p>当然，如果数据集再状态-动作空间中有着非常高的覆盖率，那么这个扰动层就是不必要的。</p><h1>4 Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot</h1><blockquote><p><strong>标题</strong>：演示引导强化学习与手术机器人任务自动化的高效探索<br><strong>作者团队</strong>：香港中文大学（刘云辉团队）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/med-air/DEX">https://github.com/med-air/DEX</a></p></blockquote><h2 id="4-1-目标问题">4.1 目标问题</h2><p>虽然基于强化学习的方法为手术自动化提供了可能的方案，但是通常需要大量收集数据才能进行学习。因此本文目的是提高从演示中探索学习的效率，有效地利用专家演示数据。</p><p>具体而言，目前的问题如下：</p><ul><li>使用强化学习，如果不给出演示数据而仅通过探索学习，需要收集大量的数据来解决任务；</li><li>使用演示数据的方法，例如赋予演示数据相对于机器人探索数据更高的优先级，效率仍然低下，设置额外奖励函数的方法不仅只能针对特定环境，且容易引起局部最优；</li><li>使用 actor-critic 框架，通过正则化 actor 损失来衡量机器人与专家之间的行为差异，但是这种方式效率较低（尤其在初期机器人与演示差距较大情况下），且没有考虑 critic 的正则化，容易导致高估问题。</li></ul><p>本文贡献：</p><ul><li>提出一种 actor-critic 框架，降低 critic 的高估问题，提高强化学习过程中类似专家的行动进行探索。</li><li>使用非参数引导传播，实现未观测状态的探索</li><li>在 SurRoL 手术机器人上实验验证，效果优秀，同时部署在 dVRK 上，同样表示出强大的潜力。</li></ul><blockquote><p>dVRK(da Vinci Research Kit，达芬奇手术机器人系统)</p></blockquote><h2 id="4-2-方法-2">4.2 方法</h2><p>DEX(Demonstration-guided EXploration)，演示引导探索。</p><p><strong>（0）问题定义</strong></p><p>将手术机器人动作学习考虑为一个 off-policy 的智能体，在由马尔可夫决策过程构建的环境中进行交互。</p><blockquote><p>off-policy，指智能体不使用当前的策略来决定行动，而是使用不同的策略来生成行为数据，从过去的经历中学到最优的行为决策方法。</p></blockquote><p>在 $t$ 时刻，机器人根据当前状态 $s_t$ 以及确定性策略 $\pi$ 执行行动，环境用 $r_t=r(s_t,a_t)$ 奖励智能体，然后状态转移 $s_{t+1}$。</p><p>循环此过程，每次智能体将经验 $(s+t,a_t,r_t,s_{t+1})$ 存入重放缓冲区 $D_A$。</p><p>同时设置一个演示缓冲区 $D_E$，用于存放专家策略 $\pi$ 经验。</p><p><img src="https://img.mahaofei.com/img/202311151037433.png" alt="image.png"></p><p>如图，该方法由两部分组成：</p><ul><li>基于 actor-critic 的策略学习模块（右下角），用于从演示数据中指导探索；</li><li>基于最近邻匹配和局部加权回归的非参数模块（左上角），用于将与当前状态相差过大的演示传播到为当前状态。</li></ul><p><strong>（1）专家引导的 actor-critic 框架</strong></p><p>现有的 actor-critic 方法通过最大化预期回报来学习最优策略，但是如果 Q 值估计不准确，会阻碍探索。本文通过利用智能体和专家策略之间的动作差距来增强环境奖励。</p><p>$$<br>\max_{\pi}\mathbb{E}<em>{\pi}\left[\sum</em>{t=0}^{\infty}\gamma^{t}(r_{t}-\alpha d(a_{t},a_{t}^{e}))\right],a_{t}^{e}:=\pi^{e}(s_{t}),<br>$$</p><p>其中 $\alpha$ 是探索系数，$d()$ 衡量智能体动作和专家动作之间的相似性距离度量。</p><p>基于此奖励，本文设计了正则化 Q 函数（critic），并最小化动作价值和状态价值的差距。</p><p><strong>（2）有限演示情况下的引导的传播</strong></p><p>智能体在初始学习阶段很容易探索演示未覆盖的区域，无法实现监督 actor 探索。</p><p>常规的解决思路有行为克隆，但是当状态相差较大时，策略与专家行动仍会有较大的不同。因此本文使用非参数回归模型，从有限的演示中将经验传播实现更稳定的引导。</p><p>首先从演示缓冲区采样一小批状态和动作，然后给定一个当前状态，在一小批状态中搜索，利用 k 近邻方法找到最接近的状态，然后使用指数和函数的局部加权回归方法近似专家策略。</p><p>$$<br>\hat{\pi}^e(s)=\frac{\sum_{i=1}^k\exp\left(-|s-s^{(i)}|<em>2\right)\cdot a^{(i)}}{\sum</em>{i=1}^k\exp\left(-|s-s^{(i)}|_2)\right)}.<br>$$</p><h1>5 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics</h1><blockquote><p><strong>标题</strong>：剩余技能策略：学习基于技能的适应性行动空间，用于机器人强化学习<br><strong>作者团队</strong>：昆士兰科技大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://krishanrana.github.io/reskill">https://krishanrana.github.io/reskill</a></p></blockquote><h2 id="5-1-目标问题-2">5.1 目标问题</h2><p>基于技能的学习已经成为加速机器人学习的方法，技能从专家演示中提取，是短序列的单步操作（平移、抓取、抬起等动作），这些技能嵌入到潜在空间中，构成上层 RL 策略的行动空间。但是这种方式存在一些问题：</p><ul><li>对所有技能进行随机抽样探索，效率极低，因为其中只有一小部分技能与当前执行的任务相关，并且这些相关的技能通常不会聚集在技能空间的同一邻域内。</li><li>该方法假设技能是最优的，并且下层的任务来自于技能空间的相同分布，因此学习的通用性和变化适应性有限，例如从移动方块中学习技能，则无法应对障碍物、物体变化、不同摩擦等情况。</li></ul><p>为解决上述问题，本文提出了以下创新方法，称为残差技能策略（Residual Skill Policies，ReSkill）：</p><ul><li>状态条件技能先验：对相关技能进行采样来引导探索</li><li>底层残差策略：通过对技能进行细粒度的技能适应，实现任务变化的适应</li></ul><h2 id="5-2-方法">5.2 方法</h2><p>总的来说，该方法将经典控制器产生的演示轨迹分解为与任务无关的技能，并将其嵌入到连续到技能空间中，利用技能空间实现真正的通用学习，上层智能体能够从技能空间中访问但不动作，降低了对数据集详细程度的要求。</p><ul><li>从现有控制器中提取技能</li><li>学习技能嵌入和先验技能</li><li>训练一个分层强化学习策略，在技能空间中使用底层残差适应性策略。</li></ul><p><img src="https://img.mahaofei.com/img/202311161019261.png" alt="image.png"></p><p><strong>（1）数据收集</strong></p><p>本文通过手动控制收集演示数据（基本操作任务，如推物体、抓物体），虽然任务简单，但轨迹包含复杂的技能，可以重新组合解决复杂的任务。</p><p>轨迹是由 state-action 成对组成的，本文从中随机切片 $H$ 长度的片段进行无监督技能提取，利用提取的动作 a 和状态 s 学习下一小节中的 state-action。</p><p>其中状态 s 包括关节角度、关节速度、夹具位置、物体位置，动作是连续的 4D 向量，包括末端位置和速度。</p><p><strong>（2）学习强化学习的状态条件技能空间</strong></p><ul><li>将提取的技能嵌入到潜在空间中：使用变分自动编码器 VAE 将技能 $a$ 嵌入到潜在空间中，VAE 包括编码器和解码器，编码器将完整的 state-action 序列编码为 $z$，解码器根据当前状态 $s_t$ 和技能编码 $z$ 重建动作。</li><li>在探索过程中采样的技能状态条件先验：学习潜在技能空间上的条件概率密度。传统的高斯密度不能处理多模态信息，本文使用 real NVP 方法，实值非体积保留变换。学习从 $Z\times S-&gt;G$ 的映射，该映射就可以从简单分布 G 变换到技能空间 Z，因此 f 就是技能先验。</li></ul><blockquote><p><strong>变分自编码器</strong>，是一种深度生成模型<br><strong>传统</strong>：传统的自编码器包括编码器和解码器两部分，经过反复训练，输入数据被编码成一个编码向量，编码向量的每一个维度表示学习到的数据的特征，解码器尝试从编码向量中解码原始输入<br><strong>缺陷</strong>：传统的方法，使用单个值表示输入在某个潜在特征的表现。但实际上，将潜在特征表示为可能的取值范围会更合理。<br><strong>改进</strong>：因此变分自编码器就是使用取值的概率分布，代替原来的单值表示特征。<br><strong>优势</strong>：每个潜在特征表示为概率分布，解码时从潜在状态分布中随机采样，生成一个编码向量作为解码器的输入。实现了连续且平滑的潜在空间表示（潜在空间中彼此相邻的值重构出的结果相似）<br>参考理解:<a href="https://zhuanlan.zhihu.com/p/64485020">https://zhuanlan.zhihu.com/p/64485020</a></p></blockquote><p><strong>（3）状态条件技能空间中的强化学习</strong></p><p>一旦训练完成，解码器和技能先验权重就会被冻结，并合并到 RL 框架中。高级强化学习策略 $\pi$ 是一个神经网络，将状态映射到技能先验变化中的向量 g，在转换为潜在技能 Z。</p><p>然后解码器根据技能范围 H 的当前状态顺序重构动作。同时有一个底层残差策略，调整解码后的技能。</p><h2 id="5-3-总结">5.3 总结</h2><p>该方法是一种基于技能的强化学习方法。</p><ol><li>数据收集：使用最基本的控制器生成一些基本任务轨迹（移动、抓取），然后将这些轨迹分割成固定长度的序列，每一小段包括动作和对应的状态。</li><li>学习技能空间，使用变分自编码器将技能编码到潜在空间中；使用realNVP将技能潜在空间+机器人状态空间映射到简单分布空间（高斯分布），这样可以直接根据状态采样技能，称为技能先验。</li><li>强化学习：使用一个高层策略网络，根据当前的状态生成一个向量，根据技能先验（与当前状态有关的技能）中选择一个技能，利用技能解码器解码成机器人动作。</li></ol><h1>6 Watch and Match: Supercharging Imitation with Regularized Optimal Transport</h1><blockquote><p><strong>标题</strong>：观看与匹配：通过正则化最优传输增强模仿<br><strong>作者团队</strong>：纽约大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://rot-robot.github.io/">https://rot-robot.github.io/</a></p></blockquote><h2 id="6-1-目标问题-2">6.1 目标问题</h2><p>目前模仿学习通常使用逆强化学习，给出演示的情况下，交替推理奖励函数和策略。但是这种方式需要大量的在线交互来解决复杂的控制问题。</p><p>本文提出了正则化最佳传输（Regularized Optimal Transport，ROT）方法，即使只有少量的演示，也能自适应的匹配轨迹奖励与行为克隆，加速模仿。</p><blockquote><p>基于最佳传输的模仿学习（Optimal Transport，OT）：模仿学习实在给定专家策略或轨迹的情况下学习行为行为策略 $\pi^b$，逆强化学习根据专家轨迹 $T^e$ 推断奖励函数 $r^e$，然后利用奖励优化策略来得到行为策略 $\pi^b$。为了计算 $r^e$，基于 OT 的逆学习方法就是一种思路。专家轨迹和行为轨迹的接近程度可以通过测量两个轨迹之间的最佳传输来计算。</p></blockquote><h2 id="6-2-方法-2">6.2 方法</h2><p><strong>（1）BC 预训练</strong></p><p>使用 BC 对专家演示的数据进行随机初始化策略的训练。</p><p>BC 对应求解公式中的最大似然问题，这里的专家轨迹 $T^e$ 指的是专家演示，训练后，它能够使 $\pi^{BC}$ 模仿与演示中想对应的动作，但是如果出现未见过的状态，那么很容易会导致推理失败。</p><p><strong>（2）在线 IRL 微调</strong></p><p>在 BC 训练的模型基础上，进行在线微调策略。由于本文操作没有明确的任务奖励，因此使用基于 OT 的轨迹匹配获得奖励。（本文使用了 n 步 DDPG 方法实现连续控制）</p><ol><li>正则化微调：由于在线部署期间很容易因为错误累计导致分布偏移，本文通过基于引导 RL 和离线 RL 将 $\pi^{ROT}$ 与 BC 损失相结合来规范 $\pi^{ROT}$ 的训练，此处设置了一个 $\lambda(\pi)$ 自适应权重来控制两个损失项的贡献。</li><li>柔性 Q 滤波的自适应正则化：自适应权重调整 $\lambda(\pi)$，通过比较当前策略 $\pi^{ROT}$ 和与训练策略 $\pi^{BC}$ 在一段重放缓冲区采样的一批数据的性能表现来完成。</li><li>基于图像观测的考虑：对视觉观测进行数据增强，将图像输入 CNN 编码器，获得 OT 奖励的计算，减少 ROT 模仿过程中的非平稳性。</li></ol><h2 id="6-3-实验">6.3 实验</h2><p>本文的模型包括三个神经网络：encoder、actor、critic，三者均使用均方误差进行训练。</p><p>使用 n 步 DDPG 作为 RL 主干，actor 使用确定性策略梯度进行训练。critic 使用 clipped double Q-learning 进行训练，主要时为了减少高估问题，因此使用两个 Q 函数实现 critic 的学习。</p>]]></content>
    
    
    <summary type="html">基于强化学习的机器人动作模仿方法论文调研与笔记。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【模仿学习笔记】行为克隆 Behavior Cloning</title>
    <link href="https://www.mahaofei.com/post/1d3a3b82.html"/>
    <id>https://www.mahaofei.com/post/1d3a3b82.html</id>
    <published>2023-11-03T09:12:44.000Z</published>
    <updated>2023-11-03T09:12:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、行为克隆概念</h1><p>行为克隆属于模仿学习中的方法，不是强化学习。</p><blockquote><p>强化学习：从环境给出的奖励中进行监督；<br>模仿学习：从人类动作经验中监督<br>区分两者主要在于，模仿学习没有奖励回报，知识模仿专家动作。</p></blockquote><h1>二、行为克隆过程</h1><ol><li>观测当前状态 $s_t$</li><li>策略网络做出预测 $p_t$</li><li>专家的动作是 $a_t^*$，向量化从而得到 $y_t$</li><li>计算损失 CrossEntropy($y_t,p_t$)</li><li>使用梯度下降来更新策略网络</li></ol><h1>三、行为克隆的优势与不足</h1><p>如果当前的状态出现在训练数据中，则可以根据行为克隆训练得到的策略网络，执行类似于人类专家的动作。</p><p>但是如果当前状态没有出现在训练数据中，那么策略网络输出的动作可能不会很好，而且错误会累加。这种情况尤其出现在状态极为复杂的情况下。</p><h1>一、模仿学习简介</h1><p>模仿学习是让智能体从专家示例中学习，从而像人类专家一样能够智能决策。</p><ul><li>不同于传统的监督学习算法，监督学习需要考虑大量的约束条件，根据约束设计特定的监督方法来引导智能体；</li><li>而模仿学习旨在让人类为之恩能够提提供大量的示例行为，利用这些专家示例来教会智能体进行决策。</li></ul><p>目前模仿学习主要分为两类：</p><ul><li>行为克隆：尝试最小化智能体策略和专家策略的动作差异，把模仿学习作为回归或分类任务学习；</li><li>对抗式模仿学习：通过逆强化学习来构建对抗的奖励函数，最大化这个奖励函数来模仿专家行为。</li></ul><h1>二、数学基础</h1><h2 id="2-1-马尔可夫决策过程">2.1 马尔可夫决策过程</h2><p>考察一个有限状态的马尔可夫链，状态空间$S={1,2,3,\cdots,|S|}$，由于马尔可夫性，状态转移与历史状态无关，因此对于有限状态的马尔可夫链，状态转移矩阵 $P$ 如下：</p><p>$$<br>P=\begin{bmatrix}{}<br>p_{1,1} &amp; p_{1,2} &amp; \cdots &amp; p_{1,|S|}\<br>p_{2,1} &amp; p_{2,2} &amp; \cdots &amp; p_{2,|S|}\<br>\cdots &amp; \cdots &amp; \cdots &amp; \cdots\<br>p_{|S|,1} &amp; p_{|S|,2} &amp; \cdots &amp; p_{|S|,|S|}<br>\end{bmatrix}<br>$$</p><p>其中每一个元素 $p_{i,j}$ 表示由状态 $i$ 转移到 $j$ 的概率，满足：</p><p>$$<br>\sum^{|S|}<em>{i=1}P</em>{i,j}=1, \sum^{|S|}<em>{j=1}P</em>{i,j}=1<br>$$</p><p>为了完整表示状态转移过程，需要指定初始状态分布 $\rho$ ，便于递归计算某个状态 $i$ 在 $t$ 时刻出现的概率（上一时刻为 $j$ 状态的概率 $\times$ $j$ 状态到 $i$ 状态的状态转移概率）：</p><p>$$<br>\mathbb{P}(s_t=i)=\sum_{j\in S}\mathbb{P}(s_{t-1}=j)p_{j,i}<br>$$</p><p>对于一个马尔可夫决策过程，状态转移不仅受到上移时刻状态的影响，还取决于当前动作，因此不仅要考虑状态，还要额外考虑动作和奖励，即一个马尔可夫决策过程可以表示为 $M=(S,A,\rho,P,r)$ ，数学上表示为 $P(s_{t+1}|s_t, a_t)$。</p><p>为了表示动作产生的过程，引入了策略 $\pi(a|s)$ ，代表在状态 $s$ 处选择动作 $a$ 的概率，因为我们最多有 $|S|$ 个状态和 $|A|$ 个动作，$\pi$ 也可以用一个 $|S| \times |A|$ 的矩阵来表示。</p><blockquote><p>参考链接：</p><ol><li><a href="https://www.lamda.nju.edu.cn/xut/docs/Imitation_Learning.pdf">许天, 李子牛, 俞扬. 模仿学习简洁教程. 2021</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">学习模仿学习中的行为克隆部分的笔记</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>【强化学习笔记】强化学习基础</title>
    <link href="https://www.mahaofei.com/post/cfa8c737.html"/>
    <id>https://www.mahaofei.com/post/cfa8c737.html</id>
    <published>2023-11-02T02:01:55.000Z</published>
    <updated>2023-11-02T02:01:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、基本概念</h1><h2 id="1-1-专业术语">1.1 专业术语</h2><p><strong>（1）状态（State）</strong>：状态可以被理解为当前环境的情况。</p><p><strong>（2）动作（Action）</strong>：动作是智能体（agent）采取的行为。</p><p><strong>（3）策略（Policy）</strong>：策略是用于在给定观测状态下做出决策的函数，通常表示为 $\pi(a|s)$，其中 $a$ 是动作，$s$ 是状态。强化学习的目标是学习策略函数，通常以概率密度函数的形式表示。</p><p><strong>（4）奖励（Reward）</strong>：奖励定义了奖励的方式，对强化学习的结果产生重要影响。</p><p><strong>（5）状态转移（State Transition）</strong>：状态转移表示在当前状态下，当智能体执行一个动作后，环境可能随机转移到的下一个状态的概率，通常表示为 $P(S’|S, A)$。</p><p><strong>（6）回报（Return）</strong>：回报又被称为未来奖励的累积，通常表示为 $U_t = R_t + R_{t+1} + R_{t+2} + \ldots$。</p><p><strong>（7）折扣回报（Discounted Return）</strong>：折扣回报考虑未来奖励的折扣效应，用折扣率 $\gamma$ 表示，通常表示为 $U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots$。</p><p><strong>（8）动作价值函数（Action-Value Function）</strong>：动作价值函数表示在给定状态和动作下，智能体可以获得的期望回报，通常表示为 $Q_\pi(s_t, a_t) = E[U_t | S_t=s_t, A_t=a_t]$。最优策略下的动作价值函数被表示为 $Q^*(s_t, a_t) = \max_\pi Q_\pi(s_t, a_t)$，使用动作价值函数可以评估当前动作的质量。</p><p><strong>（9）状态价值函数（State-Value Function）</strong>：状态价值函数表示在给定状态下，按照策略函数的预期回报，通常表示为 $V_\pi(s_t) = E_A[Q_\pi(s_t, A)]$。状态价值函数可以告诉我们当前状态的好坏程度。</p><p><strong>（10）交叉熵（Cross Entropy）</strong>：交叉熵用于度量两个概率分布之间的差异，通常表示为 $H(\textbf p, \textbf q) = -\sum^m_{j=1}p_j\cdot \log(q_j)$。当两个概率分布相同时，交叉熵达到最小值。</p><h2 id="1-2-强化学习的随机性">1.2 强化学习的随机性</h2><p><strong>（1）Action动作的随机性</strong></p><p>因为动作是根据策略函数随机抽样得到的，因此agent有可能做策略中的任何一种动作，虽然这些动作的概率有大有小，但是动作本身是随机的。</p><p><strong>（2）State transitions状态转移的随机性</strong></p><p>假定agent作出了一个动作，环境会用概率随机抽样，给出下一个状态。</p><h2 id="1-3-强化学习如何控制agent">1.3 强化学习如何控制agent</h2><p><strong>（1）如果有策略函数 $\pi(a|s)$</strong></p><ol><li>给定一个观测状态 $s_t$</li><li>利用策略函数从所有可能的动作中随机采样 $a_t~\pi(\cdot|s_t)$</li></ol><p><strong>（2）如果有最优的动作价值函数 $Q^<em>(s,a)</em>$</strong></p><ol><li>给定一个观测状态 $s_t$</li><li>最大化 $a_t=argmax_a Q^*(s_t,a)$ 来选择动作</li></ol><h1>二、价值学习 Deep Q-Network(DQN)</h1><p>$U_t$ 反映未来奖励的总和，因此我们要知道 $U_t$ 的大小，由于其是一个随机变量，我们可以对 $U_t$ 求期望，只留下 $s_t$ 和 $a_t$ 两个变量。</p><p>$$Q_\pi(s_t,a_t)=E[U_t|S_t=s_t,A_t=a_t]$$</p><p>要想进一步消除策略函数 $\pi$，可以对 $Q_\pi$ 关于 $\pi$ 球最大化，记为 $Q^*$</p><p>$$Q^*(s_t,a_t)=max_\pi Q_\pi(s_t,a_t)$$</p><p>这个参数告诉我们不管在什么情况 $s_t$ 下做动作 $a_t$，那么期望顶多就是 $Q^*(s_t,a_t)$。</p><blockquote><p>目标：完成任务（最大化总回报）<br>问题：如果已知 $Q^<em>(s,a)$，那么最好的动作就是 $a^</em>=argmax_a Q^<em>(s,a)$，因为 $Q^</em>$ 指示了该agent在s状态下选择a动作的好坏程度<br>挑战：我们不知道 $Q^*(s,a)$</p></blockquote><p><strong>（1）什么是 DQN</strong></p><p>我们使用神经网络 $Q(s,a;w)$ 来近似 $Q^*(s,a)$，其中 w 是要近似的参数，s 是输入，a 是输出是对所有动作的打分。</p><p><img src="https://img.mahaofei.com/img/202311030917032.png" alt="image.png"></p><p>当前观测到状态 $s_t$，用DQN把 $s_t$ 作为输入，为所有动作打分，选出分数最高的动作作为 $a_t$。</p><p>agent 作出动作 $a_t$ 后，环境会改变，用状态转移函数 $p$ 随机抽取一个新的状态 $s_{t+1}$，环境还会告诉我们一个回报 $r_t$，这个 $r_t$ 就是训练DQN的关键。</p><p><strong>（2）如何训练 DQN</strong></p><p>常规的网络训练过程如下：</p><ol><li>首先对任务结果做一个预测 $q=Q(w)$</li><li>完成任务后获得目标 $y$</li><li>计算损失 $L=\frac{1}{2}(q-y)^2$</li><li>计算梯度 $\frac{\partial L}{\partial w}=\frac{\partial L}{\partial q}\cdot \frac{\partial q}{\partial w}$</li><li>更新参数 $w_{t+1}=w_t-\alpha \cdot\frac{\partial L}{\partial w}|_{w=w_t}$</li></ol><p>但这种方式需要完整完成一次任务后才能更新参数，而能否执行一部分任务后就开始更新参数，因此有了 Temporal Difference Learning （TD算法），过程如下：</p><ol><li>首先对任务结果做一个预测 $q=Q(w)$</li><li>执行一部分任务后，对任务结果再进行预测 $y$，此时的 $y$ 包括已经完成的部分和对剩下部分的预测，因此比 $q$ 更可靠</li><li>计算损失 $L=\frac{1}{2}(q-y)^2$</li><li>计算梯度 $\frac{\partial L}{\partial w}=\frac{\partial L}{\partial q}\cdot \frac{\partial q}{\partial w}$</li><li>更新参数 $w_{t+1}=w_t-\alpha \cdot\frac{\partial L}{\partial w}|_{w=w_t}$</li></ol><p>在深度强化学习中，也就是下面这个公式</p><p>$$Q(s_t,a_t;w)\approx r_t+\gamma\cdot Q(s_{t+1},a_{t+1};w)$$</p><p>对未来奖励总和的期望，就是真实已经观测到的奖励，加在t+1时刻对未来奖励的期望。</p><ol><li>首先进行预测 $Q(s_t,a_t;w_t)$</li><li>获得TD目标 $y_t=r_t+\gamma\cdot Q(s_{t+1},a_{t+1};w_t)=r_t+\gamma\cdot max_a Q(s_{t+1},a;w_t)$</li><li>计算损失 $L_t=\frac{1}{2}[Q(s_t,a_t;w)-y_t]^2$</li><li>进行剃度下降 $w_{t+q}=w_t-\alpha\cdot\frac{\partial L_t}{\partial w}|_{w=w_t}$</li></ol><p><img src="https://img.mahaofei.com/img/202311031636429.png" alt="image.png"></p><p><strong>（3）经验回放</strong></p><p>之前我们使用在线梯度下降来更新 $w$，以此来减小TD errer $\delta_t=q_t-t_t$。</p><p>我们定义一个经验transition为$(s_t,a_t,r_t,s_{t+1})$，传统的方法再每使用一个transition后就会丢弃它，这回造成经验的浪费。此外传统的方法还忽略了不同经验之间的相关性。</p><p>将最近的n个transition存储进一个replay buffer，当有新的经验进来后，就删除老的transition。</p><ol><li>每次从buffer中随机抽取一个transition</li><li>计算TD error</li><li>极端梯度</li><li>进行随机梯度下降（实际一般使用minibatch SGD，一次取多个transition）</li></ol><p><strong>优先经验回放</strong>：为了解决数据的不均匀性，可以使用重要性抽样代替平均采样。可以根据TD error抽样，误差越大的，transition被抽样的概率越大。</p><p><strong>学习率比例设置</strong>：如果一个transition有较大的抽样概率，那么其学习率应该设置的比较小。</p><p><strong>更新TD error</strong>：如果一个transition没有被用过，那么就设置它的TD error为最大值，在训练DQN的同时，对TD error进行更新。</p><h1>三、策略学习</h1><p>策略函数 $\pi(a|s)$是一个概率密度函数，对每一个给定的状态 $s$，策略函数会抽取一个最优的动作 $a$ 作为将要执行的动作。</p><p>理想情况下，列出所有的状态和动作，计算所有状态和动作之间的概率即可。</p><p>但是实际情况下有无数个状态，不可能记录所有的状态对应的动作，因此需要函数近似。一般使用神经网络进行近似，即policy network $\pi(a|s;\theta)$</p><p>状态价值函数$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]$，状态价值函数可以告诉我们当前的局势好不好。在状态已知时，还可以判断策略好不好，策略越好，$V_\pi$ 越大，任务完成成功率越高，$V_\pi$ 可以表示为：</p><p>$$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]=\sum_a\pi(a|s_t)\cdot Q_\pi(s_t,a)$$</p><p>使用神经网络替换策略函数，因此得到：</p><p>$$V_\pi(s_t;\theta)=\sum_a\pi(a|s_t;\theta)\cdot Q_\pi(s_t,a)$$</p><p>给定状态 $s$，策略函数函数越好，价值函数越大。因此可以考虑通过改变神经网络参数 $\theta$，让 $V(s;\theta)$ 变大，基于这个思想，可以求期望：</p><p>$$J(\theta)=E_s[V(S;\theta)]$$</p><p>策略网络越好，$J(\theta)$ 就越大，为了改变 $\theta$，我们使用策略梯度算法。</p><ol><li>观测状态 $s$</li><li>更新策略 $\theta=\theta+\beta\cdot\frac{\partial V(s;\theta)}{\partial \theta}$，做梯度上升，因为我们希望价值函数越大越好。</li></ol><p>对于离散的动作，使用$\frac{\partial V(s;\theta)}{\partial \theta}=\sum_a \frac{\partial \pi(a|s;\theta)}{\partial \theta}\cdot Q_\pi(s,a)$</p><p>对于连续的动作，使用$\frac{\partial V(s;\theta)}{\partial \theta}=E_{A~\pi(\cdot|s;\theta)} [\frac{\partial \pi(a|s;\theta)}{\partial \theta}\cdot Q_\pi(s,a)]$</p><p><img src="https://img.mahaofei.com/img/202311031636316.png" alt="image.png"></p><h1>四、Actor-Crictic</h1><p>状态价值函数的定义如下：</p><p>$$V_\pi(s_t)=\sum_a\pi(a|s_t)\cdot Q_\pi(s_t,a)$$</p><p>策略网络（产生动作）：</p><ul><li>使用神经网络 $\pi(a|s;\theta)$ 来近似策略函数 $\pi(a|s)$</li><li>其中 $\theta$ 是训练的参数</li></ul><p>价值网络（产生评判标准）：</p><ul><li>使用神经网络 $q(s,a;w)$ 来近似价值函数 $Q_\pi(s,a)$</li><li>其中 $w$ 是训练的参数</li></ul><p>因此状态价值函数可以写成</p><p>$$V_\pi(s_t)=\sum_a\pi(a|s_t;\theta)\cdot Q_\pi(s_t,a;w)$$</p><p>同时训练策略网络和价值网络，就称为 Actor-Critic Method，大致步骤如下：</p><ol><li>观测当前状态 $s_t$</li><li>根据策略函数 $\pi(\cdot|s_t;\theta_t)$ 随机采样获得动作 $a_t$</li><li>执行动作 $a_t$，并观测新的状态 $s_{t+1}$ 和回报 $r_t$</li><li>更新价值网络的参数 $w$，使用TD算法</li><li>更新策略网络的参数 $\theta$，使用策略梯度算法</li></ol><p>训练过程中需要同时训练策略网络和价值网络，利用价值网络对策略网络进行评分。训练完成后就不需要价值网络了，只需要策略网络生成动作。</p><h1>五、蒙特卡洛树搜索（Monte Carlo Tree Search）</h1><h2 id="5-1-基本思想">5.1 基本思想</h2><p>蒙特卡洛树搜索的思想是人们必须要向前看很多步，看到未来时间内所有可能的情况，挑选最优的执行动作。</p><ol><li>如果我在此时选择执行动作 $a_t$</li><li>那么未来一段时间环境的反馈是怎么变化的 $s_{t+1}$</li><li>基于这种环境变化，我又会执行动作 $a_{t+1}$</li><li>此时环境又会如何变化</li></ol><p>如果一个agent能够穷举所有的可能性直到任务完成，那么这个任务一定有很高的成功率。</p><h2 id="5-2-过程">5.2 过程</h2><p><strong>（1）选择</strong></p><p>根据分数选择一个动作（假想的动作，实际上并不会执行）；</p><p>首先对所有可能的动作 $a$，计算得分：</p><p>$$score(a)=Q(a)+\eta\cdot\frac{\pi(a|s_t;\theta)}{1+N(a)}$$</p><p>其中 $Q(a)$ 是蒙特卡洛树搜索计算的动作价值<br>$\pi(a|s_t;\theta)$ 是学习好的策略网络，动作越好，策略分数越高<br>$N(a)$ 是给定环境状态 $s_t$ 情况下，目前为止选择动作 $a$ 的次数，如果同一个动作被探索太多次，该项分母就会变大。</p><p><strong>（2）扩展</strong></p><p>假想环境更新；</p><p><strong>（3）评估</strong></p><p>评价状态价值得分 $v$ 和回报 $r$，将动作的分数设为 $\frac{v+r}{22}$；</p><p><strong>（4）备份</strong></p><p>用动作的分数 $\frac{v+r}{2}$ 更新动作价值：</p><p>$$Q(a_t)=mean(the recorded V’s)$$</p><p>将以后所有步的状态价值进行平均。</p><h1>六、连续控制</h1><p>在实际进行强化学习时，可能有离散动作空间（例如上下左右控制游戏人物），也可能是连续动作（机械臂关节控制）。</p><p>在进行离散控制时，可以直接使用分类的思想，得到一个onehot向量，每个向量元素代表执行该动作的得分，以此来获得应该执行那种动作。而连续控制中动作空间是有无穷维的，因此不能直接使用这种思想实现连续控制。</p><p>比较常规的一种解决思路是将动作空间离散化，但这种方式也有问题，例如机械臂的6个自由度，就算每个自由度离散为360个点，那么整个动作空间也有 $360^6$ 个点，这会造成维度灾难，在训练时非常困难。</p><p>因此有两种方式实现连续控制：</p><ul><li>确定性策略网络</li><li>随机策略网络</li></ul><h2 id="6-1-确定策略梯度（Deterministic-Policy-Gradient-DPG）">6.1 确定策略梯度（Deterministic Policy Gradient DPG）</h2><p>考虑一个只有2自由度的机械臂，基座运动范围为(0,180)，机械臂运动范围为(0,360)，因此机械臂的动作空间是 $A=[0,180]\times[0,360]$ 的连续集合，动作就是一个二维向量。</p><p>DPG 是一种 Actor-Critic 方法</p><ul><li>有一个<strong>策略网络</strong>，控制 agent 运动，它根据状态 s 做出决策 a；<br>使用策略网络 $a=\pi(s;\theta)$ 根据输入状态 s，输出一个<strong>确定</strong>的动作 a，这里的动作 a 就是机器人的二维动作向量。</li><li>有一个<strong>价值网络</strong>，不控制 agent，它根据状态 s，给动作 a 打分，从而指导策略网络做出改进。<br>使用价值网络 $q(s,a;w)$，输入状态 s 和动作 a，输出一个实数 value 是对动作的评价，动作越好，value 越大。</li></ul><p>因此 DPG 的原理就是训练这两个网络。</p><p><img src="https://img.mahaofei.com/img/202311041607515.png" alt="image.png"></p><p><strong>（1）价值网络训练</strong></p><ol><li>每次得到一个训练数据 transition $(s_t,a_t,r_t,s_{t+1})$</li><li>用价值网络预测当前时刻 t 下的动作价值 $q_t=q(s_t,a_t;w)$</li><li>用价值网络预测下一时刻 t+1 的动作价值 $q_{t+1}=q(s_{t+1},a_{t+1}‘;w)$ ，其中 $a_{t+1}’=\pi(s_{t+1};\theta)$，这个动作并不是 agent 真正执行的动作，$a_{t+1}'$ 只用于更新价值网络。</li><li>计算 TD error：$\delta_t=q_t-(r_t+\gamma\cdot q_{t+1})$，其中第二项是 TD Target，它一部分是真实观测到的奖励，另一部分是价值网络自己做出的预测。因为我们认为第二项中由于包含本步真实奖励，比单纯的 $q_t$ 更接近真实情况，因此要让 $q_t$ 与 TD Target 接近，也就是让 TD error 尽可能小。</li><li>进行梯度下降更新 w：$w=w-\alpha\cdot\gamma_t\cdot\frac{\partial q(s_t,a_t;w)}{\partial w}$</li></ol><p>但这其中有一个问题，就是计算 TD error $\delta_t=q_t-(r_t+\gamma\cdot q_{t+1})$ 这一步时，会出现 bootstrapping 问题，也就是如果初始值高估或者低估，那么 TD target 就会有高估或低估，并传播回价值网络自身，导致高估或低估一直存在，解决方案就是用不同的神经网络计算 TD Target，也就是用 Target Networks。</p><ol><li>每次得到一个训练数据 transition $(s_t,a_t,r_t,s_{t+1})$</li><li>用价值网络预测当前时刻 t 下的动作价值 $q_t=q(s_t,a_t;w)$</li><li>用价值网络预测下一时刻 t+1 的动作价值 $q_{t+1}=q(s_{t+1},a_{t+1}‘;w^-)$ ，其中 $a_{t+1}’=\pi(s_{t+1};\theta^-)$<br>$\pi(s_{t+1};\theta^-)$ 是 Target policy network 用来代替策略网络，它的网络结构和策略网络一模一样，但是参数不一样。<br>$q(s_{t+1},a_{t+1}';w^-)$ 是 Target value network，它与价值网络结构一样，参数不同。</li></ol><p><strong>（2）策略网络训练</strong></p><p>训练策略网络，需要靠价值网络评价动作的好坏，从而指导策略网络进行改进。</p><p>也就是更新策略网络的参数 $\theta$ 让价值网络认为动作 $a=\pi(s;\theta)$ 更好，也就是改进 $\theta$ 让价值 $q(s,a;w)=q(s,\pi(s;\theta);w)$ 尽可能大。</p><p>由于给定状态 s，策略网络会输出一个确定的动作 a，而如果价值网络也是确定的，那么输出的价值就是确定的。</p><p>因此问题中只需要改变 $\theta$，使得价值 q 变大，也就是计算 $q(s,a;w)$ 对 $\theta$ 的梯度，然后用梯度上升更新 $\theta$，就可以让 $q$ 变大，这个梯度就叫<strong>确定策略梯度 DPG</strong>。</p><p>$$<br>g=\frac{\partial q(s,\pi(s;\theta);w)}{\partial\theta}=\frac{\partial a}{\partial \theta}\cdot\frac{\partial q(s,a;w)}{\partial a}<br>$$</p><p>其中 $a=\pi(s;\theta)$，然后进行梯度上升 $\theta=\theta+\beta\cdot g$</p><p><strong>策略网络和价值网络联合具体步骤如下：</strong></p><ol><li>策略网络做一个决策：$a=\pi(s;\theta)$</li><li>计算价值网络的输出：$q_t=q(s,a;w)$</li><li>用 DPG 更新策略网络： $\theta=\theta+\beta\cdot \frac{\partial a}{\partial \theta}\cdot\frac{\partial q(s,a;w)}{\partial a}$</li><li>利用 Target networks $\pi(s;\theta^-)$ 和 $q(s,a;w^-)$ 计算 $q_{t+1}$</li><li>计算 TD error：$\delta_t=q_t-(r_t+\gamma\cdot q_{t+1})$</li><li>更新价值网络：$w=w-\alpha\cdot\gamma_t\cdot\frac{\partial q(s_t,a_t;w)}{\partial w}$</li><li>更新 Target networks 的参数：$w^-=\tau\cdot w+(1-\tau)\cdot w^-$，$\theta^-=\tau\cdot \theta+(1-\tau)\cdot \theta^-$，其中 $\tau$ 是超参数</li></ol><h2 id="6-2-随机策略用于连续控制">6.2 随机策略用于连续控制</h2><p>首先考虑自由度等于 1 的随机策略连续控制，也就是动作都是实数。</p><p>设 $\mu$ 代表均值，和 $\sigma$ 代表标准差，都是状态 s 的函数。</p><p>用正态分布的概率密度函数作为策略函数：</p><p>$$<br>\pi(a|s)=\frac{1}{\sqrt{6.28}\sigma}\cdot exp(-\frac{(a-\mu)^2}{2\sigma^2})<br>$$</p><p>对于 d 维情况一样，动作是 d 维向量。</p><p>设向量 $\mu$ 代表均值，和向量 $\sigma$ 代表标准差，都是状态 s 的函数。</p><p>使用特殊正态分布作为策略函数：</p><p>$$<br>\pi(a|s)=\prod_{i=1}^d\frac{1}{\sqrt{6.28}\sigma_i}\cdot exp(-\frac{(a_i-\mu_i)^2}{2\sigma^2_i})<br>$$</p><p>但这里我们不知道 $\mu$ 和 $\sigma$，也就不知道策略函数。</p><p>因此可以用神经网络来近似 $\mu(s;\theta^{\mu})$ 和 $\rho(s;\theta{\rho})$，其中 $\rho_i=ln\sigma_i^2$</p><p>将策略函数进行取对数，将连乘变成连加，得到辅助神经网络 $f(s,a;\theta)=\sum^d_{i=1}[-\frac{\rho_i}{2}-\frac{(a_i-\mu_i)^2}{2\cdot exp(\rho_i)}]$，计算 f 关于其中卷积层和全连接层的参数的梯度，进而实现反向传播更新参数。</p><p><img src="https://img.mahaofei.com/img/202311041725827.png" alt="image.png"></p><blockquote><p>参考：</p><ol><li>王树森.  <a href="https://www.youtube.com/channel/UC9qKcEgXHPFP2-ywYoA-E0Q/playlists?view=50&amp;sort=dd&amp;shelf_id=2">强化学习课程(Youtube)</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">强化学习基础部分笔记</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Python" scheme="https://www.mahaofei.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>【论文复现】MimicPlay从人类演示中学习机器人技能</title>
    <link href="https://www.mahaofei.com/post/21b38b7b.html"/>
    <id>https://www.mahaofei.com/post/21b38b7b.html</id>
    <published>2023-10-21T07:00:02.000Z</published>
    <updated>2023-10-21T07:00:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1>论文笔记</h1><h2 id="1-目标问题-23">1 目标问题</h2><p>从人类演示中学习，是教授机器人操作技能的一种很有前途的方法。</p><p>目前大多数模仿学习算法仍然局限于学习短期的操作，例如开门或抓取特定物品。</p><p>而关于长期任务的研究，目前有两个方向：分层模仿学习和从演示数据中学习。分层学习旨在通过端到端实现高级规划到低级运动控制的学习。从演示数据中学习是指人类通过遥控机器人于环境互动来收集数据。</p><p>本文提出了一个分层学习框架，从大量人类演示数据中学习潜在的计划，来指导机器人在少量演示中实现视觉运动控制。</p><h2 id="2-主要方法">2 主要方法</h2><p><img src="https://img.mahaofei.com/img/202310231710757.png" alt="image.png"></p><h3 id="2-1-收集人类数据">2.1 收集人类数据</h3><p>人类在用手与环境互动的过程中，创造了一个手的轨迹。本文使用两台经过校准的相机来跟踪人类演示数据中的3D手轨迹，手部位置检测使用现有的库。</p><h3 id="2-2-从人类数据中学习3D潜在规划">2.2 从人类数据中学习3D潜在规划</h3><p>问题：给定一个由目标图像表示的长期任务，策略产生以目标为条件的行动。</p><p>将该问题转化为分层学习策略，其中高级规划器从目标图像中提取关键特征，并转化成低维的规划，利用这些规划引导运动控制器动作。为了训练高级规划器，本文使用廉价的数据源（人类演示数据）</p><p><strong>（1）多模式潜在计划学习</strong></p><p>利用收集的人类演示数据和对应的3D手部轨迹，将学习规程转化为目标条件的3D轨迹生成任务。即将人类演示图像，目标图像处理为低维特征，利用MLP编码为潜在计划向量，利用潜在计划向量和手的位置，利用MLP解码为3D手部轨迹的预测。</p><p>为了解决不同人演示同一任务的差异，使用基于MLP的高斯混合模型来对潜在计划的轨迹分布进行建模。</p><p><strong>（2）处理人类演示数据和机器人之间的视觉差异</strong></p><p>本文考虑人与机器人在同一环境中，需要解决机器人与人类外观不同导致的视觉差异。通过计算人类和机器人的视觉编码器的特征嵌入的分布，最小化两者距离（此步骤机器人与人类视频不需要是对应的）</p><h3 id="2-3-多任务模仿学习">2.3 多任务模仿学习</h3><p><strong>（1）用于潜在计划生成的视频提示</strong></p><p>使用单镜头视频作为目标指定提示，发送给训练好的潜在规划器，生成机器人可执行的潜在计划。</p><p>规划器将视频分成多个帧，每个时间步长，规划器从序列中取一个图像帧作为目标图像，生成潜在规划引导机器人动作。</p><p><strong>（2）基于Transformer的计划引导与模仿</strong></p><p>在执行复杂任务时，仅使用高层规划是不够的，还需要考虑底层的细节。因此考虑将机器人腕部相机和本体感觉都转换成低维特征向量，与潜在计划进行结合，利用Trasformer架构（因为其擅长管理长期运动生成）进行处理。</p><p><strong>（3）多任务</strong></p><p>在同一环境中的所有任务中共享相同的规划器和策略模型。</p><h1>算法复现</h1><h2 id="1-环境搭建">1 环境搭建</h2><h3 id="1-1-代码准备">1.1 代码准备</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/j96w/MimicPlay.git</span><br></pre></td></tr></table></figure><h3 id="1-2-conda环境配置">1.2 conda环境配置</h3><p><strong>（1）进入所下载代码环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay</span><br></pre></td></tr></table></figure><p><strong>（2）创建 conda 环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mimicplay python=3.8 -y</span><br><span class="line">conda activate mimicplay</span><br></pre></td></tr></table></figure><p><strong>（3）安装 MuJoCo</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mujoco==2.3.0</span><br></pre></td></tr></table></figure><blockquote><p><strong>如果出现问题</strong>：<code>imgaug 0.4.0 requires XXXXXX, which is not installed.</code><br><strong>解决方法如下</strong>：</p><ol><li>安装报错提示的imgaug所需的依赖项：<code>pip install imageio matplotlib Pillow scikit-image six opencv-python</code></li><li>重新安装mujoco：<code>pip install mujoco</code></li></ol></blockquote><p><strong>（4）安装robosuite</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ARISE-Initiative/robosuite.git</span><br><span class="line">cd robosuite</span><br><span class="line">git checkout v1.4.1_libero</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -r requirements-extra.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><blockquote><p>第一次安装失败，然后<code>git checkout v1.4.1_libero</code>之后才安装成功</p></blockquote><p><strong>（5）安装BDDL</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/StanfordVL/bddl.git</span><br><span class="line">cd bddl</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><strong>（6）安装LIBERO</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git</span><br><span class="line">cd LIBERO</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><strong>（7）安装robomimic</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/ARISE-Initiative/robomimic</span><br><span class="line">cd robomimic</span><br><span class="line">git checkout mimicplay-libero</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><blockquote><p>第一次安装失败，然后<code>git checkout mimicplay-libero</code>之后才安装成功</p></blockquote><p><strong>（8）安装MimicPlay</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/j96w/MimicPlay.git</span><br><span class="line">cd MimicPlay</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h2 id="2-数据准备（从虚拟机器人动作数据中学习机器人动作）">2 数据准备（从虚拟机器人动作数据中学习机器人动作）</h2><h3 id="2-1-官方数据集">2.1 官方数据集</h3><p>训练集和测试视频在<a href="https://drive.google.com/drive/folders/1FUKd3vr-KBiYRnKIymNmGClmVx9U45XG">此处</a>下载。训练集是一系列没有指定特定任务（没有标签）的人类演示视频。</p><p>作者推荐下载原始数据<code>demo.hdf5</code>，然后在本地电脑上将其处理为具有图像观察的训练数据集<code>demo_image.hdf5</code>，因为这样可以很好的检查环境库是否安装正确，具体步骤如下：</p><p><strong>（1）将下载的数据集移动到<code>mimicplay/datasets</code></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">mv mimicplay_release_dataset your_installation_path/mimicplay/datasets</span><br></pre></td></tr></table></figure><p><strong>例如：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">mkdir -p datasets/playdata</span><br><span class="line">mv ~/Downloads/demo.hdf5 ./datasets/playdata/</span><br></pre></td></tr></table></figure><p><strong>（2）将原始数据转换为图像数据集</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">python scripts/preprocess_hdf5.py -i ./datasets/playdata/demo.hdf5 -o ./datasets/playdata/demo_modified.hdf5</span><br><span class="line">python scripts/dataset_states_to_obs.py --dataset &#x27;datasets/playdata/demo_modified.hdf5&#x27; --done_mode 0 --camera_names agentview robot0_eye_in_hand --camera_height 84 --camera_width 84 --output_name image_demo_local.hdf5 --exclude-next-obs</span><br></pre></td></tr></table></figure><blockquote><p><strong>如果出现问题</strong>：<code>ileNotFoundError: [Errno 2] No such file or directory: 'patchelf'</code><br><strong>解决方法如下</strong>：<code>sudo apt-get install patchelf</code></p></blockquote><p><strong>（3）提取末端轨迹用于上层规划器的训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/dataset_extract_traj_plans.py --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27;</span><br></pre></td></tr></table></figure><p><strong>（4）检查数据：重新播放数据集中的图像，保存成视频</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/playback_robomimic_dataset.py --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27; --use-obs --render_image_names agentview_image --video_path image_demo_local_replay.mp4</span><br></pre></td></tr></table></figure><h3 id="2-2-自制数据集">2.2 自制数据集</h3><p><strong>（1）使用BDDL文件收集数据</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/collect_playdata.py --bddl-file &#x27;scripts/bddl_files/KITCHEN_SCENE9_playdata.bddl&#x27; --device &#x27;keyboard&#x27;</span><br></pre></td></tr></table></figure><p>收集的原始数据可以在<code>robosuite/robosuite/models/assets/demonstrations/</code>路径下找到。</p><p><strong>（2）将原始数据转换成robomimic格式</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/convert_playdata_to_robomimic_dataset.py --dataset &#x27;path_to_your_data&#x27;</span><br></pre></td></tr></table></figure><p><strong>（3）现在有了robomimic格式的数据，按照#1.3 公共数据集中的步骤生成特定任务的视频提示</strong></p><h2 id="3-数据准备（从人类演示中学习机器人动作）">3 数据准备（从人类演示中学习机器人动作）</h2><h3 id="3-1-配置人手检测模型">3.1 配置人手检测模型</h3><p><strong>（1）配置开源的 hand_object_detector</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda create --name handobj python=3.6</span><br><span class="line">conda activate handobj</span><br><span class="line">conda install pytorch=1.0.1 torchvision cudatoolkit=10.0 -c pytorch</span><br><span class="line">cd mimicplay/scripts/human_playdata_process</span><br><span class="line">git clone https://github.com/ddshan/hand_object_detector &amp;&amp; cd hand_object_detector</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">cd lib</span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></table></figure><p><strong>（2）下载fast_rcnn模型，并放置在指定位置</strong></p><p>从Google Drive中下载<a href="https://drive.google.com/file/d/1H2tWsZkS7tDF8q1-jdjx6V9XrK25EDbE/view">faster_rcnn_1_8_132028.pth (361M)</a>，移动到下面的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd hand_object_detector</span><br><span class="line">mkdir -p models/res101_handobj_100K/pascal_voc</span><br><span class="line">mv faster_rcnn_1_8_132028.pth models/res101_handobj_100K/pascal_voc/.</span><br></pre></td></tr></table></figure><p><strong>（3）将mimicplay的python脚本放入人手检测器的目录下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd mimicplay/scripts/human_playdata_process/</span><br><span class="line">cp demo_mp4.py hand_object_detector/</span><br></pre></td></tr></table></figure><h3 id="3-2-从人类演示生成数据集">3.2 从人类演示生成数据集</h3><p><strong>（1）复制两个示例视频</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp vis_1.mp4 hand_object_detector/</span><br><span class="line">cp vis_2.mp4 hand_object_detector/</span><br></pre></td></tr></table></figure><p><strong>（2）生成hdf5数据文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd hand_object_detector/</span><br><span class="line">python demo_mp4.py</span><br></pre></td></tr></table></figure><p><strong>（3）可视化数据集</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd scripts/human_playdata_process/</span><br><span class="line">python vis_processed_human_play_data.py</span><br></pre></td></tr></table></figure><h2 id="4-训练">4 训练</h2><h3 id="4-1-训练高级规划器">4.1 训练高级规划器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">python scripts/train.py --config configs/highlevel.json --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27;</span><br></pre></td></tr></table></figure><p>训练结束后，选择评估分数最高的checkpoint，将其路径作为</p><h3 id="4-2-训练低级机器人控制器">4.2 训练低级机器人控制器</h3><blockquote><p>参考：</p><ol><li>Mimicplay: Long-horizon imitation learning by watching human play. [Project](<a href="https://mimic-play.github.io/">MimicPlay | Long-Horizon Imitation Learning by Watching Human Play (mimic-play.github.io)</a>) <a href="https://github.com/j96w/MimicPlay">Code</a>, <a href="https://arxiv.org/abs/2302.12422">arXiv</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">斯坦福大学李飞飞团队的通过观看人类动作进行长期模仿学习方法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>PCL库的安装与基本使用</title>
    <link href="https://www.mahaofei.com/post/8ea92350.html"/>
    <id>https://www.mahaofei.com/post/8ea92350.html</id>
    <published>2023-10-20T10:53:14.000Z</published>
    <updated>2023-10-20T10:53:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Ubuntu 20.04 安装 PCL 库</h1><p><strong>推荐安装方法</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libpcl*</span><br></pre></td></tr></table></figure><p>如果不用最新的PCL功能的话，正常点云处理显示等使用还是没问题的。</p><p><strong>源码安装（建议使用cmake-gui）</strong>：</p><p>下载<a href="https://github.com/PointCloudLibrary/pcl/releases/tag/pcl-1.10.0">PCL-1.10</a>，一定要下载1.10，因为ROS安装的过程中会安装pcl-1.10的库，正常使用没问题，但是缺少一些新功能例如<code>pcl/surface/on_nurbs</code>，我想要使用这个库，因此能够从源码编译，如果使用其它版本安装，会出现冲突例如<code>error: redefinition of</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd pcl-pcl-1.10</span><br><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake-gui</span><br></pre></td></tr></table></figure><p>选择source code为下载解压的文件夹pcl-pcl-1.10，第二个选择刚创建的build文件夹</p><p>点击<code>Configure</code>，点击<code>Finish</code>，出现一些初始化配置。</p><p>注意要选<code>BUILD_surface_on_nurbs</code>，不然会报错<code>pcl/surface/on_nurbs/fitting_surface_tdm.h: 没有那个文件或目录</code>，当然最好把能选的都选了，我这里除了默认的，还额外勾选了</p><ul><li>BUILD_CUDA</li><li>BUILD_GPU</li><li>BUILD_examples</li><li>BUILD_simulation</li><li>BUILD_surface_on_nurbs</li><li>BUILD_kinfu_tools</li></ul><p><img src="https://img.mahaofei.com/img/202312221852931.png" alt="image.png"></p><p>直到没有红色区域之后，点击<code>configure</code>，配置完成后点击<code>generate</code>按钮，会在build文件夹下生成工程文件，然后关闭cmake-gui界面就可以了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">make</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者使用make -j4，make -j8，后面的数字为同时使用的线程数，量力而行，线程过多可能会系统直接卡死</span></span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p><strong>源码卸载</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate pcl-1.13 #查看pcl-1.13的位置 </span><br><span class="line">sudo rm -r /usr/local/include/pcl-1.13 /usr/local/share/pcl-1.13</span><br><span class="line">sudo updatedb</span><br><span class="line">locate pcl-1.13 #检查是否全部删除</span><br></pre></td></tr></table></figure><h1>PCL基本使用</h1><h2 id="1-基本数据类型">1. 基本数据类型</h2><ul><li>点：<code>pcl::PointXYZ</code>、<code>pcl::PointXYZRGB</code>、<code>pcl::PointXYZI</code></li><li>点云：<code>pcl::PointCloud</code><ul><li>宽高：<code>PointCloud::width</code>与<code>PointCloud::height</code>，都为<code>int</code>类型，如果是规律排列的，则分别代表点云的长宽，如果是无序的则高为1，宽为点数量</li><li>点：<code>PointCloud::points</code>存放点的vector变量</li><li>指针：<code>PointCloud::Ptr</code>：只想<code>PointCloud</code>的智能指针</li></ul></li></ul><h2 id="2-读写点云数据">2. 读写点云数据</h2><p><strong>读取点云</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pcl::PCDReader pcd_reader;</span><br><span class="line">pcd_reader.<span class="built_in">read</span>(<span class="string">&quot;xxx.pcd&quot;</span>, *cloud);</span><br><span class="line"><span class="comment">// 或</span></span><br><span class="line">pcl::io::<span class="built_in">loadPCDFile</span>&lt;pcl::PointXYZ&gt;(<span class="string">&quot;xxx.pcd&quot;</span>, *cloud);</span><br></pre></td></tr></table></figure><p><strong>保存点云</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pcd_writer.<span class="built_in">write</span>&lt;pcl::PointXYZ&gt;(<span class="string">&quot;xxx.pcd&quot;</span>, *cloud, <span class="literal">false</span>);  <span class="comment">//false表示保存为ASCII文件</span></span><br><span class="line"><span class="comment">// 或</span></span><br><span class="line">pcl::io::<span class="built_in">savePCDFileASCII</span>(<span class="string">&quot;xxx.pcd&quot;</span>, *cloud);</span><br></pre></td></tr></table></figure><h2 id="3-点云滤波">3. 点云滤波</h2><p><strong>直通滤波</strong>：直接对x,y,z三个方向上设置要保留的点的距离范围，滤除距离范围外的点</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pcl::passThrough&lt;pcl::PointXYZ&gt; pass;</span><br><span class="line">pass.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">pass.<span class="built_in">setFilterFieldName</span>(<span class="string">&quot;x\y\z&quot;</span>);</span><br><span class="line">pass.<span class="built_in">setFileterLimits</span>(<span class="number">0.0</span>, <span class="number">3.0</span>);</span><br><span class="line">pass.<span class="built_in">filter</span>(*output_cloud);</span><br></pre></td></tr></table></figure><p><strong>体素滤波</strong>：将空间划分为一定体积的网格，用每个网格内所有的点的质心来代替所有的点，将每个网格内的点云压缩为一个质心点。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pcl::VoxelGrid&lt;pcl::PointXYZ&gt; vg;</span><br><span class="line">vg.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">vg.<span class="built_in">setLeafSize</span>(<span class="number">0.01f</span>, <span class="number">0.01f</span>, <span class="number">0.01f</span>);  <span class="comment">// 网格的长宽高</span></span><br><span class="line">vg.<span class="built_in">filter</span>(*output_cloud)</span><br></pre></td></tr></table></figure><p><strong>离群点滤波</strong>：统计每个点与周围最临近的若干个点的平均距离，若平均距离大于设定的阈值则判定为离群点而滤除。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pcl::StatisticalOutlierRemoval&lt;pcl::PointXYZ&gt; sor;</span><br><span class="line">sor.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">sor.<span class="built_in">setMeanK</span>(<span class="number">50</span>);  <span class="comment">//设置参与计算平均距离的点数</span></span><br><span class="line">sor.<span class="built_in">setStddevMulThresh</span>(<span class="number">0.1</span>);  <span class="comment">//设置平均距离的阈值（米）</span></span><br><span class="line">sor.<span class="built_in">filter</span>(*cloud_filtered);</span><br></pre></td></tr></table></figure><p><strong>半径滤波</strong>：对一定半径范围内点数少于设定值的点进行滤波。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pcl::RadiusOutlierRemoval&lt;pcl::PointXYZ&gt; outrem;</span><br><span class="line">outrem.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">outrem.<span class="built_in">setRadiusSearch</span>(<span class="number">0.8</span>)  <span class="comment">//设置半径大小</span></span><br><span class="line">outrem.<span class="built_in">setMinNeighborsInRadius</span>(<span class="number">2</span>);  <span class="comment">//设置点数阈值</span></span><br><span class="line">outrem.<span class="built_in">filter</span>(*output_cloud)</span><br></pre></td></tr></table></figure><h2 id="4-点云聚类与分割">4. 点云聚类与分割</h2><p><strong>RANSAC</strong>：随机抽样一致性算法，解决了传统最小二乘法全数据参与不能排除错误数据干扰的问题，可以拟合出更精确的模型，算法思路如下：</p><ol><li>在原本数据集中随机抽取最少可以拟合出模型的数据量进行拟合。最少数据量一般由模型位置的参数来确定（例如直线就是两个），假设拟合得到的模型为M。</li><li>利用M，对数据集中剩余数据计算各个数据与模型M的误差值p，若p&lt;给定阈值n，则认为该数据为内点；若大于阈值，则认为是外点。计算所有内点和外点，得到内点的集合S。</li><li>判断集合S的点数是否大于给定的点数阈值K，若大于K，则认为该次拟合的模型适合离得。若小于K，则认为不合理，直接丢弃。</li><li>若模型合理，则在用得到的内点集合S与之前随机得到的点再拟合依次模型，得到新的模型M’</li><li>重新随机采样，重复1-4过程，得到多个模型M’。</li><li>若采样次数达到给定阈值，则停止采样，就在得到的M’模型中，选择最优的作为最终结果。或者当某一模型M’的误差在给定精度阈值内，则停止采样，以该模型为最终结果。</li></ol><p><strong>平面分割</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pcl::PointIndices::Ptr inliers_plane;</span><br><span class="line">pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg;</span><br><span class="line">seg.<span class="built_in">setOptimizeCoefficients</span>(<span class="literal">true</span>);</span><br><span class="line">seg.<span class="built_in">setModelType</span>(pcl::SACMODEL_PLANE);  <span class="comment">//设置平面模型</span></span><br><span class="line">seg.<span class="built_in">setMethodType</span>(pcl::SAC_RANSAC);  <span class="comment">//使用RANSAC算法</span></span><br><span class="line">seg.<span class="built_in">setDistanceThreshold</span>(<span class="number">0.01</span>)  <span class="comment">//容差范围0.01m</span></span><br><span class="line">seg.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">seg.<span class="built_in">segment</span>(*inliers_planc, *coefficients);  <span class="comment">//得到模型中点的索引，模型参数</span></span><br></pre></td></tr></table></figure><p><strong>法线估计</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pcl::PointCloud&lt;pcl::Normal&gt;::<span class="function">Ptr <span class="title">cloud_normals</span> <span class="params">(<span class="keyword">new</span> pcl::PointCloud&lt;pcl::Normal&gt;)</span></span>;  <span class="comment">//保存法线信息的点云</span></span><br><span class="line">pcl::NormalEstimation&lt;pcl::PointXYZ, pcl::Normal&gt; ne;</span><br><span class="line">ne.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">pcl::search::KdTree&lt;pcl::PointXYZ&gt;::<span class="function">Ptr <span class="title">tree</span> <span class="params">(<span class="keyword">new</span> pcl::search::KdTree&lt;pcl::PointXYZ&gt; ())</span></span>;</span><br><span class="line">ne.<span class="built_in">setSearchMethod</span>(tree)  <span class="comment">//设置搜索方法</span></span><br><span class="line">ne.<span class="built_in">setRadiusSearch</span>(<span class="number">0.03</span>);  <span class="comment">//搜索半径，利用0.03米范围内的点计算法线</span></span><br><span class="line">ne.<span class="built_in">compute</span>(*cloud_normals)</span><br></pre></td></tr></table></figure><p><strong>圆柱分割</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pcl::PointIndices::Ptr inliers_cylinder;</span><br><span class="line">pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg;</span><br><span class="line">seg.<span class="built_in">setOptimizeCoefficients</span>(<span class="literal">true</span>);</span><br><span class="line">seg.<span class="built_in">setModelType</span>(pcl::SACMODEL_CYLINDER);  <span class="comment">//设置圆柱模型</span></span><br><span class="line">seg.<span class="built_in">setMethodType</span>(pcl::SAC_RANSAC);  <span class="comment">//使用RANSAC算法</span></span><br><span class="line">seg.<span class="built_in">setNormalDistanceWeight</span>(<span class="number">0.1</span>)  <span class="comment">//法线在估计的权重</span></span><br><span class="line">seg.<span class="built_in">setMaxIterations</span>(<span class="number">10000</span>);  <span class="comment">//迭代次数</span></span><br><span class="line">seg.<span class="built_in">setDistanceThreshold</span>(<span class="number">0.05</span>);  <span class="comment">//距离容差</span></span><br><span class="line">seg.<span class="built_in">setRadiusLimits</span>(<span class="number">0</span>, <span class="number">0.1</span>);  <span class="comment">//半径范围</span></span><br><span class="line">seg.<span class="built_in">setInputCloud</span>(cloud);  <span class="comment">//输入点云</span></span><br><span class="line">seg.<span class="built_in">setInputNormals</span>(cloud_normals);  <span class="comment">//输入法线点云</span></span><br><span class="line">seg.<span class="built_in">segment</span>(*inliers_cylinder, *coefficients_cylinder); <span class="comment">//得到圆柱的点，模型参数</span></span><br></pre></td></tr></table></figure><p><strong>索引提取</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pcl::PointIndices::Ptr inliers;</span><br><span class="line">pcl::ExtractIndices&lt;pcl::PointXYZ&gt; extract;</span><br><span class="line">extract.<span class="built_in">setInputCloud</span>(cloud);</span><br><span class="line">extract.<span class="built_in">setIndices</span>(inliers);</span><br><span class="line">extract.<span class="built_in">setNegative</span>(<span class="literal">false</span>);</span><br><span class="line">extract.<span class="built_in">filter</span>(*cloud_p);</span><br></pre></td></tr></table></figure><h2 id="5-可视化">5. 可视化</h2><p><strong>程序中可视化</strong></p><p>第一种方式，会造成程序暂停，需要先在点云窗口中按<code>w</code>，调整到一个比较好的视角，退出时按<code>q</code>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pcl::<span class="function">visualization::CloudViewer <span class="title">viewer</span><span class="params">(<span class="string">&quot;Simple Cloud Viewer&quot;</span>)</span></span>; <span class="comment">//括号内是窗口名称</span></span><br><span class="line">viewer.<span class="built_in">showCloud</span>(cloud);</span><br><span class="line"><span class="keyword">while</span>(!viewer.<span class="built_in">wasStopped</span>())&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二种方式，将显示放在了循环内，不会造成程序中断</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pcl::<span class="function">visualization::CloudViewer <span class="title">viewer</span><span class="params">(<span class="string">&quot;Simple Cloud Viewer&quot;</span>)</span></span>;</span><br><span class="line"><span class="keyword">while</span>()</span><br></pre></td></tr></table></figure><p><strong>命令行可视化</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcl_viewer xxx.pcd</span><br></pre></td></tr></table></figure><blockquote><p>参考链接：</p><ol><li><a href="https://www.bilibili.com/video/BV1N24y1R7v8">哈工大竞技机器人队. 【视觉组竞培营】第四讲 PCL点云库. Bilibili. 2022.09.30</a></li><li><a href="https://blog.mangoeffect.net/tools/vcpkg-install-pcl-visualization-module">芒果的技术博客. vcpkg安装pcl-visualization模块. 2021.10.28</a></li><li><a href="https://pcl.readthedocs.io/projects/tutorials/en/master/#">Point Cloud Library. Tutorials</a></li><li><a href="https://blog.csdn.net/qq_42731705/article/details/129380907">Cc1924. Ubuntu18安装新版本PCL-1.13，并和ROS自带PCL-1.8共存. CSDN. 2023.03.07</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">C++使用最广泛的点云库</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="3D点云" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/3D%E7%82%B9%E4%BA%91/"/>
    
    
    <category term="PCL" scheme="https://www.mahaofei.com/tags/PCL/"/>
    
    <category term="点云" scheme="https://www.mahaofei.com/tags/%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>Nerf(instant-ngp)快速实现三维重建</title>
    <link href="https://www.mahaofei.com/post/ce3c8324.html"/>
    <id>https://www.mahaofei.com/post/ce3c8324.html</id>
    <published>2023-10-10T12:59:32.000Z</published>
    <updated>2023-10-10T12:59:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1>搭建环境</h1><p><strong>（1）创建conda环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n nerf-ngp python=3.8</span><br><span class="line">conda activate nerf-ngp</span><br><span class="line">pip install commentjson imageio numpy opencv-python-headless pybind11 pyquaternion scipy tqdm</span><br></pre></td></tr></table></figure><p><strong>（2）下载instant-ngp应用</strong></p><blockquote><p>项目地址：<a href="https://github.com/NVlabs/instant-ngp">https://github.com/NVlabs/instant-ngp</a></p></blockquote><p>快速使用可以下载官方提供的<code>instant-ngp.exe</code>应用，根据自己的显卡版本下载即可：</p><ul><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-RTX-3000-and-4000.zip"><strong>RTX 3000 &amp; 4000 series, RTX A4000–A6000</strong>, and other Ampere &amp; Ada cards</a></li><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-RTX-2000.zip"><strong>RTX 2000 series, Titan RTX, Quadro RTX 4000–8000</strong>, and other Turing cards</a></li><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-GTX-1000.zip"><strong>GTX 1000 series, Titan Xp, Quadro P1000–P6000</strong>, and other Pascal cards</a></li></ul><p>（如果链接失效请参考源项目中Installation部分，如果在ubuntu下使用，需要下载源码构建。）</p><p>根据自己的情况，下载完成后解压即可：</p><p><img src="https://img.mahaofei.com/img/202310102108837.png" alt="image.png"></p><p><strong>（3）测试</strong></p><p>打开<code>instant-ngp.exe</code>，将<code>data\nerf\</code>下的<code>fox</code>文件直接拖到窗口中即可</p><p><img src="https://img.mahaofei.com/img/202310102110957.png" alt="image.png"></p><h1>Colmap计算相机位姿</h1><p><strong>（1）录制视频</strong></p><p>对于要三维重建的物体或场景，使用手机录制一段视频。</p><p>尽量均匀扫描，手机不要移动太快或抖动。</p><p><strong>（2）使用Colmap计算相机位姿</strong></p><p>在项目文件夹内新建一个文件夹，将录制的视频放进去。</p><p><img src="https://img.mahaofei.com/img/202310102113654.png" alt="image.png"></p><p><code>cd</code>到视频所在的目录下。在命令行内执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate nerf-ngp</span><br><span class="line">python ..\..\scripts\colmap2nerf.py --video_in desk.mp4 --run_colmap --overwrite</span><br></pre></td></tr></table></figure><p>需要等待较长的一段时间</p><p>完成后会出现分割好的image文件夹</p><p>再继续执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ..\..\scripts\colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 16 --overwrite</span><br></pre></td></tr></table></figure><p>在等待比较长的一段时间，完成。</p><h1>instant-ngp三维重建</h1><p>打开<code>instant-ngp.exe</code>，将desk文件夹整体拖进去就ok了</p><p><img src="https://img.mahaofei.com/img/202310102127121.png" alt="image.png"></p><p>视觉效果还是相当可以的，不过导出mesh模型效果比较差</p>]]></content>
    
    
    <summary type="html">Instant-NGP全称Instant Neural Graphics Primitives，它通过多分辨率哈希编码，解决了NeRF对全连接神经网络进行参数化时的效率问题，大大提升了网络的训练速度，使三维重建速度可以从几小时缩短到几秒钟。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="3D点云" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/3D%E7%82%B9%E4%BA%91/"/>
    
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="三维重建" scheme="https://www.mahaofei.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    <category term="Nerf" scheme="https://www.mahaofei.com/tags/Nerf/"/>
    
  </entry>
  
  <entry>
    <title>Google_Mediapipe关节检测框架</title>
    <link href="https://www.mahaofei.com/post/5090460e.html"/>
    <id>https://www.mahaofei.com/post/5090460e.html</id>
    <published>2023-09-18T06:40:00.000Z</published>
    <updated>2023-09-18T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 准备工作</h1><h2 id="1-1-安装Baze">1.1 安装Baze</h2><p><strong>（1）下载Bazelisk</strong></p><p>MediaPipe使用bazel进行构建的，安装bazellisk主要是为了更新bazel</p><p>进入bazel的项目<a href="https://github.com/bazelbuild/bazelisk/releases">https://github.com/bazelbuild/bazelisk/releases</a>，下载二进制文件<a href="https://github.com/bazelbuild/bazelisk/releases/download/v1.18.0/bazelisk-linux-amd64">bazelisk-linux-amd64</a>。</p><p>然后将文件移动到<code>/usr/local/bin/bazel/</code>，并修改其可执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mv bazelisk-linux-amd64 /usr/local/bin/bazel</span><br><span class="line">sudo chmod u+x /usr/local/bin/bazel</span><br></pre></td></tr></table></figure><p>检查bazel是否安装成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel version</span><br></pre></td></tr></table></figure><p>可以查看到bazel的版本就算成功。</p><p><strong>（2）安装Bazel</strong></p><p>从 <a href="https://github.com/bazelbuild/bazel/releases">GitHub 上的 Bazel 版本页面</a>下载名为 bazel-version-installer-linux-x86_64.sh的shell脚本文件。</p><p>例如我本次安装的就是<code>bazel-6.3.2-installer-linux-x86_64.sh</code></p><p>执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">给bazel-6.3.2-installer-linux-x86_64.sh脚本可执行权限</span></span><br><span class="line">chmod +x bazel-6.3.2-installer-linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载bazel</span></span><br><span class="line">./bazel-6.3.2-installer-linux-x86_64.sh --user</span><br></pre></td></tr></table></figure><p>根据提示，将 <code>source /home/mahaofei/bin/bazel/bin/bazel-complete.bash</code> 添加到 <code>~/.bashrc</code> 中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;source /home/mahaofei/.bazel/bin/bazel-complete.bash&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><p>重启终端，使用<code>bazel --version</code>命令检查是否安装成功。</p><h2 id="1-2-下载Mediapipe">1.2 下载Mediapipe</h2><p><strong>（1）克隆项目</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:google/mediapipe.git</span><br></pre></td></tr></table></figure><p><strong>（2）安装opencv和ffmpeg</strong></p><p>进入克隆的项目中，为<code>setup_opencv.sh</code>添加可执行权限，并运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x setup_opencv.sh</span><br><span class="line">./setup_opencv.sh</span><br></pre></td></tr></table></figure><h1>2 使用</h1><p><strong>（1）python环境</strong></p><p>创建虚拟环境，并安装必要的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mediapipe python=3.8</span><br><span class="line">conda activate mediapipe</span><br><span class="line">pip install opencv-python</span><br><span class="line">pip install opencv-contrib-python</span><br><span class="line">pip install mediapipe</span><br></pre></td></tr></table></figure><p><strong>（2）使用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line">mp_drawing = mp.solutions.drawing_utils</span><br><span class="line">mp_drawing_styles = mp.solutions.drawing_styles</span><br><span class="line">mp_holistic = mp.solutions.holistic</span><br><span class="line"></span><br><span class="line">cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> mp_holistic.Holistic(</span><br><span class="line">    min_detection_confidence=<span class="number">0.5</span>,</span><br><span class="line">    min_tracking_confidence=<span class="number">0.5</span>) <span class="keyword">as</span> holistic:</span><br><span class="line">  <span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    success, image = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> success:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;Ignoring empty camera frame.&quot;</span>)</span><br><span class="line">      <span class="comment"># If loading a video, use &#x27;break&#x27; instead of &#x27;continue&#x27;.</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    image.flags.writeable = <span class="literal">False</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    results = holistic.process(image)</span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line">    image.flags.writeable = <span class="literal">True</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</span><br><span class="line">    mp_drawing.draw_landmarks(</span><br><span class="line">        image,</span><br><span class="line">        results.face_landmarks,</span><br><span class="line">        mp_holistic.FACEMESH_CONTOURS,</span><br><span class="line">        landmark_drawing_spec=<span class="literal">None</span>,</span><br><span class="line">        connection_drawing_spec=mp_drawing_styles</span><br><span class="line">        .get_default_face_mesh_contours_style())</span><br><span class="line">    mp_drawing.draw_landmarks(</span><br><span class="line">        image,</span><br><span class="line">        results.pose_landmarks,</span><br><span class="line">        mp_holistic.POSE_CONNECTIONS,</span><br><span class="line">        landmark_drawing_spec=mp_drawing_styles</span><br><span class="line">        .get_default_pose_landmarks_style())</span><br><span class="line"></span><br><span class="line">    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)</span><br><span class="line">    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#右手21个节点坐标</span></span><br><span class="line">    <span class="keyword">if</span> results.right_hand_landmarks:</span><br><span class="line">        <span class="keyword">for</span> index, landmarks  <span class="keyword">in</span> <span class="built_in">enumerate</span>(results.right_hand_landmarks.landmark):</span><br><span class="line">            <span class="built_in">print</span>(index,landmarks )</span><br><span class="line"><span class="comment">#鼻子坐标</span></span><br><span class="line">    <span class="comment">#print(results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE])</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;MediaPipe Holistic&#x27;</span>, cv2.flip(image, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">5</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">cap.release()</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202312050848155.png" alt="image.png"></p>]]></content>
    
    
    <summary type="html">Google所开发的通用框架。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="实例分割" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="关节检测" scheme="https://www.mahaofei.com/tags/%E5%85%B3%E8%8A%82%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【模仿动作】从人类演示中学习机器人动作规划方法</title>
    <link href="https://www.mahaofei.com/post/d76756ed.html"/>
    <id>https://www.mahaofei.com/post/d76756ed.html</id>
    <published>2023-09-07T06:41:50.000Z</published>
    <updated>2023-09-07T06:41:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 MimicPlay: Long-Horizon Imitation Learning by Watching Human Play</h1><blockquote><p><strong>标题</strong>：模拟游戏：通过观看人类游戏进行的长期模拟学习<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://mimic-play.github.io/">https://mimic-play.github.io/</a>(code is coming soon)</p></blockquote><h2 id="1-1-目标问题-5">1.1 目标问题</h2><p>由于人类比遥控机器人能更快的完成长时间任务，因此启发从人类演示中学习机器人规划策略。</p><p>为了减少数据需求，采用人类与环境的交互视频作为数据。开发一个分层学习框架，从人类数据中学习潜在的规划控制方法。</p><h2 id="1-2-方法-4">1.2 方法</h2><p><img src="https://img.mahaofei.com/img/202309071622980.png" alt="image.png"></p><p><strong>（1）从人类数据中学习潜在规划</strong></p><p>给定输入：视觉观察$o_t$，未来的目标图像$g_t$，当前手部位置$l_t$<br>训练过程中，$g_t$被视为执行动作后的未来帧<br>规划期的目标是根据视频提示V生成目标图像的动作规划。</p><ol><li>人类演示数据收集</li><li>跟踪人手三维轨迹：使用双目相机获取人手的3D轨迹，利用现成的<a href="https://github.com/ddshan/hand_object_detector">手部检测器</a>确定2维图像中的手部位置，然后利用双目视图重建手的3D轨迹。</li><li>学习潜在规划：使用两个卷积网络分别将当前图像和目标图像处理为低维特征，再与手部位置连接在一起，使用MLP处理为潜在规划特征。生成3D手部运动轨迹。为了处理同一个任务的不同方式的实现，使用高斯混合模型对潜在规划的轨迹分布进行建模。</li></ol><p><strong>（2）计划引导的多任务模仿学习</strong></p><p>机器人的底层策略使用行为克隆算法进行训练，使用通过遥操作收集的机器人演示数据。</p><ol><li>视频条件下的潜在规划生成：使用遥操作机器人任务视频来提示训练时潜在规划器生成相应的规划。</li><li>基于Transformer的规划引导模仿：将机器人手上相机观察和本体姿态信息处理为低维向量，再与潜在计划连接起来，通过Transformer架构来计算最终的机器人控制命令。</li><li>多任务学习</li></ol><h2 id="1-3-思考-4">1.3 思考</h2><p>李飞飞团队的作品，从视频中学习人手的运动轨迹，code is coming soon，等待后续再细看。</p><h1>2 One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</h1><blockquote><p><strong>标题</strong>：通过领域自适应元学习观察人类的一次性模仿<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<br><a href="https://github.com/tianheyu927/mil">官方版: https://github.com/tianheyu927/mil</a><br><a href="https://github.com/daiyk/daml_pytorch">Pytorch版: https://github.com/daiyk/daml_pytorch</a></p></blockquote><h2 id="2-1-目标问题-2">2.1 目标问题</h2><p>提出一种从人类视频中进行学习的方法，通过使用各种先前任务的人类和机器人演示数据，使机器人执行人类演示的任务。</p><h2 id="2-2-方法-2">2.2 方法</h2><p><strong>（1）问题描述</strong></p><p>将先验知识和少量证据组合起来，形成一个人类演示的形式。从中推断出完成任务的机器人的策略参数。</p><p><strong>（2）领域自适应元学习</strong></p><p>能够处理从人类的视频演示中学习，学习一组参数，以便在人类演示的基础上进行梯度下降后，模型可以有效地执行新任务。</p><p>由于人类和机器人的动作无法直接对应，因此考虑学习只对策略激活起作用。</p><p><strong>（3）学习时间适应目标</strong></p><p>要从人类的视频中进行学习，需要捕获视频中的相关信息，例如人类的意图和任务的相关对象。要确定哪些行为正在被演示，哪些对象是相关的，通常需要同时检查多个帧来确定人类的运动。因此本文的学习适应目标将多个时间步长耦合，从多个时间步骤对策略进行操作。</p><p>此处使用卷积网络来表示自适应目标，使用递归神经网络LSTM进行时间卷积。</p><p><strong>（4）概率解释</strong></p><p>将学习到的自适应目标纳入到概率图模型的框架中，推断特定任务的策略参数。</p><h2 id="2-3-思考">2.3 思考</h2><p>思路看起来很可以，就是数学推理比较复杂，很难看得懂。</p><h1>3 Waypoint-Based Imitation Learning for Robotic Manipulation</h1><blockquote><p><strong>标题</strong>：基于航路点的机器人操纵模拟学习<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></p></blockquote><p>行为克隆BC目前有很多问题，路径点可以通过减少BC的范围来解决这个问题，但是传统路径点需要人工监督标注。</p><p>本文提出了线性运动近似的，模仿学习的自动轨迹点提取模块，将演示分解为一组轨迹点，进行线性插值，近似实现演示动作。</p><p>并且该方法可以与任务BC算法相结合，提高其成功率。</p><h1>4 Building Robot Intelligence by Scaling Human Supervision</h1><blockquote><p><strong>标题</strong>：通过扩展人类监督构建机器人智能<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2021</p></blockquote><h2 id="4-1-研究背景">4.1 研究背景</h2><p><strong>几十年来，我们一直在想象一个机器人可以充当个人助理的世界，能够完成我们每天做的各种任务和家务，比如做饭、打扫卫生、洗衣，甚至组装橱柜。机器人领域的研究人员一直致力于实现这一梦想。然而，不幸的是，今天的自主机器人远未达到操纵能力的水平。尽管研究在使机器人能够完全自主地完成特定任务的方面取得了令人印象深刻的进展，包括拾取物体，或将它们堆叠在一起。但机器人和人类的操作能力之间存在很大差距。人类智能地使用物体，并在日常生活中以丰富的方式与它们互动，比如当我们用刀切菜做饭时，或者用螺丝刀拧紧螺丝组装橱柜时。这种有目的的与物体的互动对机器人来说是十分困难的。</strong></p><p><strong>作为人类，我们在一生中积累了一系列不同的先前经验，这些经验我们可以在日常生活中借鉴。此外，即使我们不知道如何做某事，我们也可以通过观看其他人的视频来快速学习，例如通过观看YouTube上其他人组装橱柜的视频来学习如何组装橱柜。这就提出了一个问题——我们是否可以类似地为机器人提供丰富多样的先前经验，并使他们能够从这些数据集中学习操作技能？</strong></p><p><strong>这激发了数据驱动的机器人，这是一种有用的范式，让机器人从大型数据集中学习操作。但是这种方法通常有两种变体，第一种是机器人自行收集数据，数据一开始是随机的，但会随着时间推移慢慢变好。由于机器人必须自己学习，限制了可以学习的人物的复杂性。第二种则是人类控制机器人并引导它完成任务，然是这通常是不可扩展的，因此可以收集的数据量很小，这再次限制了任务的复杂性。</strong></p><p><strong>相比之下，计算机数据和自然语言处理等领域已经通过大规模高质量数据集开创了前所未有的成就，我们希望在机器人技术方面看到类似的突破。</strong></p><p><strong>为了复制这一成功经验，并解决数据驱动机器人中任务复杂性有限的问题，我们需要解决两个关键挑战。首先，收集大规模的人类数据具有挑战性。在计算机视觉领域，注释可以由人类直接标注，很容易实现并行标注和大规模人员标注。相比之下，在机器人技术中，人类必须与机器人实时互动，引导机器人完成任务。这使得提供直观和可扩展的方法来收集来自多个人的数据变得很困难。其次，从大规模数据集中学习可能并不简单。在其他领域，我们可以训练网络预测注释，这些注释对应的都是真实的标签。然而在机器人技术中，没有一种真正的方法来执行任务，不同的人可能会收集不同的轨迹，不同的策略，我们需要确定如何从这些数据集中学习。</strong></p><h2 id="4-2-研究目标">4.2 研究目标</h2><p>第一部分，讨论了如何通过充满丰富交互的人类监督来收集大规模数据，这些数据体现了机器人的类人操作能力。包括一个为解决机器人操作中对大规模人类数据集需求构建的平台，和现实世界的数据收集。</p><p>第二部分，讨论了如何使用丰富的数据集来学习机器人操作技能。</p><p>第三部分，讨论了该方法可能的进一步拓展和应用。</p><h2 id="4-3-收集人类操作数据">4.3 收集人类操作数据</h2><p><strong>为了使数据能够捕捉人类的操作，首先数据应该在所展示的解决问题的策略的种类上是多样化的。作为人类，我们很清楚什么时候应该尝试不同的方法类实现目标，而机器人应该从所有这些策略中学习，因为在特定的情况下可能需要其中的一种。其次，数据应该包含灵巧的操作，我们希望我们的机器人了解它们如何通过武力方式操作物体来实现预期的结果。最后，数据应该是大规模的，人类非常擅长在无数情况下解决问题，但机器人还不能做到这一点。我们向他们展示的数据越多，他们也就越有可能获得这种能力。</strong></p><p>在这一部分，我们提出了RoboTurk平台，一个数据收集平台，允许人类实时远程操作机器人。操作员在他们的网络浏览器中看到机器人的工作空间的视频流，用他们的智能手机控制机械臂，他们手机的运动与机器人的运动相耦合，可以自然地控制手臂，这使得人们可以轻松的提供任务演示，连接的过程快速而简单。实验表明，这些数据能够在多步骤操作任务上进行策略学习，并且在策略学习的过程中使用大量的演示可以在学习一致性和最终性能方面带来好处。</p><h2 id="4-4-从大规模人类数据集中学习操作">4.4 从大规模人类数据集中学习操作</h2><p><strong>在这一部分，我们讨论了机器人如何才能够大规模人类数据集中学习操作技能。此类数据集可能表现出巨大的多样性，并由次优解决方案组成，因此从中学习具有挑战性。我们提出了一种从大规模演示数据集中学习的新算法，即无规模交互的内隐强化IRIS算法。IRIS将控制问题分解为目标条件的低级控制器和高级目标选择机制，前者模仿短演示序列，后者为低级控制器设置目标，并选择性的组合部分次优解决方案，从而更成功的完成任务。</strong></p><p>尽管最近在模仿学习和强化学习方面缺乏开源的人类数据集和可重复的学习方法，使得评估该领域的状态变得困难。我们对六个离线学习机器人操作算法，在五个仿真和三个不同复杂度的真实环境中进行多阶段操作任务测试。我们得到了一系列经验，包括对不同算法设计选择的敏感性，对演示质量的依赖性，以及由于训练不同的目标而导致的不同停止标准。我们还强调了从人类数据集学习的可能性，例如在当前强化学习方法范围之外的具有挑战性的多阶段任务中学习熟练策略的能力，以及轻松扩展到只有原始感官信号可用的自然、真实世界操作场景的能力。我们已经开源了我们的数据集和所有算法实现，以促进未来的研究和从人类演示数据中学习的公平比较。</p><h2 id="4-5-使用人类数据集构建能力更强的机器人">4.5 使用人类数据集构建能力更强的机器人</h2><p>这一部分探讨了几个不同的应用程序，使我们更接近于我们希望的机器人能够在未来能够处理的任务。主要探讨了多任务领域（如厨房），高精度操作，和需要协作的多臂操作任务。</p><p><strong>模仿学习方法的一个常见的局限是由于训练集中的数据有限，在所展示的行为之外进行泛化是一个开放的挑战。例如在厨房场景中，我们可能希望机器人实现多种可能的配置，具有多个要操作和交互的对象，如食物、出轨、微波炉、水槽等。本章我们介绍了通过模仿进行任务泛化，这是一种新颖的模仿学习框架，使机器人能够从少量的人类演示中有效的学习复杂的现实世界操作任务。合成收集的演示中未包含的新行为。多任务领域通常呈现出一种潜在的结构，不同的任务轨迹在状态空间的公共区域相交。GTI是一个两阶段在线模仿学习算法，该算法利用交叉结构来训练目标导向的策略，这些测类推广到看不见的开始和目标状态组合。在GTI的第一阶段，我们训练了一个随机策略，该策略利用轨迹交叉点来有能力从不同的演示轨迹中组合行为在一起。在GTI的第二阶段，我们从第一阶段的无条件随机策略中收集了一小组推理，并训练一个目标导向的agent来推广到新的启动和目标配置。我们在模拟领域和现实世界中具有挑战性的长期机器人操作领域中验证了GTI。</strong></p><p><strong>模仿学习方法通常也很难完成高精度的操作任务，因为它们需要一系列精确的动作才能取得有意义的进展，比如机器人将pod插入咖啡机制作咖啡。经过培训的策略可能会在这些场景失败，因为行动上的微小偏差可能会导致策略进入未被演示覆盖的区域。基于干预的策略学习是解决这一问题的一种替代方案——它允许操作员监控经过训练的策略，并在遇到故障时接管控制权。</strong> 我们扩展了RoboTurk，使远程操作员能够监控和干预经过培训的政策。我们开发了一个简单的在系统收集的新数据上迭代训练策略的有效算法。我们证明，根据我们基于干预的系统和算法收集的数据训练的代理优于根据非干预演示者收集的同等数量样本训练的代理，并进一步证明，我们的方法在从具有挑战性的机器人线程任务和咖啡制作任务。</p><p><strong>最后，虽然通过远程操作收集的人类演示中的模仿学习（IL）是教授机器人操作技能的强大范式，但它大多局限于单臂操作。然而，许多现实世界中的任务需要多个手臂，例如举起重物或组装桌子。不幸的是，将IL应用于多臂操作任务一直具有挑战性</strong>——要求人类控制多个机械臂可能会带来巨大的认知负担，而且通常最多只能控制两个机械臂。为了应对这些挑战，我们介绍了多臂RoboTurk（MART），这是一个多用户数据收集平台，允许多个远程用户同时远程操作一组机械臂，并收集多臂任务的演示。使用MART，我们从几个地理位置不同的用户那里收集了五个新的双臂和三臂任务的演示。我们表明，从这些数据中学习因此给集中式代理带来了挑战，这些代理直接尝试同时对所有机器人动作进行建模，并对数据进行全面不同的策略架构，对我们的任务具有不同的集中程度。最后，<strong>我们提出并评估了一个基本残差策略框架，该框架允许经过训练的策略更好地适应多臂操作中常见的混合协调设置，并表明用去中心化残差模型增强的集中式策略在我们的基准任务集上优于所有其他模型。</strong></p><h1>5 Understanding and Learning Robotic Manipulation Skills From Humans</h1><blockquote><p><strong>标题</strong>：从人类身上理解和学习机器人操作技能<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="5-1-背景和动机">5.1 背景和动机</h2><p>制造机器人的性能是通过它们的精度、准确性和速度来衡量的。这导致了刚性和笨重的机器人的设计，这些机器人与人类一起工作是不安全的。他们的控制器在不使用力传感的情况下执行预先编程的轨迹，使其对位置误差高度敏感。通过使用夹具和夹具，例如装配线上的夹具，可以减少环境中的不确定性。</p><p>现实世界的环境需要低重量、人类安全、扭矩控制的机器人。<strong>如果机器人要在环境不断变化、感知能力有限的日常环境中真正发挥作用，就必须找到通过预编程轨迹控制机器人的替代方案。一种很有前途的方法是将复杂的任务划分为健壮且可重用的动作或基元。</strong> 在本文中，我们通过使用可推广的顺应原语，为在更高抽象级别上编程机器人奠定了理论和实践基础。</p><p><strong>方法的第一步是从人类演示中收集数据。然后，我们将数据分割成在任务期间执行的动作序列——基元。接下来，我们将数据投影到一个低维和物理意义的空间中，使我们能够理解人类的策略。最后，我们将这些行为编码到能够执行任务的机器人控制器上。</strong> 此外，我们的框架利用视觉和触觉反馈，让人类处于故障恢复和持续学习的循环中。</p><h2 id="5-2-从人类演示中学习">5.2 从人类演示中学习</h2><p><strong>本文的工作属于示范学习LfD的范畴。人类在操作方面非常有能力，因此从人类演示中收集数据使学习机器人新行为的一种流行方式。事实上，我们不仅可以学习单臂行为，还可以学习双臂行为，我们的系统已经证明了这一点，并在其它工作中进行了探索。</strong> 大多数先前的工作侧重于从视觉数据中学习。而我们的工作强调在执行富含接触的任务时里和数据的重要性。</p><p>近年来，互动学习是一个不断发展的研究领域，它使人类保持在循环学习的过程中。为了实现类似的工作方式，我们的框架通过使用触觉接口使人类处于循环中，我们系统手机故障恢复数据可以与从故障中学习的工作相结合，易产生更稳健的自主行为。</p><h2 id="5-3-机器人基本单元">5.3 机器人基本单元</h2><p>在这项研究中，基元是有一个兼容的框架和一组所需的任务参数定义的。顺应性框架是一个原点和空间中的三个方向，我们沿着它们控制运动和顺应性。柔顺框架附着到要操纵的对象上。任务参数包括所需的力、力矩、位置和方向。这种与机器人无关的任务规范提供了一种有物理意义的低维表示。</p><p><strong>基元库。生成一个由n个基元组成的库，对基本的操作技能进行编码，通过组合这些基元，可以以一种方式解决新的复杂任务，即所需基元的数量不会随着任务数量的增加而增加。广义上讲，关于运动基元的文献主要解决了三个主要的研究问题，生成运动基元，参数化基元以及将基元组合在一起以成功完成任务。</strong></p><p><strong>基元生成。基元的生成可以通过手动编码所需策略或者从数据中自动提取策略来实现。先前的研究已经转向人类寻求灵感，并试图提取策略。</strong></p><p><strong>基元参数化。参数化基元处理定义动作的方式。基元通常使用轨迹段进行参数化。用轨迹定义运动基元已经被证明是成功的，但该方法假设环境不确定性较低。</strong></p><p>兼容基元。我们使用框架的概念来参数化我们的原始控制器。先前的研究使用了以对象为中心的任务控制器的相同概念。然而，与本文中的工作相反，仅从视觉数据中提取控制器参数，我们认为，在处理复杂任务是，考虑序列数据是有利的。在存在位置不确定性的情况下，顺从性在任务中也起着重要作用，对于接触丰富的任务，比如抓获或本文中研究的任务。例如，基元的概念，其中柔顺基元就是用于实现对小物体的鲁棒抓取。</p><p><strong>使用基元进行规划。组合运动基元的概率方法利用了决策过程的固有不确定性，这些方法可以是完全自动化的，也可以是使用混合的方法，将自动决策算法与用户指定的图像相结合。</strong> 最近的其他方法使用语义模型来学习基于是觉得操纵任务计划，或者一些方法使用接触而不是视觉来指导决策过程。</p><h2 id="5-4-多层控制体系结构">5.4 多层控制体系结构</h2><p>该体系结构由三层感知-动作反馈回路组成。每个层都以不同的抽象级别运行，并以不同的频率运行。</p><p>在最底层，完全依赖于控制器，并有助于高速率感官反馈和控制的集成，以实现安全和可预测的机器人运动。该级别向机器人电机发送命令，因此，感知动作回路必须以非常高的频率闭环。下一层向全身控制器提供输入，从而以较慢的速率运行。最后，执行计算成本高昂的感知和规划的最高抽象级别以最低的速率运行。</p><p>全身控制级别使用任务优先级。基于优先级的控制使我们在设计原始动作时能够专注于对象及其几何约束。完整的机器人行为可以被视为由具有不同优先级的不同任务组成。例如，高优先级任务可以是避免奇异配置，另一个任务可以处理障碍或摩擦约束。以类似的方式，有一项任务专门负责实现操纵对象之间所需的几何关系。此任务被编码为基元。换句话说，基本动作只涉及对象，因为任务的所有其他方面，包括非几何约束和机器人运动学，都由控制器的其他组件处理。</p><p>基于优先级的全身控制使用零空间投影来确保满足所有不同的约束。此外，操作空间公式——使用Jacobian的动态解耦逆来计算递归零空间投影——确保具有不同优先级的任务动态解耦。先前的工作也在操作原语的上下文中使用了这种分层框架。</p><h1>6 Scaling Deep Robotic Learning to Broad Real-World Data</h1><blockquote><p><strong>标题</strong>：将深度机器人学习扩展到广泛的真实世界数据<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="6-1-背景">6.1 背景</h2><p>机器人的一个长期梦想是一种通用的家用机器人，它可以被放置在家庭环境中，也许是它以前从未见过的，并执行一系列有用的任务，如煮咖啡、清洁和烹饪。这样一个机器人无疑将在经济上和通过他们的帮助提高人类生活质量方面产生巨大影响。当然，这个梦想仍然是这样，在实现这个目标方面存在着无数的挑战，包括更好的机器人硬件、电池技术和传感。然而，核心挑战之一在于泛化，即机器人在新的物体、环境和任务中取得成功的能力。事实上，人类有这种能力，正是这种能力使我们能够完成像煮一杯咖啡这样的任务，即使在有新厨房和新物体的情况下也是如此。因此，相关的问题仍然存在——我们如何训练我们的机器人，使其能够广泛推广。</p><p>解决这个问题的一种方法是利用人类的直觉，以及用于机器人规划和控制的手部设计系统和表示。在这种方式中，人类定义了相关的对象类及其属性和关系（例如，颜色、形状、姿势、上方与下方等），然后可以使用状态估计技术从传感器观测中测量这些量，并且可以使用经典的搜索和运动规划方法来执行任务。然而，至关重要的是，这种方法是基于人类对相关对象、特性的规范，在某些情况下，甚至是每个环境和任务的对象的3D模型，这阻碍了这种方法在存在新对象和环境的情况下容易使用。</p><p>有一项工作研究了机器人如何在制造通用机器人时不依赖人类的直觉和手部设计，而是纯粹通过数据和自己的试错来学习行为。具体来说，深度强化学习研究了学习深度神经网络策略的问题，该策略在给定传感器观测的情况下采取行动，从而使策略通过从交互中学习来最大化一些定义的奖励。原则上，这种方法可以让机器人完全靠自己学习技能，而只有少量的成功指标。然而，在实践中，在机器人上运行深度强化学习带来了许多挑战，例如在重置和奖励方面需要人工监督。然而，最关键的是，深度强化学习通常需要在目标环境中进行数百万次在线环境交互才能进行学习，并且一旦完成，所学习的策略只对所训练的环境和数据有效。因此，标准的深度强化学习在消除了人手设计的大部分需求的同时，仍然没有立即让我们更接近能够在新环境和任务中操作的机器人。</p><p>退一步看，人们可能会从机器学习的其他领域寻找灵感，特别是过去几年里，自然语言处理和计算机视觉的研究领域取得了巨大进展。主要基于一个简单的配方：：（1）大量、多样化的离线数据集，（2）自监督或廉价监督的训练目标，以及（3）表达性的端到端训练的神经网络模型。这种基础模型的范例特别令人兴奋，因为这些模型表现出了令人印象深刻的泛化——例如，来自ImageNet的视觉模型可以适应癌症检测这样的全新任务，而像BERT这样的预训练语言模型的应用范围从医学编码到视觉问答。事实上，这种概括水平正是我们希望在一个通用机器人中看到的，它可以被放入一个新的环境中，并快速地学会解决新的任务。</p><p>所以为什么这个配方还没有在机器人中实现呢？现实世界中的机器人操作带来了许多独特的挑战，这使得直接复制这一配方变得困难——我们既没有足够大和多样化的机器人交互数据集，也不清楚什么类型的学习算法或监督来源可以使我们从这些数据集中大规模学习有用的技能。本文的目标在于解决这些挑战，并在机器人操作的背景下复制大规模数据和学习的配方。具体来说，我的研究集中在回答三个广泛的问题上。首先，我们如何可伸缩地收集在物理世界中交互的机器人的大型和多样化的数据集？其次，我们如何设计能够消耗如此广泛的离线数据的自我监督强化学习算法，这些数据可能来自非专家，缺乏奖励标签，并从中学习达到看不见的目标？第三，我们如何解锁网络上存在的广泛数据来源，如人类视频和自然语言，以便在我们的机器人中进行更有效的学习？</p><h1>7 Learning Perceptual Prediction: Learning From Humans and Reasoning About Objects</h1><blockquote><p><strong>标题</strong>：学习感知预测：向人类学习和对物体的推理<br><strong>作者团队</strong>：University of Pennsylvania<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="7-1-目标问题-2">7.1 目标问题</h2><p>人类在使用各种各样的感知模式进行预测时，主要关注从视觉学习。人类的视觉似乎经过了高度的优化，可以用于预测未来的视觉观测。</p><p>研究使用视觉传感器的预测也提供了许多实际优势。首先，高质量的相机很容易获得，并且尺寸、重量和功率要求都很低，这使得它们可以被包括在大多数机器人平台上，由于相机在非机器人应用中的普及，它们已经被商品化了。其次，视觉观察提供了关于环境的丰富信息，包括姿势、纹理和语义，这些信息是其他传感器无法轻易匹配的。获取大量丰富的世界信息对于使代理人能够与世界互动非常重要。</p><p>当前学习动作条件视觉预测模型的方法依赖于访问大量的具体数据，这是昂贵且耗时的，从而阻止了基于视觉预测的方法在许多应用中使用。对于机器人来说尤其如此，因为收集大量机器人数据既昂贵又耗时，而且可能不安全。现有工作表明，基于视觉预测的方法随着数据量的增加而扩展良好，因此找到新的数据来源对于使这些模型能够广泛使用至关重要。</p><p>在这篇论文中，我提出了三种不同的方法来利用非机器人数据来改进视觉预测和机器人控制。在前两项工作中，我使用人类数据来提高机器人的性能，而在第三项工作中我使用现有的非机器人数据集来实现以对象为中心的预测框架。</p><h2 id="7-2-从人类学习">7.2 从人类学习</h2><p>大型和多样化的真实世界数据集对于广泛的泛化和高性能至关重要。大型数据集可以通过自动化管道或人类远程操作进行收集。自动化数据收集过程可以收集非常大的数据集，但在到达环境中感兴趣的部分以及需要与环境进行大量交互方面存在问题。习得的探索策略可以提高代理达到有趣配置的能力，但这些方法仍然需要大量的探索。第二种方法是收集人类演示的远程操作轨迹。这种方法允许数据集轻松地达到有趣的和任务相关的配置。然而，它受到了影响，因为它依赖于人类来操作机器人，这需要训练有素的操作员，而且很快变得非常昂贵。通过从人类学习中汲取灵感，可以找到一种避免这两种方法困难的替代方法。</p><p>人类不仅有能力从自己与世界的互动中学习技能，也有能力通过观察他人来学习技能。考虑一个婴儿学习使用工具。为了成功地使用一个工具，它需要学习该工具如何与其他对象交互，以及如何移动工具来触发这种交互。这种直观的物理概念可以通过观察成年人如何使用工具来学习。更普遍地说，观察是关于世界以及行动如何导致结果的强大信息来源。然而，在存在身体差异的情况下（例如成人身体和婴儿身体之间），利用观察是具有挑战性的，因为演示者和观察者的行为之间没有直接对应关系。来自神经科学的证据表明，人类可以有效地推断出这种对应关系，并利用它们从观察中学习。</p><p>利用对人类的观察提供了大幅增加可用数据的规模和有用性的机会。与自主收集的数据不同，人类数据可以只关注配置空间中有趣的部分，避免危险或无聊的交互。与通过远程操作收集的数据集不同，人类数据集可以具有更大的规模。公开可用的人类数据集，如Ego4D或SomethingSomething，包含数十万个视频和数千小时的镜头，分布在数百个任务和数十个地点。这些数据集与自主收集的数据集的大小相当]，并比通过远程操作收集的数据集中的大小高出一到两个数量级。更重要的是，从人类的无行动观察中学习，开启了从互联网上公开生成的视频中学习的可能性，比如YouTube上的视频，这些视频提供了更多数量级的数据。目前的方法只能将这些数据的有限子集用于特定任务，但我们的工作为更广泛的利用提供了一步。</p><p>在这篇论文中，我们考虑了这样一个问题：主体能否学会利用自己的互动和其他主体的被动观察来解决任务？我们在两个环境中探讨了这个问题，第一个是学习动作条件视觉预测模型，第二个是端到端强化学习策略。</p><p>在第3章中，我们提出了一种使用人类的无动作数据和主体自己的探索来执行强化学习的方法。我们提出了克服野外人类数据和模拟机器人数据之间的域转换的方法，将动作添加到无动作的人类数据中，以及估计人类数据的奖励的方法。通过利用在现实世界中收集的人类视频，我们能够加快模拟机器人代理的学习速度。</p><h2 id="7-3-对象推理">7.3 对象推理</h2><p>虽然从人类观察中学习可以获得大量数据，但它并不能回答应该学习什么的问题。我们重点学习端到端模型，这些模型直接从传感器输入映射到预测的未来帧或内部结构很少的期望动作。通过为任务选择正确的归纳偏差集，并利用在非机器人数据上预训练的现有模型，我们应该能够用更少的数据训练我们的模型，并实现更高的性能。正确设计学习问题的结构也可以使模型更容易地用于下游任务。我们关注的是假设世界是由物体组成的简单归纳偏见。</p><p>大多数用于操纵的动态交互可以通过将场景分解为对象来建模。虽然有些材料，如液体或颗粒介质，不容易被表示为对象，但大多数操作任务都涉及操作离散对象。分拣箱子、重组房间，甚至喝杯咖啡，这些任务主要由与离散对象的交互控制。</p><p>以对象为中心的预测模型在预测和困难任务方面表现出了成功的性能。通过将场景分解为离散对象，这些预测模型可以在更长的时间范围内保持每个对象的内聚性。此外，通过将世界状态内部表示为对象集合，以对象为中心的预测模型可以轻松地与规划者对接，并提供一个非常可解释的界面，有助于调试和验证。</p><h1>8 Affordances from Human Videos as a Versatile Representation for Robotics</h1><blockquote><p><strong>标题</strong>：人类视频作为机器人的通用表示<br><strong>作者团队</strong>：CMU, Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://robo-affordances.github.io/">https://robo-affordances.github.io/</a></p></blockquote><h2 id="8-1-目标问题-2">8.1 目标问题</h2><p>从人类视频中学习可操作的动作表示，该模型在未来帧的监督下预测接触点和轨迹路径点。</p><p>论文主要关注三个问题：</p><ol><li>如何表示可操作性？</li><li>如何以数据驱动和可扩展的方式学习这种表示？</li><li>如何实现跨机器人的视觉启发的方法部署？</li></ol><p>对应这三个问题，本文提出了以下三种观点：</p><ol><li>接触点和接触之后的轨迹是比较好的机器人操作的表示方法；</li><li>利用了自我中心的数据集，聚焦于所有有人类的帧，来预测接触点和接触之后的轨迹，通过使用现成的工具来估计自我运动、人体姿势和手-物体交互；</li><li>实现了一种称为Vision-Robotics Bridge(VRB)的方法，实现这些功能与不同类型机器人的无缝集成。</li></ol><h2 id="8-2-方法-2">8.2 方法</h2><p><strong>（1）可操作性表示</strong></p><p>提取人类启示的最自然的方式是观察人们如何与世界互动。常规的思路是从视频中准确模拟人类的运动，但这导致了一个以人为中心的模型，很不容易推广，因为人类的形态和机器人完全不同。</p><p>因此本文采用机器人需求驱动的第一性原理，机器人的本体的信息通常是已知的，因此使用运动规划达到3D空间中的点是很容易实现的，关键难点在于与环境的互动位置在哪里，以及接触之后如何移动。</p><p>受此启发，采用接触点 $c$ 和接触之后的轨迹 $\tau$ 作为视觉启发的简单操作表示，可以很容易的传递给机器人。其中 $\tau=f(I_t,h_t)$，$I_t$ 是时间步长 $t$ 的图像，$h_t$ 是像素空间中人手位置。</p><p><strong>（2）从自我中心的视频中学习操作</strong></p><p>接下来的问题是如何处理视觉输入的人体或手，从人类视频中提取接触点 $c$ 和轨迹 $\tau$。</p><ol><li>从人类视频中提取操作</li></ol><p>对于给定的视频 $V$，例如人开门，使用现有的手部对象检测模型，对每一帧图像 $I_t$ 生成手的2D边界框和离散接触变量 $o_t$，使用这些信息，我们可以过滤每个图像中 $o_t$ 表示接触的帧，从而找到发生<strong>接触的第一个时间步长</strong> $t_{contanct}$。</p><p><strong>手的像素空间位置构成了接触之后的轨迹</strong> $\tau$，为了提取接触点，我们使用手边界框以及颜色分割来找到手与其他物体边界框相交的点，利用高斯混合模型拟合这些接触点。</p><p>同时要考虑，一个人打开门的时候，人手不仅会移动，<strong>相机也会移动</strong>，需要补偿相机的运动，使用但应矩阵来解决这一问题，通过匹配连续帧之间的特征来获得单应性矩阵，产生变换后的轨迹。</p><p>需要完成<strong>视觉的转移</strong>，即训练视频中包含人手，但机器人任务中的视角不会有，因此考虑将所有的可操作性映射回第一帧，即人类还没有进入场景时。如果人总在视频中，要么将人裁剪出去，要么丢弃。</p><ol start="2"><li>训练模型</li></ol><p>以输入图像为条件，训练模型预测接触点和接触后的轨迹。然而由于学习的任务是多模态的，比如人从桌子上拿起杯子可能是要喝水或者倒到其他地方，因此考虑建立空间概率分布，预测多个heatmap处理这一问题。</p><p>输入图像使用ResNet进行编码，给出潜在空间表示，然后使用卷积层将这个潜在表示投影到K个概率分割中，得到GMM均值的标签的估计。</p><p><img src="https://img.mahaofei.com/img/202311011426427.png" alt="image.png"></p><p>为了估计接触后的轨迹，本文使用基于Transformer的预测。给定场景中，人类可能与许多对象进行交互，这些对象可能不存在在训练数据中，因此我们通过对接触点周围进行采样来解决，实现更好的泛化。</p><p><strong>（3）机器人学习</strong></p><p>本文用于引导现有的机器人学习方法，考虑了四种不同的机器人模式。</p><p><img src="https://img.mahaofei.com/img/202311011430303.png" alt="image.png"></p><ol><li>离线数据采集中的模仿学习</li></ol><p>给定一个图像输入，模型产生接触点和轨迹，我们将这一组数据存储在数据集中。收集到足够的数据后，我们使用模仿学习来控制策略，实现特定的任务。</p><ol start="2"><li>自由奖励的探索</li></ol><p>目标是发现尽可能多的不同技能，然而现实中从头开始探索效率太低了，因为机器人可能会花费大量时间尝试探索，但仍然无法学习有意义的技能来解决人类想要的任务。</p><p>我们考虑先收集数据，然后对所有轨迹进行排序。对于后续的数据采集从高度探索性的轨迹开始进行引导，进一步探索。</p><ol start="3"><li>目标条件的学习</li></ol><p>利用已知的知识，例如打开的门的图像，监督其进行探索学习。</p><ol start="4"><li>可操作性作为动作空间</li></ol><p>将机器人在连续空间中的操作，以空间的方式进行参数化，为每个位置分配一个基元。通过学习获得大量预测，利用GMM拟合到这些轨迹点上，获得离散的接触点和轨迹，机器人只需要在这个空间上进行搜索。</p>]]></content>
    
    
    <summary type="html">从人类抓取物体的视频中学习机器人的动作生成与规划方法，相关的研究的调研。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【模仿抓取】从人类演示中学习机械臂抓取</title>
    <link href="https://www.mahaofei.com/post/f9be0f4e.html"/>
    <id>https://www.mahaofei.com/post/f9be0f4e.html</id>
    <published>2023-08-13T02:50:01.000Z</published>
    <updated>2023-08-13T02:50:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 DemoGrasp: Few-Shot Learning for Robotic Grasping with Human Demonstration</h1><blockquote><p><strong>标题</strong>：DemoGrasp: 机器人抓握的少镜头学习与人体演示<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：</p></blockquote><h2 id="1-1-目标问题-4">1.1 目标问题</h2><h3 id="1-1-1-现存问题">1.1.1 现存问题</h3><p>现有的位姿估计方法要么需要计算目标物体的6D位姿，要么需要学习一组抓取点。前者的方法不能很好的扩展到多个对象实例或类，后者需要大型注释数据集，并且由于其对新几何图形的泛化能力交叉而受到阻碍。</p><h3 id="1-1-2-解决思路">1.1.2 解决思路</h3><p>通过简单简短的人类演示教机器人如何抓取物体，不需要许多带注释的图像，也不局限于特定的几何形状。</p><h3 id="1-1-3-大致方法">1.1.3 大致方法</h3><p>首先构建一个人机交互的RGB-D图像序列。利用该序列来构建表示交互的手和对象网格。完成重建对象形状的缺失部分，并估计重建模型与场景中可见对象之间的相对变换。最后将物体和人手之间的相对姿态的先验知识以及对场景中当前物体姿态的估计转化为机器人必要的抓取指令。</p><h3 id="1-1-4-引言总结">1.1.4 引言总结</h3><p><strong>为什么要做这个研究：</strong><br>当前的机器人抓取缺乏泛化能力，因为它们要么专注于估计物体姿态，要么学习抓取点，这需要物体的详细先验信息或大量注释。就像人手一样，机器人的抓取器和手臂的运动范围也有自然的限制，自由度也有限，这限制了它们可能的抓取姿势。虽然机器人抓取器和人手的运动模型可能有很大差异，但应该可以从人类操作中提取信息，并从中推断出目标机器人的足够抓取命令。通过有限的人类演示，机器人可以模仿人类行为，从而无缝抓取物体。</p><p><strong>本文主要做了什么：</strong><br>我们专注于这种模仿，机器人反映了人类的互动，如图1。该任务可以分为视觉感知和解释部分，其中人类教员演示先验操纵（Demo），机器人从中推断出操纵当前场景所需的抓握信息（抓握）。如果从人手到机器人抓取器有足够的映射，将任务分解为这两个阶段可以使我们的方法扩展到大量不同的抓取器。最终，这为通过自然人类演示来教授机器人铺平了道路，从而实现更高水平的自动化，尤其是在结构较少的环境中。</p><p><strong>本文大致是如何实现的：</strong><br>在从各种不同的角度向机器人演示物体（Demo）的过程中，我们的方法不断跟踪手和物体，这些手和物体被融合到截断有符号距离场（TSDF）中，用于3D重建。使用手和对象的语义分割，可以分离并进一步处理重建，以检索对象和手的完整3D表示。然后，我们利用MANO手部模型提取相关的3D手部网格，并将其与重建对象紧密对齐。在推理过程中，我们使用PPF FoldNet来预测对象是否存在，以及它从对象到相机空间的相对变换。然后，应用所估计的姿势从所估计的手网格导出最终抓握指令。</p><h2 id="1-2-方法-3">1.2 方法</h2><p><strong>总体流程：</strong></p><ol><li>在一组人类演示RGB-D图像上分割手和物体，并使用记录的深度图重建它们的形状</li><li>补全物体形状</li><li>提取手部姿态</li><li>估计对象的6D姿态，转换手部模型，推理抓取指令</li></ol><p><img src="https://img.mahaofei.com/img/202308131124867.png" alt="image.png"></p><h3 id="1-2-1-人-物交互的三维重建">1.2.1 人-物交互的三维重建</h3><p>使用MaskRCNN对手和物体进行分割，并且应用了二进制交叉熵来防止类间竞争。</p><p>利用分割后的深度图像，通过KinectFusion创建相应的TSDF体素，并通过输入帧与TSDF之间的ICP配准实现无漂移跟踪。<br>（因为家用物体几何形状简单，因此同时跟踪手和物体，手的结构复杂稳定了跟踪结果）</p><p>利用分割结果，通过两个单独的TSDF重建将手和物体分离开。</p><h3 id="1-2-2-物体形状补全">1.2.2 物体形状补全</h3><p>由于自遮挡和部分可见性，重建的模型还不完整。</p><p>使用3D CNN直接矫正TSDF体积，然后通过行进立方体进行形状提取。<br>（这里使用了UNet的3D变体，输入是64x64x64的体素，输出每个体素的预测分数表示体素是否被占用。</p><h3 id="1-2-3-手部姿态估计">1.2.3 手部姿态估计</h3><p>从重建的手形状中估计手部参数模型。</p><p>使用MANO手部模型，将手部姿态和形状参数映射到网格中。由于手部也受到了部分遮挡，因此使用辅助接触和碰撞损失联合训练CNN进行手部网格和物体网格估计。</p><p>为了进一步改进抓握位置，使用ICP将手部网格与手部TSDF体素对齐。</p><h3 id="1-2-4-抓取指令生成">1.2.4 抓取指令生成</h3><p>首先检索物体姿态，然后用它来变换手部网格，并用手部模型的拇指和食指计算抓握点。</p><h2 id="1-3-思考-3">1.3 思考</h2><ol><li>物体的三维重建可以采用其他方式，或者结合CAD模型补全的方式，相比于使用3D CNN预测效果会更好。</li><li>手部姿态的提取也可以考虑采用更新的算法，例如识别手部关键点，而不是预测手部网格的方式。</li><li>抓取姿态生成是直接使用拇指食指作为二指抓取姿态，是否可以考虑其他方式，提高抓取的可靠性。</li></ol><h1>2 Learning to Grasp Familiar Objects Based on Experience and Objects’ Shape Affordance</h1><blockquote><p><strong>标题</strong>：基于经验和物体形状的相似目标抓取<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS<br><strong>时间</strong>：2019<br><strong>代码</strong>：</p></blockquote><h2 id="2-1-目标问题">2.1 目标问题</h2><h3 id="2-1-1-现存问题">2.1.1 现存问题</h3><p>对于已知物体的抓取方法，物体具有抓取数据库，机器人通过估计物体姿态，然后利用国旅行假设找到合适抓取姿态，但是这些方法的缺点是不可能将所有对象的模型都放入机器人的数据库。</p><p>需要一种从以前的经验推广到新对象的模型的能力。</p><h3 id="2-1-2-解决思路">2.1.2 解决思路</h3><p>整合人类抓握经验中的关键线索（拇指指尖和手腕的位置方向），提出了一种有效的抓握方法。</p><h2 id="2-2-方法">2.2 方法</h2><h3 id="2-2-1-从不完整点云上生成抓取点">2.2.1 从不完整点云上生成抓取点</h3><p>在抓取时，熟悉对象上的抓取点在对象上具有相似的相对位置。</p><p>基于这个原理，使用3D SHOT形状描述符描述物体，能够精确的描述兴趣点相对于整个对象和表面的位置。具体学习抓取点的过程如下：</p><ol><li>收集从部分点云中选择的兴趣点的SHOT特征、LR特征、RGB特征</li><li>通过计算简单的统计数据，如范围、均值、标准差、熵等，降低沿点维度的特征维度</li><li>将特征输入到用于对象分类的极限学习机中。</li></ol><h3 id="2-2-2-构建抓取模型">2.2.2 构建抓取模型</h3><p>没看懂。</p><p>大概是建立大拇指和物体之间的坐标变换关系，然后将其转换为三指夹爪与物体之间的坐标变换。</p><h3 id="2-2-3-腕关节约束估计">2.2.3 腕关节约束估计</h3><p>主要是解决受外在单一视角下点云被遮挡，无法精确确定手腕方向的问题。</p><h1>3 R3M: A Universal Visual Representation for Robot Manipulation</h1><blockquote><p><strong>标题</strong>：R3M:机器人操纵的通用视觉表示<br><strong>作者团队</strong>：斯坦福大学，Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://tinyurl.com/robotr3m">https://tinyurl.com/robotr3m</a></p></blockquote><h2 id="3-1-目标问题">3.1 目标问题</h2><p>训练机器人根据图像完成操作任务。<strong>给定一段文字，例如“将铲子放到锅里”，机器人根据视觉执行相应的动作</strong>。</p><p><strong>（1）传统方法的局限性</strong></p><p>传统且广泛使用的方法是使用同构数据从头开始训练端到端的模型，但是由于训练数据难以获取，限制了这种方法的泛化。而我们还有没合适的机器人数据集，最近的数据集都是由少数不同环境有限任务组成，因此泛用性受到限制。</p><p><strong>（2）本文的突破思想</strong></p><p>参考<code>ImageNet</code>等通用有效的模型，机器人领域目前还没有类似的模型出现，但是思想可以借鉴，就是使用丰富的<code>in-the-wild data</code>（野生数据？），也就是使用人类与环境交互的视频，这些数据庞大且多样化，包含全球各种场景与任务。</p><p><strong>（3）本文方法简述</strong></p><p>训练了一种机器人操纵表示方法R3M。R3M能够学习具有挑战性的任务，例如将菜放入锅中，折叠毛巾等。</p><h2 id="3-2-方法">3.2 方法</h2><p>本文认为，机器人操作的良好表现由以下三个方面组成</p><ul><li>机器人应该捕获时间动态，因为机器人在环境中要按时间顺序完成任务</li><li>机器人应该捕获于一相关的特征</li><li>机器人应该是紧凑的</li></ul><p><strong>（1）时间对比学习</strong></p><p>训练编码器生成一个表示，是的时间上较近的图像之间的距离小于时间上较远的图像或来自不同视频的图像。</p><p><strong>（2）视频语言对齐</strong></p><p>捕获语言的特征，学习视频场景中的语义部分。</p><p><strong>（3）正则化</strong></p><p>降低状态空间的维度来保证克隆训练的策略符合专家状态分布。</p><h2 id="3-3-思考">3.3 思考</h2><p>与本人方向有差别，本文更偏向于语义，视觉只是作为一个感知手段。</p><h1>4 Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video</h1><blockquote><p><strong>标题</strong>：对抗性技能网络：来自视频的无监督机器人技能学习<br><strong>作者团队</strong>：德国弗赖堡大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="http://robotskills.cs.uni-freiburg.de/">http://robotskills.cs.uni-freiburg.de/</a></p></blockquote><h2 id="4-1-目标任务">4.1 目标任务</h2><p>从未标记的多视角视频中学习机器人操作任务。</p><p><strong>（1）传统方法的局限性</strong></p><p>现有的强化学习方法尽管有一些进展，但是这些方法都是学习每项任务的解决方案，并且依赖于手动的、面向任务设置的奖励函数，所获得的策略也是针对于特定任务的，无法转移到新任务上。</p><p><strong>（2）本文的创新点</strong></p><p>提出一种无监督的技能学习方法，称为对抗性技能网络ASN，通过观看视频来发现和学习可转移的技能。学习到的技能被用于RL，以便通过组合以前的技能来解决更广泛的任务。</p><p>该方法不需要帧和任务ID的对应关系，不需要任何额外的监督。</p><h2 id="4-2-方法">4.2 方法</h2><p><strong>Adversarial Skill Networks对抗性技能网络</strong></p><p>我们在对抗性框架中学习技能度量空间。网络的编码部分试图最大化熵以增强通用性。鉴别器在测试时不使用，它试图最小化其预测的熵，以提高对技能的识别。最后，最大化所有技能的边际类熵会导致所有任务类的统一使用。请注意，不需要关于框架和它们所源自的任务之间关系的信息。<br>（没看懂）</p><h2 id="4-3-思考">4.3 思考</h2><p>似乎可以从无标签的视频中学习任务。但是过于理论化。</p><h1>5 BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning</h1><blockquote><p><strong>标题</strong>：BC-Z:利用机器人模仿学习实现零样本任务泛化<br><strong>作者团队</strong>：谷歌、加州大学伯克利分校、斯坦福<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://sites.google.com/view/bc-z/home">https://sites.google.com/view/bc-z/home</a></p></blockquote><h2 id="5-1-目标问题">5.1 目标问题</h2><p>使基于视觉的机器人操作系统能推广到新任务。</p><p>为此，开发了一个交互式模仿学习系统，可以传达人物的不同信息作为条件，包括自然语言或者人类演示视频，该系统可以从演示中进行学习。并且发现学习到100个任务之后，可以执行24个未训练的任务且不需要演示。</p><p><strong>（1）现存问题</strong></p><p>机器人技术的一大挑战就是创造一种能够在非结构化环境中基于任意的用户命令执行大量任务。这一工作的关键挑战是泛化。机器人必须要能处理新的环境，识别和操纵以前从未见过的物体，并且理解从未被要求执行过的命令的意图。</p><p>传统的方法是在像素级进行端到端的学习，然后由足够的真实世界的数据，这些方法原则上能够使机器人在新的任务、对象、场景中进行泛化。但实际上这一目标还是遥不可及。</p><p>本文要解决的问题就是通过零样本或者少样本推广基于视觉的机器人操纵任务的问题。</p><p><img src="https://img.mahaofei.com/img/202308151409631.png" alt="image.png"></p><h2 id="5-2-数据收集">5.2 数据收集</h2><p>为100个预先指定的任务手机了人类演示的视频，这些视频包含了推物体、拿取放置物体等9项基本任务。</p><p>搭建一套远程操作系统，远程操作设备通过USB连接到机器人上，通过两个手持控制器遥控操作站在机器人后面，使用控制器以第三人称视角操作机器人，机器人实时响应跟随操作员演示各种任务。</p><h2 id="5-3-方法">5.3 方法</h2><h3 id="5-3-1-语言和视频编码">5.3.1 语言和视频编码</h3><p>编码器以语言命令或人类视频作为输入，并生成任务。</p><ul><li>如果是语言命令，使用预训练的多语言语句编码器为每个任务生成512维语言向量</li><li>如果是视频，使用基于ResNet18的卷积网络</li></ul><h3 id="5-3-2-训练策略">5.3.2 训练策略</h3><p>给定固定的任务，我们通过XYZ和轴角预测的Huber损失和抓取器角度的对数损失来训练。</p><p>开环辅助检测，如果以开环的方式运行，将采取是个行动的开环轨迹。开环预测提供了一个辅助训练目标，并可以离线检查闭环规划质量。</p><p>将状态差异作为操作，标准的模仿学习会将演示动作直接作为目标标签，而本文的专家克隆行为会导致一些小动作或抖动，因此考虑将动作定义为未来目标和下一步的差异，使用自适应算法确定手臂和夹爪的移动量。</p><h3 id="5-3-3-网络架构">5.3.3 网络架构</h3><p>使用ResNet18作为主干，从主干最后一个平均池化层分出多个head，每个head是一个多层感知机，对末端执行器动作的一部分进行建模，具体见原文。</p><h2 id="5-3-思考">5.3 思考</h2><p>首先提供演示视频和文字，然后手动控制机器人执行任务收集数据。似乎仍然较为繁琐。</p><h1>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training</h1><blockquote><p><strong>标题</strong>：VIP：通过价值内隐预训练实现普遍的视觉奖励和表现<br><strong>作者团队</strong>：Meta AI，宾夕法尼亚大学<br><strong>期刊会议</strong>：ICLR<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://sites.google.com/view/vip-rl">https://sites.google.com/view/vip-rl</a></p></blockquote><h2 id="6-1-目标问题">6.1 目标问题</h2><p>特定任务的机器人数据的成本较高且稀缺。从大型、多样化的离线人类视频中学习已经成为获得普遍有效的途径。然而如何将这些<strong>人类视频</strong>用于通用的<strong>奖励学习</strong>仍然是一个未解决的问题。</p><p>与模拟环境中的机器人控制不同，真实世界中的机器人任务无法获得很好的环境状态信息或者定义良好的奖励函数。现有的方法学习每一项任务都需要大量的准备工作。相反，一个简单的方法来指定真实世界操作任务就是提供一个目标图像，图像捕捉环境所需要的视觉变化。然而现有的方法不能产生有效的奖励函数。</p><p>本文提出了一种隐含价值预训练方法（Value-Implicit Pre-training, VIP），一种自监督的预训练视觉表示，为机器人任务生成奖励函数。</p><p>本文的关键在于，将强化学习本身作为强化学习的预训练机制，但是由于人类视频中没有可以用于策略学习的动作信息，因此我们使用这种双价值函数，在没有动作的情况下以完全自我监督的方式进行预训练。</p><h2 id="6-2-方法">6.2 方法</h2><p><strong>（1）从人类视频中自我监督的价值学习</strong></p><p>虽然人类视频不是机器人域的数据，但是它们是学习人类行动的目标条件策略的领域中的数据。因此考虑使用离线人类视频进行学习的一个合理方法是在人类的策略空间上解决目标条件的强化学习问题，提取视觉表示（本文考虑使用 KL 方法进行离线强化学习）。</p><p>由于动作不出现在这个强化学习的目标中，并且所有数据都可以用离线数据集采样，因此可以通过适当选择奖励函数来对双价值函数进行自监督。</p><p><strong>（2）隐含的时间对比学习</strong></p><p>当有意义的指示任务的开始和结束的两个帧在嵌入空间中接近时，初始帧和目标帧之间能够捕获长程语义时间依赖性。</p><p><strong>（3）基于隐含价值的预训练</strong></p><p>具体算法见原文。</p><h2 id="6-3-代码实验">6.3 代码实验</h2><p>VIP 算法使用 ResNet50 作为视觉 backbone，并在 Ego4D 数据集上进行训练。</p><p>算法与 R3M 进行了比较</p><h2 id="6-3-思考">6.3 思考</h2><h1>7 Graph-Structured Visual Imitation</h1><blockquote><p><strong>标题</strong>：图形结构的视觉模仿<br><strong>作者团队</strong>：索尼<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2019<br><strong>代码</strong>：无</p></blockquote><h2 id="7-1-目标问题">7.1 目标问题</h2><p>当机器人动作使工作空间中检测到的相应视觉实体的相对空间配置与演示更好的匹配时，会得到奖励。</p><p>本文使用人类手指关键点检测器、使用合成增强进行离线训练的对象检测器、由视点变化监督的点检测器。在没有人类注释数据或机器人交互的情况下为每次演示学习多个视觉实体检测器。</p><h2 id="7-2-方法">7.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308151609226.png" alt="image.png"></p><p><strong>（1）检测视觉实体</strong></p><p>人手关键点检测：使用现有的手部检测器，并使用D435i获取3D位置。将机器人平行钳口夹持器映射到演示者的拇指和食指指尖。通过在两个指尖设置距离阈值来检测抓取和释放动作。</p><p>点特征检测器：训练后，在模仿者和演示者的环境中匹配点特征，建立对应关系。</p><p>合成数据扩充：使用背景移除来提取出2D掩模，并使用合成数据增强来训练视觉检测器。</p><p><strong>（2）动态图构造的运动显著性</strong></p><p><strong>（3）基于可视化实体图的策略学习</strong></p><p>目标是当机器人从单个人类演示中模仿物体操纵任务。具体的成本代价函数参考原文。</p><h2 id="7-3-思考">7.3 思考</h2><p>提取手部关键点映射到机器人夹爪，同时使用物体关键点检测来实现运动策略的生成。思路上不如DemoGrasp更直观。</p><p>可行的方法</p><ol><li>提取演示视频中物体位姿</li><li>模仿执行动作<ol><li>将当前视角下物体位姿与演示视频中每一帧位姿计算误差损失</li><li>根据误差实时计算末端位姿调整姿态</li><li>将当前帧手臂关键点加入，获取机械臂各关节应到的位姿</li><li>执行机械臂动作（期间加入机械臂避障与轨迹平滑）</li></ol></li></ol><h1>8 Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</h1><blockquote><p><strong>标题</strong>：通过观看学习：人体视频中操纵技能的物理模拟<br><strong>作者团队</strong>：多伦多大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="http://www.pair.toronto.edu/lbw-kp/">http://www.pair.toronto.edu/lbw-kp/</a></p></blockquote><h2 id="8-1-目标问题">8.1 目标问题</h2><p>通过观看学习，通过模仿指定任务的单个视频来进行策略学习的算法框架。</p><ul><li>由于人类手臂与机器人手臂形态不同，我们的框架学习无监督的人-机器人的翻译来克服形态不匹配问题。</li><li>为了捕捉对学习状态至关重要的显著区域的细节，我们的模型采取了无监督关键点检测。检测到的关键点形成包含语义上有意义的信息的结构化表示，并可以直接用于计算奖励和策略学习。</li></ul><h2 id="8-2-方法">8.2 方法</h2><p>本文所提出的LbW框架由三个部分组成</p><ul><li>图像到图像的翻译网络：逐帧翻译输入的人类演示视频，生成机器人演示视频</li><li>关键点检测器：将生成的机器人演示视频作为输入，提取每帧的关键点，形成关键点轨迹</li><li>策略网络：将当前的基于关键点的观察表示传递给策略网络，用于预测与环境交互的动作</li></ul><p><img src="https://img.mahaofei.com/img/202308151841683.png" alt="image.png"></p><h2 id="8-3-思考">8.3 思考</h2><p>与其说是模仿学习网络，不如说是一个图像翻译网络，基于CycleGAN的图像翻译，将人手演示翻译成机器人动作视频，然后提取视频中机器人的关键点轨迹，通过策略函数实现实物机器人的动作。</p><h1>9 Learning Periodic Tasks from Human Demonstrations</h1><blockquote><p><strong>标题</strong>：从人类演示中学习周期性任务<br><strong>作者团队</strong>：卡内基梅隆大学<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="9-1-目标问题">9.1 目标问题</h2><p>使用主动学习来优化参数，提出了一个目标最大限度的提高机器人操纵物体的运动与演示视频中物体运动之间的相似性。重点在于可变形物体和颗粒物体。（用布擦拭表面，缠绕电缆，用勺子搅拌颗粒物质等）</p><h2 id="9-2-方法">9.2 方法</h2><p>本文提出的框架由两部分组成</p><ul><li>表示学习模块：关键点检测模型从独立收集的非特定任务的人类和机器人数据中提取一致的关键点</li><li>姿态优化模块：将产生在检测的关键点方面与人类演示相匹配的机器人视频</li></ul><h2 id="9-3-思考">9.3 思考</h2><p>给定人类演示动作和手动操控机器人演示动作，机器人学习两者的相似性，然后重复演示动作使其更接近人类演示效果。</p><h1>10 One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks</h1><blockquote><p><strong>标题</strong>：复合视觉运动任务的一次性层次模拟学习<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<a href="https://sites.google.com/view/one-shot-hil">https://sites.google.com/view/one-shot-hil</a></p></blockquote><h2 id="10-1-目标问题">10.1 目标问题</h2><p>真实机器人上从人类执行任务的视频中学习多阶段任务。</p><h2 id="10-2-方法">10.2 方法</h2><p>对于每个子任务，我们提供多个人类演示和多个机器人演示（需要对象和执行的任务对应，但是不用相同的对象位置、执行速度）</p><p><strong>（1）基元的合成</strong>：训练了一个人类相位预测器和机器人相位预测器，从人类执行视频中学习特定的机器人策略</p><p><strong>（2）原始相位预测</strong>：学习如何分割复合任务的人类演示；何时学习策略过度到下一个。</p><h2 id="10-3-思考">10.3 思考</h2><p>提供人的演示视频，机器人的演示视频，然后训练策略。最后利用训练的策略，提供一段人类演示视频，机器人执行对应的操作。</p><h1>11 Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller</h1><blockquote><p><strong>标题</strong>：基于解耦层次控制器的第三人称视觉模仿学习<br><strong>作者团队</strong>：MIT<br><strong>期刊会议</strong>：NeurIPS<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="https://pathak22.github.io/hierarchical-imitation/">https://pathak22.github.io/hierarchical-imitation/</a></p></blockquote><h2 id="11-1-目标问题">11.1 目标问题</h2><p>通过从第三人称视角观看人类演示视频，可以在未知场景中操纵新物体。</p><h2 id="11-2-方法">11.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308152040902.png" alt="image.png"></p><p><strong>（1）目标生成器</strong></p><p>从人类演示视频中推断像素空间中的目标，并以像素级的表示形式将其转化为机器人环境中的目标。</p><p>也是使用图像翻译的方法，将人类演示图像翻译为机器人演示图象。</p><p><strong>（2）反向控制器</strong></p><p>跟踪视觉目标推理模型中生成的线索，并生成机器人要执行的动作。</p><p>使用ResNet18模型。</p><p><strong>（3）第三人称模仿</strong></p><p>以交替方式运行目标生成器和反向控制器。目标生成器生成子目标，低级控制器生成机器人关节角度，直到人类演示结束。</p><h2 id="11-3-思考">11.3 思考</h2><p>还是使用图像翻译的思路，把人手操作图像翻译成机械臂操作图像，再由控制器生成机器人关节角度。</p><h1>12 You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration</h1><blockquote><p><strong>标题</strong>：Yodo：单一视觉演示的类别级操作<br><strong>作者团队</strong>：罗格斯大学<br><strong>期刊会议</strong>：RSS<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="12-1-目标问题">12.1 目标问题</h2><p>由于最近的跨对象类别级操作虽然有很好的结果，但通常需要昂贵的真实数据收集和为每个对象类别和任务手动指定语义关键点。并且粗略的关键点预测和忽略中间动作序列阻止了在抓取和防止之外的复杂任务的应用。</p><p>本工作提出了一种新的操作框架。该框架利用了无模型6D跟踪技术，解析单个演示视频中的类别级任务轨迹，整个执行过程被分解为远程、无碰撞运动和最后一英寸操作三个步骤。</p><h2 id="12-2-方法">12.2 方法</h2><p>对于每个演示视频帧，通过无模型6D位姿估计跟踪目标位姿，对象位姿在容器的坐标系中表示，这样允许泛化到新的场景。</p><p><strong>（1）类别级表示的离线学习</strong></p><p>建立了一个9D物体表示方法，6D位姿+3D缩放</p><p><strong>（2）无模型的物体6D跟踪</strong></p><p>物体运动跟踪要实现两个目的</p><ul><li>演示阶段，解析录制的视频，提取容器坐标系中被操纵的对象的6D运动轨迹</li><li>在线执行期间，为闭环控制器提供视觉反馈</li></ul><p><strong>（3）类别级行为克隆作为最后一步策略</strong></p><p>产生密集的离散轨迹，以便机器人能沿轨迹到达下一个目标</p><p><strong>（4）基于局部注意的动态类别级框架</strong></p><p>自动动态地规范坐标系原点。</p><p><strong>（5）抓取物体并使其沿关键点移动</strong></p><p>常规的抓取方法</p><h2 id="12-3-思考">12.3 思考</h2><p>将目标位姿表示为相对于另一个物体的相对位姿，这样有助于场景的泛化。</p><p>整体思想就是使用6D位姿估计获得目标的运动轨迹，然后重复这条轨迹。</p>]]></content>
    
    
    <summary type="html">从人类抓取物体的视频中学习机器人抓取的相关工作调研。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu截图与录屏工具</title>
    <link href="https://www.mahaofei.com/post/9c4a6943.html"/>
    <id>https://www.mahaofei.com/post/9c4a6943.html</id>
    <published>2023-05-26T07:31:45.000Z</published>
    <updated>2023-05-26T07:31:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 截屏软件：Shutter</h1><h2 id="1-1-介绍">1.1 介绍</h2><p><a href="http://shutter-project.org/">Shutter</a> 是一个对所有主流 Linux 发行版都适用的屏幕截图工具，功能十分强大，且近几年已知保持更新，功能稳定。</p><p>可以实现以下功能：</p><ul><li>截屏特定区域、整个桌面、整个框口、某个网站</li><li>倒计时截屏</li><li>截屏后标注（文本、箭头、举行、椭圆、马赛克、自动编号）</li></ul><h2 id="1-2-安装">1.2 安装</h2><p>添加软件库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:linuxuprising/shutter</span><br></pre></td></tr></table></figure><p>安装shutter与gnome-web-photo增强</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install shutter gnome-web-photo</span><br></pre></td></tr></table></figure><h2 id="1-3-使用">1.3 使用</h2><p>开始栏找到shutter，打开即可使用。</p><p><img src="https://img.mahaofei.com/img/202311261548285.png" alt="image.png"></p><p>设置快捷键：打开系统设置-Keyboard Shortcuts，添加自定义快捷键，设置如下</p><p><img src="https://img.mahaofei.com/img/202311261552664.png" alt="image.png"></p><p>设置完成后就可以直接【Ctrl + Alt + A】截屏了。</p><h1>2 录屏工具 Obs-Studio</h1><h2 id="2-1-介绍">2.1 介绍</h2><p>OBS Studio 是一款用于视频录制和直播的免费、轻量级开源软件。 它可用于捕获、录制、流式传输和编码视频内容。 它可以同时录制来自多个来源的视频，组合并将它们流式传输到流媒体平台。</p><p>简单来说，他可以把屏幕、摄像头等放在一个画面里进行录制，或者直播推流，这里我只用它来录屏。</p><h2 id="2-2-安装">2.2 安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:obsproject/obs-studio</span><br><span class="line">sudo apt install obs-studio</span><br></pre></td></tr></table></figure><h2 id="2-3-使用">2.3 使用</h2><p><img src="https://img.mahaofei.com/img/202311261602791.png" alt="image.png"></p><p>通过Sources的小加号可以添加屏幕、摄像头等。</p><p>添加屏幕后，在obs主界面按住alt，拖动画面边框可以实现裁剪。</p>]]></content>
    
    
    <summary type="html">Ubuntu上默认只能截屏不能截屏，且体验十分差，因此介绍两款个人常用的截屏和录屏软件。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Ubuntu" scheme="https://www.mahaofei.com/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ROS系统Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/4add66b0.html"/>
    <id>https://www.mahaofei.com/post/4add66b0.html</id>
    <published>2023-05-15T09:05:05.000Z</published>
    <updated>2023-05-15T09:05:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、安装问题</h1><h2 id="ROS安装时rosdep-init与rosdep-update问题解决方法">ROS安装时rosdep_init与rosdep_update问题解决方法</h2><p><strong>解决方法</strong></p><p>使用下面的命令替代上面两行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3-pip</span><br><span class="line">sudo pip3 install rosdepc</span><br><span class="line">sudo rosdepc init</span><br><span class="line">rosdepc update</span><br></pre></td></tr></table></figure><h1>二、环境问题</h1><h2 id="Unable-to-find-either-executable-‘empy’-or-Python-module-‘em’…-try-installing-the-package-‘python3-empy’">Unable to find either executable ‘empy’ or Python module ‘em’…  try  installing the package ‘python3-empy’</h2><p><strong>（1）问题原因</strong></p><p>Anaconda使用的是Python3版本，但是ROS使用的Python2</p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="Could-not-find-a-package-configuration-file-provided-by-“某某包”-with-any-of-the-following-names">Could not find a package configuration file provided by “某某包” with any of  the following names</h2><p><strong>（1）问题原因</strong></p><p>缺少<code>某某包</code></p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ros-noetic-某某包</span><br></pre></td></tr></table></figure><h1>三、配置问题</h1><h2 id="ERROR-cannot-launch-node-of-type-robot-state-publisher-state-publisher-Cannot-locate-node-of-type-state-publisher-in-package-robot-state-publisher-Make-sure-file-exists-in-package-path-and-permission-is-set-to-executable-chmod-x）">ERROR: cannot launch node of type [robot_state_publisher/state_publisher]: Cannot locate node of type [state_publisher] in package [robot_state_publisher]. Make sure file exists in package path and permission is set to executable (chmod +x）</h2><p><strong>（1）问题原因</strong></p><p>使用launch文件启动某个节点时出现这个问题，是因为launch文件中name、pkg、type不统一导致的。</p><p><strong>（2）解决方法</strong></p><p>检查launch文件，确保name、pkg、type一样，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot; /&gt;</span><br></pre></td></tr></table></figure><h2 id="joint-state-publisher-gui没有显示">joint state publisher gui没有显示</h2><p><strong>（1）问题描述</strong></p><p>使用ROS进行仿真，想用joint state publisher进行机械臂控制，但是启动launch文件后没有报错信息，但也没有joint state publisher gui。</p><p><strong>（2）解决方法</strong></p><p>2020年开始，gui已经移出了 joint state publisher, 并且成为了一个新的package：joint state publisher gui. 之前那种使用gui参数的方式调用joint state publisher 是仍然可行的，但是不会调用gui。</p><p>在launch文件中，将joint state publisher 替换成joint__state__publisher_gui。</p>]]></content>
    
    
    <summary type="html">在使用ROS系统进行机器人实验中，遇到的各种错误信息汇总，不定时更新。</summary>
    
    
    
    <category term="机器人" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="ros" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/ros/"/>
    
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
  </entry>
  
</feed>
