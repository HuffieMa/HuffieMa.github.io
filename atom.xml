<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>马浩飞丨博客</title>
  
  <subtitle>无限进步！！！</subtitle>
  <link href="https://www.mahaofei.com/atom.xml" rel="self"/>
  
  <link href="https://www.mahaofei.com/"/>
  <updated>2023-08-13T10:22:02.000Z</updated>
  <id>https://www.mahaofei.com/</id>
  
  <author>
    <name>马浩飞</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Latex基础</title>
    <link href="https://www.mahaofei.com/post/f22aab63.html"/>
    <id>https://www.mahaofei.com/post/f22aab63.html</id>
    <published>2023-08-13T10:22:02.000Z</published>
    <updated>2023-08-13T10:22:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Latex文件结构"><a href="#一、Latex文件结构" class="headerlink" title="一、Latex文件结构"></a>一、Latex文件结构</h1><h2 id="1-1-源文件基本结构"><a href="#1-1-源文件基本结构" class="headerlink" title="1.1 源文件基本结构"></a>1.1 源文件基本结构</h2><p>一个Latex文档分为导言区和正文区。</p><p><strong>（1）导言区</strong></p><p>导言区主要进行全局设置。</p><ul><li>使用<code>\documentclass&#123;&#125;</code>引入文档类，如<code>book, article, report, letter</code>类等，不同的类的格式不同，例如book和letter有封面，letter类没有title等。</li><li>使用<code>\title&#123;&#125;</code>命令定义文档的标题</li><li>使用<code>\author&#123;&#125;</code>命令定义文档的作者</li><li>使用<code>\date&#123;&#125;</code>命令定义文档的创作时间</li></ul><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 导言区</span></span><br><span class="line"><span class="keyword">\documentclass</span>&#123;article&#125; <span class="comment">% book, report, letter</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\title</span>&#123;My Latex Tutorial&#125;</span><br><span class="line"><span class="keyword">\author</span>&#123;Haofei Ma&#125;</span><br><span class="line"><span class="keyword">\date</span>&#123;<span class="keyword">\today</span>&#125; <span class="comment">% \today指今天</span></span><br></pre></td></tr></table></figure><p><strong>（2）正文区</strong></p><p>正文区撰写文档的内容。</p><ul><li>使用<code>\begin&#123;&#125;</code>和<code>\end&#123;&#125;</code>输入一个环境，<code>&#123;&#125;</code>内是环境的名称，一个latex文件只能有一个ducument环境</li><li>使用<code>\maketitle</code>输出标题</li></ul><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 正文区  </span></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125; <span class="comment">% document为环境名称</span></span><br><span class="line"><span class="keyword">\maketitile</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 内容(可以直接输入，空行代表段落)</span></span><br><span class="line">Hello World!</span><br><span class="line"></span><br><span class="line">Let <span class="built_in">$</span>F(x)<span class="built_in">$</span> be defined by formula <span class="built_in">$</span><span class="built_in">$</span>f(x)=3x<span class="built_in">^</span>2-x+1<span class="built_in">$</span><span class="built_in">$</span>.</span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h2 id="1-2-中文处理方法"><a href="#1-2-中文处理方法" class="headerlink" title="1.2 中文处理方法"></a>1.2 中文处理方法</h2><p>保证文档的编码为<code>UTF-8</code>格式，在导言区使用<code>\usepackage&#123;ctex&#125;</code>命令引入ctex宏包。</p><p>撰写中文时可以使用<code>\heiti</code>, <code>\kaishu</code>等命令指定字体为黑体、楷书等等。</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%导言区  </span></span><br><span class="line"><span class="keyword">\documentclass</span>&#123;article&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\title</span>&#123;<span class="keyword">\heiti</span> Latex基础&#125;<span class="comment">%指定黑体字体  </span></span><br><span class="line"><span class="keyword">\author</span>&#123;<span class="keyword">\kaishu</span> 马浩飞&#125;<span class="comment">%指定楷书字体  </span></span><br><span class="line"><span class="keyword">\date</span>&#123;<span class="keyword">\today</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\usepackage</span>&#123;ctex&#125;<span class="comment">%显示中文需要添加该指令</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="1-3-字体字号设置"><a href="#1-3-字体字号设置" class="headerlink" title="1.3 字体字号设置"></a>1.3 字体字号设置</h2><p>字体字号的设置都是在正文区进行设置（因为字体设置仅针对某一段文字）。</p><p><strong>（1）字体族设置</strong></p><p>使用<code>\textrm, \textsf, \textttt</code>设置字体族。</p><p>或使用<code>\rmfamily Roman Family</code>声明后续的字体为罗马字体。</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%正文区</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"><span class="comment">%字体族设置(罗马字体、无衬线字体、打字机字体)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%\textrm等是字体命令，大括号里是作用到的文字</span></span><br><span class="line"><span class="keyword">\textrm</span>&#123;Roman Family&#125;</span><br><span class="line"><span class="keyword">\textsf</span>&#123;Scan Serif Family&#125;</span><br><span class="line"><span class="keyword">\texttt</span>&#123;Typewriter Family&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">%\rmfamily是字体声明，后面紧跟的文字是作用到的文字</span></span><br><span class="line">&#123;<span class="keyword">\rmfamily</span> Roman Family&#125;</span><br><span class="line">&#123;<span class="keyword">\sffamily</span> Scan Serif Family&#125;</span><br><span class="line">&#123;<span class="keyword">\ttfamily</span> Typewriter Family&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><p>括号可以限定字体声明的限定范围。</p><ul><li>如果加括号，代表设置括号内的字体为指定字体；</li><li>如果无括号，代表后续所有段落字体为指定字体。</li></ul><p>遇到另一个字体声明时，会结束当前字体声明，并启用新的字体声明。</p><p><strong>（2）字体系列（粗细、宽度）</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%字体系列设置(粗细、宽度)</span></span><br><span class="line"><span class="keyword">\textmd</span>&#123;Medium Series&#125;<span class="comment">%\设置字体宽度，大括号里是作用到的文字</span></span><br><span class="line"><span class="keyword">\textbf</span>&#123;Boldface Series&#125;<span class="comment">% \textbf可以对字体加粗</span></span><br><span class="line">&#123;<span class="keyword">\mdseries</span> Medium Series&#125;<span class="comment">%\设置字体宽度，大括号里是作用到的文字</span></span><br><span class="line">&#123;<span class="keyword">\bfseries</span> Boldface Series&#125;<span class="comment">%字体声明</span></span><br></pre></td></tr></table></figure><p><strong>（3）字体形状（直立、斜体、伪斜体、小型大写）</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%字体形状(直立、斜体、伪斜体、小型大写)</span></span><br><span class="line"><span class="keyword">\textup</span>&#123;Upright Shape&#125; <span class="keyword">\textit</span>&#123;Italic Shape&#125; <span class="comment">%字体命令</span></span><br><span class="line"><span class="keyword">\textsl</span>&#123;Slanted Shape&#125; <span class="keyword">\textsc</span>&#123;Small Caps Shape&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="keyword">\upshape</span> Upright Shape&#125; &#123;<span class="keyword">\itshape</span> Italic Shape &#125;<span class="comment">%字体声明</span></span><br><span class="line">&#123;<span class="keyword">\slshape</span> Slanted Shape&#125;</span><br><span class="line">&#123;<span class="keyword">\scshape</span> Small Caps Shape&#125;</span><br></pre></td></tr></table></figure><p><strong>（4）中文字体设置</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="keyword">\songti</span> 宋体&#125; <span class="keyword">\quad</span>&#123;heiti 黑体&#125;<span class="keyword">\quad</span>&#123;<span class="keyword">\fangsong</span> 仿宋&#125;<span class="keyword">\quad</span> &#123;<span class="keyword">\kaishu</span> 楷书&#125;<span class="comment">%\quad表示空格</span></span><br><span class="line">中文字体的<span class="keyword">\textbf</span>&#123;粗体表现为黑体&#125;与<span class="keyword">\textit</span>&#123;斜体表现为楷体&#125;</span><br></pre></td></tr></table></figure><p><strong>（5）字体大小</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="keyword">\tiny</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\scriptsize</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\footnotesize</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\small</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\normalsize</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\large</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\Large</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\LARGE</span>  Hello &#125;<span class="keyword">\\</span></span><br><span class="line">&#123;<span class="keyword">\huge</span>  Hello &#125;<span class="keyword">\\</span> </span><br></pre></td></tr></table></figure><h2 id="1-4-文档基本结构"><a href="#1-4-文档基本结构" class="headerlink" title="1.4 文档基本结构"></a>1.4 文档基本结构</h2><p>Latex使用<code>\section&#123;&#125;, \subsection&#123;&#125;, \subsubsection&#123;&#125;</code>等命令构建文章的提纲。</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%正文区</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"><span class="keyword">\maketitle</span> <span class="comment">%使得导言区的设置生效</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;引言&#125; <span class="comment">% 1 引言</span></span><br><span class="line"></span><br><span class="line">引言的内容第一段。</span><br><span class="line"></span><br><span class="line">使用空行来实现分段，这里是引言内容的第二段。</span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;实验方法&#125; <span class="comment">% 2 实验方法</span></span><br><span class="line"><span class="keyword">\section</span>&#123;实验过程&#125; <span class="comment">% 3 实验过程</span></span><br><span class="line"><span class="keyword">\subsection</span>&#123;实验过程1&#125; <span class="comment">% 3.1 实验过程1</span></span><br><span class="line"><span class="keyword">\subsection</span>&#123;实验过程2&#125; <span class="comment">% 3.2 实验过程2</span></span><br><span class="line"><span class="keyword">\subsubsection</span>&#123;实验数据&#125; <span class="comment">% 3.2.1 实验数据</span></span><br><span class="line"><span class="keyword">\subsubsection</span>&#123;实验图表&#125; <span class="comment">% 3.2.2 实验图表</span></span><br><span class="line"><span class="keyword">\subsection</span>&#123;结果分析&#125;  <span class="comment">% 3.3 结果分析</span></span><br><span class="line"><span class="keyword">\section</span>&#123;结论&#125; <span class="comment">% 4 结论</span></span><br><span class="line"><span class="keyword">\section</span>&#123;致谢&#125; <span class="comment">% 5 致谢</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h1 id="二、Latex中的特殊字符"><a href="#二、Latex中的特殊字符" class="headerlink" title="二、Latex中的特殊字符"></a>二、Latex中的特殊字符</h1><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%正文区</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"><span class="keyword">\section</span>&#123;空白符号&#125;</span><br><span class="line"><span class="comment">% 空行分段，多个空行等同1个</span></span><br><span class="line"><span class="comment">% 自动缩进，绝对不能使用空格代替</span></span><br><span class="line"><span class="comment">% 英文中多个空格处理为1个空格，中文中空格将被忽略</span></span><br><span class="line"><span class="comment">% 汉字与其它字符的间距会自动被处理</span></span><br><span class="line"><span class="comment">% 禁止使用中文全角空格</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 1em（当前字体中M的宽度）</span></span><br><span class="line">a<span class="keyword">\quad</span> b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 2em</span></span><br><span class="line">a<span class="keyword">\qquad</span> b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 1/6em</span></span><br><span class="line">a<span class="keyword">\,</span>b a<span class="keyword">\thinspace</span> b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 0.5em</span></span><br><span class="line">a<span class="keyword">\enspace</span> b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 空格</span></span><br><span class="line">a<span class="keyword">\ </span>b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 硬空格（不能分割的空格）</span></span><br><span class="line">a~b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 指定宽度的空格</span></span><br><span class="line">a<span class="keyword">\kern</span> 1em b</span><br><span class="line">a<span class="keyword">\hspace</span>&#123;35pt&#125;b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 占位宽度（&#123;&#125;中的字符宽度）</span></span><br><span class="line">a<span class="keyword">\hphantom</span>&#123;xyz&#125;b</span><br><span class="line"></span><br><span class="line"><span class="comment">% 弹性长度（撑满整个空间）</span></span><br><span class="line">a<span class="keyword">\hfill</span> b</span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;<span class="keyword">\ </span>LaTex 控制符&#125; <span class="comment">% 相当于转义字符，`\`使用\textbackslash产生</span></span><br><span class="line"><span class="keyword">\#</span> <span class="keyword">\$</span> <span class="keyword">\&#123;</span>  <span class="keyword">\&#125;</span> <span class="keyword">\~</span>&#123;&#125; <span class="keyword">\_</span>&#123;&#125; <span class="keyword">\^</span>&#123;&#125; <span class="keyword">\textbackslash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;排版符号&#125;</span><br><span class="line"><span class="keyword">\S</span> <span class="keyword">\P</span> <span class="keyword">\dag</span> <span class="keyword">\ddag</span> <span class="keyword">\copyright</span> <span class="keyword">\pounds</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;<span class="keyword">\ </span>Tex 标志符号&#125;</span><br><span class="line"><span class="keyword">\ </span>TeX&#123;&#125; <span class="keyword">\ </span>LaTeX&#123;&#125;  <span class="keyword">\ </span>LaTeXe&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;引号&#125;</span><br><span class="line">`&#x27;  <span class="comment">%  `表示单引号的左边，&#x27;表示单引号的右边</span></span><br><span class="line">`` &#x27;&#x27;   <span class="comment">%  ``表示双引号的左边，&#x27;&#x27;表示双引号的右边</span></span><br><span class="line">``被引号包裹&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;连字符&#125;</span><br><span class="line">- -- ---</span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;非英文字符&#125;</span><br><span class="line"><span class="keyword">\oe</span> <span class="keyword">\OE</span> <span class="keyword">\AE</span> <span class="keyword">\aa</span> <span class="keyword">\AA</span> <span class="keyword">\o</span> <span class="keyword">\O</span> <span class="keyword">\l</span> <span class="keyword">\L</span> <span class="keyword">\ss</span> <span class="keyword">\SS</span> !` ?`</span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;重音符号&#125;</span><br><span class="line"><span class="keyword">\`</span>o <span class="keyword">\&#x27;</span>o <span class="keyword">\^</span>o <span class="keyword">\&#x27;</span>&#x27;o <span class="keyword">\~</span>o <span class="keyword">\=</span>o <span class="keyword">\.</span>o <span class="keyword">\u</span>&#123;o&#125; <span class="keyword">\v</span>&#123;o&#125; <span class="keyword">\H</span>&#123;o&#125; <span class="keyword">\r</span>&#123;o&#125; <span class="keyword">\t</span>&#123;o&#125; <span class="keyword">\b</span>&#123;o&#125; <span class="keyword">\c</span>&#123;o&#125; <span class="keyword">\d</span>&#123;o&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h1 id="三、Latex中的图表"><a href="#三、Latex中的图表" class="headerlink" title="三、Latex中的图表"></a>三、Latex中的图表</h1><h2 id="3-1-图"><a href="#3-1-图" class="headerlink" title="3.1 图"></a>3.1 图</h2><p>在导言区使用<code>\usepackage&#123;grahpicx&#125;</code>命令引入graphicx宏包。</p><p>语法：<code>\includegraphics[选项]&#123;文件名&#125;</code>，选项用于设置缩放比例、旋转等</p><p>格式：<code>EPS, PDF, PNG, JPEG, BMP</code></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%导言区 </span></span><br><span class="line"><span class="keyword">\documentclass</span>&#123;article&#125;  </span><br><span class="line"><span class="keyword">\title</span>&#123;My Latex Tutorial&#125;</span><br><span class="line"><span class="keyword">\author</span>&#123;Haofei Ma&#125;</span><br><span class="line"><span class="keyword">\date</span>&#123;<span class="keyword">\today</span>&#125;  </span><br><span class="line"><span class="keyword">\usepackage</span>&#123;ctex&#125;  </span><br><span class="line"><span class="keyword">\usepackage</span>&#123;graphicx&#125;  </span><br><span class="line"><span class="keyword">\graphicspath</span>&#123;&#123;figures/&#125;,&#123;pics/&#125;&#125;<span class="comment">%表示图片在当前目录下的figures目录或pics/目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%正文区  </span></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;  </span><br><span class="line"><span class="comment">% 插入图片</span></span><br><span class="line"><span class="keyword">\includegraphics</span>&#123;filename&#125;  </span><br><span class="line"><span class="keyword">\includegraphics</span>&#123;filename2&#125;  </span><br><span class="line"><span class="keyword">\includegraphics</span>&#123;filename3&#125;<span class="comment">%two是figures文件夹下的文件(图像)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 设置缩放比例</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[scale=0.3]&#123;filename&#125;  </span><br><span class="line"></span><br><span class="line"><span class="comment">% 高度设置</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[height=0.1<span class="keyword">\textheight</span>]&#123;filename&#125;  <span class="comment">%文档高度0.1倍高度</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[height=2cm]&#123;filename&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 宽度设置</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[width=0.1<span class="keyword">\textwidth</span>]&#123;filename&#125;  <span class="comment">% 文档宽度0.1倍宽度</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[width=2cm]&#123;filename&#125;  <span class="comment">% 文档宽度0.1倍宽度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 旋转角度</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[angle=-45,width=2cm]&#123;filename&#125;  </span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-表"><a href="#3-2-表" class="headerlink" title="3.2 表"></a>3.2 表</h2><p>使用<code>\begin&#123;tabular&#125;</code>环境生成表格，生成环境时指定列数和每一列的对齐方式</p><p>语法：<code>\begin&#123;tabular&#125;[垂直对齐方式]&#123;列格式说明&#125;</code></p><ul><li><code>l</code>: 本列左对齐</li><li><code>c</code>: 本列居中对齐</li><li><code>r</code>: 本列右对齐</li><li><code>p&#123;宽度&#125;</code>: 本列宽度固定，内容超过宽度时会自动换行</li><li><code>\\</code>: 表示换行</li><li><code>&amp;</code>: 表示不同的列</li><li><code>|</code>: 是否在列间添加竖线（<code>||</code>双竖线）</li><li><code>\hline</code>: 在行间添加横线（<code>\hline \hline</code>双横线）</li></ul><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%正文区</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;tabular&#125;&#123;|l| c| c| c|  r|&#125;<span class="comment">%会有5列，指定每列的居中形式,|表示每列中间有竖线分开</span></span><br><span class="line"><span class="keyword">\hline</span><span class="comment">%每行之间由横线分开</span></span><br><span class="line">姓名<span class="built_in">&amp;</span>语文<span class="built_in">&amp;</span>数学<span class="built_in">&amp;</span>外语<span class="built_in">&amp;</span>政治<span class="keyword">\\</span><span class="comment">%\\表示换行</span></span><br><span class="line"><span class="keyword">\hline</span></span><br><span class="line">张三<span class="built_in">&amp;</span>87<span class="built_in">&amp;</span>120<span class="built_in">&amp;</span>25<span class="built_in">&amp;</span>36<span class="keyword">\\</span></span><br><span class="line"><span class="keyword">\hline</span></span><br><span class="line">张1<span class="built_in">&amp;</span>87<span class="built_in">&amp;</span>120<span class="built_in">&amp;</span>25<span class="built_in">&amp;</span>36<span class="keyword">\\</span></span><br><span class="line"><span class="keyword">\hline</span></span><br><span class="line">张2<span class="built_in">&amp;</span>87<span class="built_in">&amp;</span>120<span class="built_in">&amp;</span>25<span class="built_in">&amp;</span>36<span class="keyword">\\</span></span><br><span class="line"><span class="keyword">\hline</span></span><br><span class="line"><span class="keyword">\end</span>&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h1 id="四、Latax中的浮动体"><a href="#四、Latax中的浮动体" class="headerlink" title="四、Latax中的浮动体"></a>四、Latax中的浮动体</h1><h1 id="五、Latex中的数学公式"><a href="#五、Latex中的数学公式" class="headerlink" title="五、Latex中的数学公式"></a>五、Latex中的数学公式</h1><p>使用<code>\equation&#123;&#125;</code>命令产生带编号的行间公式。</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;equation&#125; <span class="comment">%表示带编号的行间公式  </span></span><br><span class="line">AB<span class="built_in">^</span>2=BC<span class="built_in">^</span>2+AC<span class="built_in">^</span>2  </span><br><span class="line"><span class="keyword">\end</span>&#123;equation&#125;</span><br></pre></td></tr></table></figure><h1 id="六、Latex中的参考文献"><a href="#六、Latex中的参考文献" class="headerlink" title="六、Latex中的参考文献"></a>六、Latex中的参考文献</h1><h1 id="七、Latex中的自定义命令和环境"><a href="#七、Latex中的自定义命令和环境" class="headerlink" title="七、Latex中的自定义命令和环境"></a>七、Latex中的自定义命令和环境</h1>]]></content>
    
    
    <summary type="html">Latex基础笔记</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="科研基础" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E7%A7%91%E7%A0%94%E5%9F%BA%E7%A1%80/"/>
    
    <category term="Latex" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E7%A7%91%E7%A0%94%E5%9F%BA%E7%A1%80/Latex/"/>
    
    
    <category term="Latex" scheme="https://www.mahaofei.com/tags/Latex/"/>
    
  </entry>
  
  <entry>
    <title>【模仿抓取】DemoGrasp从人手演示学习二指抓取姿态</title>
    <link href="https://www.mahaofei.com/post/b321d8dc.html"/>
    <id>https://www.mahaofei.com/post/b321d8dc.html</id>
    <published>2023-08-13T02:52:54.000Z</published>
    <updated>2023-08-13T02:52:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-DemoGrasp-Few-Shot-Learning-for-Robotic-Grasping-with-Human-Demonstration"><a href="#1-DemoGrasp-Few-Shot-Learning-for-Robotic-Grasping-with-Human-Demonstration" class="headerlink" title="1 DemoGrasp: Few-Shot Learning for Robotic Grasping with Human Demonstration"></a>1 DemoGrasp: Few-Shot Learning for Robotic Grasping with Human Demonstration</h1><blockquote><p><strong>标题</strong>：DemoGrasp: 机器人抓握的少镜头学习与人体演示<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：</p></blockquote><h2 id="1-1-目标问题"><a href="#1-1-目标问题" class="headerlink" title="1.1 目标问题"></a>1.1 目标问题</h2><h3 id="1-1-1-现存问题"><a href="#1-1-1-现存问题" class="headerlink" title="1.1.1 现存问题"></a>1.1.1 现存问题</h3><p>现有的位姿估计方法要么需要计算目标物体的6D位姿，要么需要学习一组抓取点。前者的方法不能很好的扩展到多个对象实例或类，后者需要大型注释数据集，并且由于其对新几何图形的泛化能力交叉而受到阻碍。</p><h3 id="1-1-2-解决思路"><a href="#1-1-2-解决思路" class="headerlink" title="1.1.2 解决思路"></a>1.1.2 解决思路</h3><p>通过简单简短的人类演示教机器人如何抓取物体，不需要许多带注释的图像，也不局限于特定的几何形状。</p><h3 id="1-1-3-大致方法"><a href="#1-1-3-大致方法" class="headerlink" title="1.1.3 大致方法"></a>1.1.3 大致方法</h3><p>首先构建一个人机交互的RGB-D图像序列。利用该序列来构建表示交互的手和对象网格。完成重建对象形状的缺失部分，并估计重建模型与场景中可见对象之间的相对变换。最后将物体和人手之间的相对姿态的先验知识以及对场景中当前物体姿态的估计转化为机器人必要的抓取指令。</p><h3 id="1-1-4-引言总结"><a href="#1-1-4-引言总结" class="headerlink" title="1.1.4 引言总结"></a>1.1.4 引言总结</h3><p><strong>为什么要做这个研究：</strong><br>当前的机器人抓取缺乏泛化能力，因为它们要么专注于估计物体姿态，要么学习抓取点，这需要物体的详细先验信息或大量注释。就像人手一样，机器人的抓取器和手臂的运动范围也有自然的限制，自由度也有限，这限制了它们可能的抓取姿势。虽然机器人抓取器和人手的运动模型可能有很大差异，但应该可以从人类操作中提取信息，并从中推断出目标机器人的足够抓取命令。通过有限的人类演示，机器人可以模仿人类行为，从而无缝抓取物体。</p><p><strong>本文主要做了什么：</strong><br>我们专注于这种模仿，机器人反映了人类的互动，如图1。该任务可以分为视觉感知和解释部分，其中人类教员演示先验操纵（Demo），机器人从中推断出操纵当前场景所需的抓握信息（抓握）。如果从人手到机器人抓取器有足够的映射，将任务分解为这两个阶段可以使我们的方法扩展到大量不同的抓取器。最终，这为通过自然人类演示来教授机器人铺平了道路，从而实现更高水平的自动化，尤其是在结构较少的环境中。</p><p><strong>本文大致是如何实现的：</strong><br>在从各种不同的角度向机器人演示物体（Demo）的过程中，我们的方法不断跟踪手和物体，这些手和物体被融合到截断有符号距离场（TSDF）中，用于3D重建。使用手和对象的语义分割，可以分离并进一步处理重建，以检索对象和手的完整3D表示。然后，我们利用MANO手部模型提取相关的3D手部网格，并将其与重建对象紧密对齐。在推理过程中，我们使用PPF FoldNet来预测对象是否存在，以及它从对象到相机空间的相对变换。然后，应用所估计的姿势从所估计的手网格导出最终抓握指令。</p><h2 id="1-2-方法"><a href="#1-2-方法" class="headerlink" title="1.2 方法"></a>1.2 方法</h2><p><strong>总体流程：</strong></p><ol><li>在一组人类演示RGB-D图像上分割手和物体，并使用记录的深度图重建它们的形状</li><li>补全物体形状</li><li>提取手部姿态</li><li>估计对象的6D姿态，转换手部模型，推理抓取指令</li></ol><p><img src="https://img.mahaofei.com/img/202308131124867.png" alt="image.png"></p><h3 id="1-2-1-人-物交互的三维重建"><a href="#1-2-1-人-物交互的三维重建" class="headerlink" title="1.2.1 人-物交互的三维重建"></a>1.2.1 人-物交互的三维重建</h3><p>使用MaskRCNN对手和物体进行分割，并且应用了二进制交叉熵来防止类间竞争。</p><p>利用分割后的深度图像，通过KinectFusion创建相应的TSDF体素，并通过输入帧与TSDF之间的ICP配准实现无漂移跟踪。<br>（因为家用物体几何形状简单，因此同时跟踪手和物体，手的结构复杂稳定了跟踪结果）</p><p>利用分割结果，通过两个单独的TSDF重建将手和物体分离开。</p><h3 id="1-2-2-物体形状补全"><a href="#1-2-2-物体形状补全" class="headerlink" title="1.2.2 物体形状补全"></a>1.2.2 物体形状补全</h3><p>由于自遮挡和部分可见性，重建的模型还不完整。</p><p>使用3D CNN直接矫正TSDF体积，然后通过行进立方体进行形状提取。<br>（这里使用了UNet的3D变体，输入是64x64x64的体素，输出每个体素的预测分数表示体素是否被占用。</p><h3 id="1-2-3-手部姿态估计"><a href="#1-2-3-手部姿态估计" class="headerlink" title="1.2.3 手部姿态估计"></a>1.2.3 手部姿态估计</h3><p>从重建的手形状中估计手部参数模型。</p><p>使用MANO手部模型，将手部姿态和形状参数映射到网格中。由于手部也受到了部分遮挡，因此使用辅助接触和碰撞损失联合训练CNN进行手部网格和物体网格估计。</p><p>为了进一步改进抓握位置，使用ICP将手部网格与手部TSDF体素对齐。</p><h3 id="1-2-4-抓取指令生成"><a href="#1-2-4-抓取指令生成" class="headerlink" title="1.2.4 抓取指令生成"></a>1.2.4 抓取指令生成</h3><p>首先检索物体姿态，然后用它来变换手部网格，并用手部模型的拇指和食指计算抓握点。</p><h2 id="1-3-思考"><a href="#1-3-思考" class="headerlink" title="1.3 思考"></a>1.3 思考</h2><ol><li>物体的三维重建可以采用其他方式，或者结合CAD模型补全的方式，相比于使用3D CNN预测效果会更好。</li><li>手部姿态的提取也可以考虑采用更新的算法，例如识别手部关键点，而不是预测手部网格的方式。</li><li>抓取姿态生成是直接使用拇指食指作为二指抓取姿态，是否可以考虑其他方式，提高抓取的可靠性。</li></ol><h1 id="2-Learning-to-Grasp-Familiar-Objects-Based-on-Experience-and-Objects’-Shape-Affordance"><a href="#2-Learning-to-Grasp-Familiar-Objects-Based-on-Experience-and-Objects’-Shape-Affordance" class="headerlink" title="2 Learning to Grasp Familiar Objects Based on Experience and Objects’ Shape Affordance"></a>2 Learning to Grasp Familiar Objects Based on Experience and Objects’ Shape Affordance</h1><blockquote><p><strong>标题</strong>：基于经验和物体形状的相似目标抓取<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS<br><strong>时间</strong>：2019<br><strong>代码</strong>：</p></blockquote><h2 id="2-1-目标问题"><a href="#2-1-目标问题" class="headerlink" title="2.1 目标问题"></a>2.1 目标问题</h2><h3 id="2-1-1-现存问题"><a href="#2-1-1-现存问题" class="headerlink" title="2.1.1 现存问题"></a>2.1.1 现存问题</h3><p>对于已知物体的抓取方法，物体具有抓取数据库，机器人通过估计物体姿态，然后利用国旅行假设找到合适抓取姿态，但是这些方法的缺点是不可能将所有对象的模型都放入机器人的数据库。</p><p>需要一种从以前的经验推广到新对象的模型的能力。</p><h3 id="2-1-2-解决思路"><a href="#2-1-2-解决思路" class="headerlink" title="2.1.2 解决思路"></a>2.1.2 解决思路</h3><p>整合人类抓握经验中的关键线索（拇指指尖和手腕的位置方向），提出了一种有效的抓握方法。</p><h2 id="2-2-方法"><a href="#2-2-方法" class="headerlink" title="2.2 方法"></a>2.2 方法</h2><h3 id="2-2-1-从不完整点云上生成抓取点"><a href="#2-2-1-从不完整点云上生成抓取点" class="headerlink" title="2.2.1 从不完整点云上生成抓取点"></a>2.2.1 从不完整点云上生成抓取点</h3><p>在抓取时，熟悉对象上的抓取点在对象上具有相似的相对位置。</p><p>基于这个原理，使用3D SHOT形状描述符描述物体，能够精确的描述兴趣点相对于整个对象和表面的位置。具体学习抓取点的过程如下：</p><ol><li>收集从部分点云中选择的兴趣点的SHOT特征、LR特征、RGB特征</li><li>通过计算简单的统计数据，如范围、均值、标准差、熵等，降低沿点维度的特征维度</li><li>将特征输入到用于对象分类的极限学习机中。</li></ol><h3 id="2-2-2-构建抓取模型"><a href="#2-2-2-构建抓取模型" class="headerlink" title="2.2.2 构建抓取模型"></a>2.2.2 构建抓取模型</h3><p>没看懂。</p><p>大概是建立大拇指和物体之间的坐标变换关系，然后将其转换为三指夹爪与物体之间的坐标变换。</p><h3 id="2-2-3-腕关节约束估计"><a href="#2-2-3-腕关节约束估计" class="headerlink" title="2.2.3 腕关节约束估计"></a>2.2.3 腕关节约束估计</h3><p>主要是解决受外在单一视角下点云被遮挡，无法精确确定手腕方向的问题。</p><h1 id="3-R3M-A-Universal-Visual-Representation-for-Robot-Manipulation"><a href="#3-R3M-A-Universal-Visual-Representation-for-Robot-Manipulation" class="headerlink" title="3 R3M: A Universal Visual Representation for Robot Manipulation"></a>3 R3M: A Universal Visual Representation for Robot Manipulation</h1><blockquote><p><strong>标题</strong>：R3M:机器人操纵的通用视觉表示<br><strong>作者团队</strong>：斯坦福大学，Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://tinyurl.com/robotr3m">https://tinyurl.com/robotr3m</a></p></blockquote><h2 id="3-1-目标问题"><a href="#3-1-目标问题" class="headerlink" title="3.1 目标问题"></a>3.1 目标问题</h2><p>训练机器人根据图像完成操作任务。<strong>给定一段文字，例如“将铲子放到锅里”，机器人根据视觉执行相应的动作</strong>。</p><p><strong>（1）传统方法的局限性</strong></p><p>传统且广泛使用的方法是使用同构数据从头开始训练端到端的模型，但是由于训练数据难以获取，限制了这种方法的泛化。而我们还有没合适的机器人数据集，最近的数据集都是由少数不同环境有限任务组成，因此泛用性受到限制。</p><p><strong>（2）本文的突破思想</strong></p><p>参考<code>ImageNet</code>等通用有效的模型，机器人领域目前还没有类似的模型出现，但是思想可以借鉴，就是使用丰富的<code>in-the-wild data</code>（野生数据？），也就是使用人类与环境交互的视频，这些数据庞大且多样化，包含全球各种场景与任务。</p><p><strong>（3）本文方法简述</strong></p><p>训练了一种机器人操纵表示方法R3M。R3M能够学习具有挑战性的任务，例如将菜放入锅中，折叠毛巾等。</p><h2 id="3-2-方法"><a href="#3-2-方法" class="headerlink" title="3.2 方法"></a>3.2 方法</h2><p>本文认为，机器人操作的良好表现由以下三个方面组成</p><ul><li>机器人应该捕获时间动态，因为机器人在环境中要按时间顺序完成任务</li><li>机器人应该捕获于一相关的特征</li><li>机器人应该是紧凑的</li></ul><p><strong>（1）时间对比学习</strong></p><p>训练编码器生成一个表示，是的时间上较近的图像之间的距离小于时间上较远的图像或来自不同视频的图像。</p><p><strong>（2）视频语言对齐</strong></p><p>捕获语言的特征，学习视频场景中的语义部分。</p><p><strong>（3）正则化</strong></p><p>降低状态空间的维度来保证克隆训练的策略符合专家状态分布。</p><h2 id="3-3-思考"><a href="#3-3-思考" class="headerlink" title="3.3 思考"></a>3.3 思考</h2><p>与本人方向有差别，本文更偏向于语义，视觉只是作为一个感知手段。</p><h1 id="4-Adversarial-Skill-Networks-Unsupervised-Robot-Skill-Learning-from-Video"><a href="#4-Adversarial-Skill-Networks-Unsupervised-Robot-Skill-Learning-from-Video" class="headerlink" title="4 Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video"></a>4 Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video</h1><blockquote><p><strong>标题</strong>：对抗性技能网络：来自视频的无监督机器人技能学习<br><strong>作者团队</strong>：德国弗赖堡大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="http://robotskills.cs.uni-freiburg.de/">http://robotskills.cs.uni-freiburg.de/</a></p></blockquote><h2 id="4-1-目标任务"><a href="#4-1-目标任务" class="headerlink" title="4.1 目标任务"></a>4.1 目标任务</h2><p>从未标记的多视角视频中学习机器人操作任务。</p><p><strong>（1）传统方法的局限性</strong></p><p>现有的强化学习方法尽管有一些进展，但是这些方法都是学习每项任务的解决方案，并且依赖于手动的、面向任务设置的奖励函数，所获得的策略也是针对于特定任务的，无法转移到新任务上。</p><p><strong>（2）本文的创新点</strong></p><p>提出一种无监督的技能学习方法，称为对抗性技能网络ASN，通过观看视频来发现和学习可转移的技能。学习到的技能被用于RL，以便通过组合以前的技能来解决更广泛的任务。</p><p>该方法不需要帧和任务ID的对应关系，不需要任何额外的监督。</p><h2 id="4-2-方法"><a href="#4-2-方法" class="headerlink" title="4.2 方法"></a>4.2 方法</h2><p><strong>Adversarial Skill Networks对抗性技能网络</strong></p><p>我们在对抗性框架中学习技能度量空间。网络的编码部分试图最大化熵以增强通用性。鉴别器在测试时不使用，它试图最小化其预测的熵，以提高对技能的识别。最后，最大化所有技能的边际类熵会导致所有任务类的统一使用。请注意，不需要关于框架和它们所源自的任务之间关系的信息。<br>（没看懂）</p><h2 id="4-3-思考"><a href="#4-3-思考" class="headerlink" title="4.3 思考"></a>4.3 思考</h2><p>似乎可以从无标签的视频中学习任务。但是过于理论化。</p><h1 id="5-BC-Z-Zero-Shot-Task-Generalization-with-Robotic-Imitation-Learning"><a href="#5-BC-Z-Zero-Shot-Task-Generalization-with-Robotic-Imitation-Learning" class="headerlink" title="5 BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"></a>5 BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning</h1><blockquote><p><strong>标题</strong>：BC-Z:利用机器人模仿学习实现零样本任务泛化<br><strong>作者团队</strong>：谷歌、加州大学伯克利分校、斯坦福<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://sites.google.com/view/bc-z/home">https://sites.google.com/view/bc-z/home</a></p></blockquote><h2 id="5-1-目标问题"><a href="#5-1-目标问题" class="headerlink" title="5.1 目标问题"></a>5.1 目标问题</h2><p>使基于视觉的机器人操作系统能推广到新任务。</p><p>为此，开发了一个交互式模仿学习系统，可以传达人物的不同信息作为条件，包括自然语言或者人类演示视频，该系统可以从演示中进行学习。并且发现学习到100个任务之后，可以执行24个未训练的任务且不需要演示。</p><p><strong>（1）现存问题</strong></p><p>机器人技术的一大挑战就是创造一种能够在非结构化环境中基于任意的用户命令执行大量任务。这一工作的关键挑战是泛化。机器人必须要能处理新的环境，识别和操纵以前从未见过的物体，并且理解从未被要求执行过的命令的意图。</p><p>传统的方法是在像素级进行端到端的学习，然后由足够的真实世界的数据，这些方法原则上能够使机器人在新的任务、对象、场景中进行泛化。但实际上这一目标还是遥不可及。</p><p>本文要解决的问题就是通过零样本或者少样本推广基于视觉的机器人操纵任务的问题。</p><p><img src="https://img.mahaofei.com/img/202308151409631.png" alt="image.png"></p><h2 id="5-2-数据收集"><a href="#5-2-数据收集" class="headerlink" title="5.2 数据收集"></a>5.2 数据收集</h2><p>为100个预先指定的任务手机了人类演示的视频，这些视频包含了推物体、拿取放置物体等9项基本任务。</p><p>搭建一套远程操作系统，远程操作设备通过USB连接到机器人上，通过两个手持控制器遥控操作站在机器人后面，使用控制器以第三人称视角操作机器人，机器人实时响应跟随操作员演示各种任务。</p><h2 id="5-3-方法"><a href="#5-3-方法" class="headerlink" title="5.3 方法"></a>5.3 方法</h2><h3 id="5-3-1-语言和视频编码"><a href="#5-3-1-语言和视频编码" class="headerlink" title="5.3.1 语言和视频编码"></a>5.3.1 语言和视频编码</h3><p>编码器以语言命令或人类视频作为输入，并生成任务。</p><ul><li>如果是语言命令，使用预训练的多语言语句编码器为每个任务生成512维语言向量</li><li>如果是视频，使用基于ResNet18的卷积网络</li></ul><h3 id="5-3-2-训练策略"><a href="#5-3-2-训练策略" class="headerlink" title="5.3.2 训练策略"></a>5.3.2 训练策略</h3><p>给定固定的任务，我们通过XYZ和轴角预测的Huber损失和抓取器角度的对数损失来训练。</p><p>开环辅助检测，如果以开环的方式运行，将采取是个行动的开环轨迹。开环预测提供了一个辅助训练目标，并可以离线检查闭环规划质量。</p><p>将状态差异作为操作，标准的模仿学习会将演示动作直接作为目标标签，而本文的专家克隆行为会导致一些小动作或抖动，因此考虑将动作定义为未来目标和下一步的差异，使用自适应算法确定手臂和夹爪的移动量。</p><h3 id="5-3-3-网络架构"><a href="#5-3-3-网络架构" class="headerlink" title="5.3.3 网络架构"></a>5.3.3 网络架构</h3><p>使用ResNet18作为主干，从主干最后一个平均池化层分出多个head，每个head是一个多层感知机，对末端执行器动作的一部分进行建模，具体见原文。</p><h2 id="5-3-思考"><a href="#5-3-思考" class="headerlink" title="5.3 思考"></a>5.3 思考</h2><p>首先提供演示视频和文字，然后手动控制机器人执行任务收集数据。似乎仍然较为繁琐。</p><h1 id="VIP-Towards-Universal-Visual-Reward-and-Representation-via-Value-Implicit-Pre-Training"><a href="#VIP-Towards-Universal-Visual-Reward-and-Representation-via-Value-Implicit-Pre-Training" class="headerlink" title="VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"></a>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training</h1><blockquote><p><strong>标题</strong>：VIP：通过价值内隐预训练实现普遍的视觉奖励和表现<br><strong>作者团队</strong>：Meta AI，宾夕法尼亚大学<br><strong>期刊会议</strong>：ICLR<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://sites.google.com/view/vip-rl">https://sites.google.com/view/vip-rl</a></p></blockquote><h2 id="6-1-目标问题"><a href="#6-1-目标问题" class="headerlink" title="6.1 目标问题"></a>6.1 目标问题</h2><p>特定任务的机器人数据的成本较高且稀缺。从大型、多样化的离线人类视频中学习已经成为获得普遍有效的途径。然而如何将这些人类视频用于通用的奖励学习仍然是一个未解决的问题。</p><h2 id="6-2-方法"><a href="#6-2-方法" class="headerlink" title="6.2 方法"></a>6.2 方法</h2><p><strong>（1）从人类视频中自我监督的价值学习</strong></p><p>人类视频包含目标导向的行为，利用人类视频进行表示学习的一个合理方法是在人类策略的空间上计算离线目标条件问题，然后提取到视觉表示。</p><p><strong>（2）隐含的时间对比学习</strong></p><p>当有意义的指示任务的开始和结束的两个帧在嵌入空间中接近时，初始帧和目标帧之间能够捕获长程语义时间依赖性。</p><p><strong>（3）基于隐含价值的预训练</strong></p><p>具体算法见原文。</p><h2 id="6-3-思考"><a href="#6-3-思考" class="headerlink" title="6.3 思考"></a>6.3 思考</h2><p>类似R3M</p><h1 id="7-Graph-Structured-Visual-Imitation"><a href="#7-Graph-Structured-Visual-Imitation" class="headerlink" title="7 Graph-Structured Visual Imitation"></a>7 Graph-Structured Visual Imitation</h1><blockquote><p><strong>标题</strong>：图形结构的视觉模仿<br><strong>作者团队</strong>：索尼<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2019<br><strong>代码</strong>：无</p></blockquote><h2 id="7-1-目标问题"><a href="#7-1-目标问题" class="headerlink" title="7.1 目标问题"></a>7.1 目标问题</h2><p>当机器人动作使工作空间中检测到的相应视觉实体的相对空间配置与演示更好的匹配时，会得到奖励。</p><p>本文使用人类手指关键点检测器、使用合成增强进行离线训练的对象检测器、由视点变化监督的点检测器。在没有人类注释数据或机器人交互的情况下为每次演示学习多个视觉实体检测器。</p><h2 id="7-2-方法"><a href="#7-2-方法" class="headerlink" title="7.2 方法"></a>7.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308151609226.png" alt="image.png"></p><p><strong>（1）检测视觉实体</strong></p><p>人手关键点检测：使用现有的手部检测器，并使用D435i获取3D位置。将机器人平行钳口夹持器映射到演示者的拇指和食指指尖。通过在两个指尖设置距离阈值来检测抓取和释放动作。</p><p>点特征检测器：训练后，在模仿者和演示者的环境中匹配点特征，建立对应关系。</p><p>合成数据扩充：使用背景移除来提取出2D掩模，并使用合成数据增强来训练视觉检测器。</p><p><strong>（2）动态图构造的运动显著性</strong></p><p><strong>（3）基于可视化实体图的策略学习</strong></p><p>目标是当机器人从单个人类演示中模仿物体操纵任务。具体的成本代价函数参考原文。</p><h2 id="7-3-思考"><a href="#7-3-思考" class="headerlink" title="7.3 思考"></a>7.3 思考</h2><p>提取手部关键点映射到机器人夹爪，同时使用物体关键点检测来实现运动策略的生成。思路上不如DemoGrasp更直观。</p><h1 id="8-Learning-by-Watching-Physical-Imitation-of-Manipulation-Skills-from-Human-Videos"><a href="#8-Learning-by-Watching-Physical-Imitation-of-Manipulation-Skills-from-Human-Videos" class="headerlink" title="8 Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos"></a>8 Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</h1><blockquote><p><strong>标题</strong>：通过观看学习：人体视频中操纵技能的物理模拟<br><strong>作者团队</strong>：多伦多大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="http://www.pair.toronto.edu/lbw-kp/">http://www.pair.toronto.edu/lbw-kp/</a></p></blockquote><h2 id="8-1-目标问题"><a href="#8-1-目标问题" class="headerlink" title="8.1 目标问题"></a>8.1 目标问题</h2><p>通过观看学习，通过模仿指定任务的单个视频来进行策略学习的算法框架。</p><ul><li>由于人类手臂与机器人手臂形态不同，我们的框架学习无监督的人-机器人的翻译来克服形态不匹配问题。</li><li>为了捕捉对学习状态至关重要的显著区域的细节，我们的模型采取了无监督关键点检测。检测到的关键点形成包含语义上有意义的信息的结构化表示，并可以直接用于计算奖励和策略学习。</li></ul><h2 id="8-2-方法"><a href="#8-2-方法" class="headerlink" title="8.2 方法"></a>8.2 方法</h2><p>本文所提出的LbW框架由三个部分组成</p><ul><li>图像到图像的翻译网络：逐帧翻译输入的人类演示视频，生成机器人演示视频</li><li>关键点检测器：将生成的机器人演示视频作为输入，提取每帧的关键点，形成关键点轨迹</li><li>策略网络：将当前的基于关键点的观察表示传递给策略网络，用于预测与环境交互的动作</li></ul><p><img src="https://img.mahaofei.com/img/202308151841683.png" alt="image.png"></p><h2 id="8-3-思考"><a href="#8-3-思考" class="headerlink" title="8.3 思考"></a>8.3 思考</h2><p>与其说是模仿学习网络，不如说是一个图像翻译网络，基于CycleGAN的图像翻译，将人手演示翻译成机器人动作视频，然后提取视频中机器人的关键点轨迹，通过策略函数实现实物机器人的动作。</p><h1 id="9-Learning-Periodic-Tasks-from-Human-Demonstrations"><a href="#9-Learning-Periodic-Tasks-from-Human-Demonstrations" class="headerlink" title="9 Learning Periodic Tasks from Human Demonstrations"></a>9 Learning Periodic Tasks from Human Demonstrations</h1><blockquote><p><strong>标题</strong>：从人类演示中学习周期性任务<br><strong>作者团队</strong>：卡内基梅隆大学<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="9-1-目标问题"><a href="#9-1-目标问题" class="headerlink" title="9.1 目标问题"></a>9.1 目标问题</h2><p>使用主动学习来优化参数，提出了一个目标最大限度的提高机器人操纵物体的运动与演示视频中物体运动之间的相似性。重点在于可变形物体和颗粒物体。（用布擦拭表面，缠绕电缆，用勺子搅拌颗粒物质等）</p><h2 id="9-2-方法"><a href="#9-2-方法" class="headerlink" title="9.2 方法"></a>9.2 方法</h2><p>本文提出的框架由两部分组成</p><ul><li>表示学习模块：关键点检测模型从独立收集的非特定任务的人类和机器人数据中提取一致的关键点</li><li>姿态优化模块：将产生在检测的关键点方面与人类演示相匹配的机器人视频</li></ul><h2 id="9-3-思考"><a href="#9-3-思考" class="headerlink" title="9.3 思考"></a>9.3 思考</h2><p>给定人类演示动作和手动操控机器人演示动作，机器人学习两者的相似性，然后重复演示动作使其更接近人类演示效果。</p><h1 id="10-One-Shot-Hierarchical-Imitation-Learning-of-Compound-Visuomotor-Tasks"><a href="#10-One-Shot-Hierarchical-Imitation-Learning-of-Compound-Visuomotor-Tasks" class="headerlink" title="10 One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks"></a>10 One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks</h1><blockquote><p><strong>标题</strong>：复合视觉运动任务的一次性层次模拟学习<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<a href="https://sites.google.com/view/one-shot-hil">https://sites.google.com/view/one-shot-hil</a></p></blockquote><h2 id="10-1-目标问题"><a href="#10-1-目标问题" class="headerlink" title="10.1 目标问题"></a>10.1 目标问题</h2><p>真实机器人上从人类执行任务的视频中学习多阶段任务。</p><h2 id="10-2-方法"><a href="#10-2-方法" class="headerlink" title="10.2 方法"></a>10.2 方法</h2><p>对于每个子任务，我们提供多个人类演示和多个机器人演示（需要对象和执行的任务对应，但是不用相同的对象位置、执行速度）</p><p><strong>（1）基元的合成</strong>：训练了一个人类相位预测器和机器人相位预测器，从人类执行视频中学习特定的机器人策略</p><p><strong>（2）原始相位预测</strong>：学习如何分割复合任务的人类演示；何时学习策略过度到下一个。</p><h2 id="10-3-思考"><a href="#10-3-思考" class="headerlink" title="10.3 思考"></a>10.3 思考</h2><p>提供人的演示视频，机器人的演示视频，然后训练策略。最后利用训练的策略，提供一段人类演示视频，机器人执行对应的操作。</p><h1 id="11-Third-Person-Visual-Imitation-Learning-via-Decoupled-Hierarchical-Controller"><a href="#11-Third-Person-Visual-Imitation-Learning-via-Decoupled-Hierarchical-Controller" class="headerlink" title="11 Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller"></a>11 Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller</h1><blockquote><p><strong>标题</strong>：基于解耦层次控制器的第三人称视觉模仿学习<br><strong>作者团队</strong>：MIT<br><strong>期刊会议</strong>：NeurIPS<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="https://pathak22.github.io/hierarchical-imitation/">https://pathak22.github.io/hierarchical-imitation/</a></p></blockquote><h2 id="11-1-目标问题"><a href="#11-1-目标问题" class="headerlink" title="11.1 目标问题"></a>11.1 目标问题</h2><p>通过从第三人称视角观看人类演示视频，可以在未知场景中操纵新物体。</p><h2 id="11-2-方法"><a href="#11-2-方法" class="headerlink" title="11.2 方法"></a>11.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308152040902.png" alt="image.png"></p><p><strong>（1）目标生成器</strong></p><p>从人类演示视频中推断像素空间中的目标，并以像素级的表示形式将其转化为机器人环境中的目标。</p><p>也是使用图像翻译的方法，将人类演示图像翻译为机器人演示图象。</p><p><strong>（2）反向控制器</strong></p><p>跟踪视觉目标推理模型中生成的线索，并生成机器人要执行的动作。</p><p>使用ResNet18模型。</p><p><strong>（3）第三人称模仿</strong></p><p>以交替方式运行目标生成器和反向控制器。目标生成器生成子目标，低级控制器生成机器人关节角度，直到人类演示结束。</p><h2 id="11-3-思考"><a href="#11-3-思考" class="headerlink" title="11.3 思考"></a>11.3 思考</h2><p>还是使用图像翻译的思路，把人手操作图像翻译成机械臂操作图像，再由控制器生成机器人关节角度。</p><h1 id="12-You-Only-Demonstrate-Once-Category-Level-Manipulation-from-Single-Visual-Demonstration"><a href="#12-You-Only-Demonstrate-Once-Category-Level-Manipulation-from-Single-Visual-Demonstration" class="headerlink" title="12 You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration"></a>12 You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration</h1><blockquote><p><strong>标题</strong>：Yodo：单一视觉演示的类别级操作<br><strong>作者团队</strong>：罗格斯大学<br><strong>期刊会议</strong>：RSS<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="12-1-目标问题"><a href="#12-1-目标问题" class="headerlink" title="12.1 目标问题"></a>12.1 目标问题</h2><p>由于最近的跨对象类别级操作虽然有很好的结果，但通常需要昂贵的真实数据收集和为每个对象类别和任务手动指定语义关键点。并且粗略的关键点预测和忽略中间动作序列阻止了在抓取和防止之外的复杂任务的应用。</p><p>本工作提出了一种新的操作框架。该框架利用了无模型6D跟踪技术，解析单个演示视频中的类别级任务轨迹，整个执行过程被分解为远程、无碰撞运动和最后一英寸操作三个步骤。</p><h2 id="12-2-方法"><a href="#12-2-方法" class="headerlink" title="12.2 方法"></a>12.2 方法</h2><p>对于每个演示视频帧，通过无模型6D位姿估计跟踪目标位姿，对象位姿在容器的坐标系中表示，这样允许泛化到新的场景。</p><p><strong>（1）类别级表示的离线学习</strong></p><p>建立了一个9D物体表示方法，6D位姿+3D缩放</p><p><strong>（2）无模型的物体6D跟踪</strong></p><p>物体运动跟踪要实现两个目的</p><ul><li>演示阶段，解析录制的视频，提取容器坐标系中被操纵的对象的6D运动轨迹</li><li>在线执行期间，为闭环控制器提供视觉反馈</li></ul><p><strong>（3）类别级行为克隆作为最后一步策略</strong></p><p>产生密集的离散轨迹，以便机器人能沿轨迹到达下一个目标</p><p><strong>（4）基于局部注意的动态类别级框架</strong></p><p>自动动态地规范坐标系原点。</p><p><strong>（5）抓取物体并使其沿关键点移动</strong></p><p>常规的抓取方法</p><h2 id="12-3-思考"><a href="#12-3-思考" class="headerlink" title="12.3 思考"></a>12.3 思考</h2><p>将目标位姿表示为相对于另一个物体的相对位姿，这样有助于场景的泛化。</p><p>整体思想就是使用6D位姿估计获得目标的运动轨迹，然后重复这条轨迹。</p>]]></content>
    
    
    <summary type="html">基于模仿学习的二指机械臂抓取方法论文笔记</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>ROS系统Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/4add66b0.html"/>
    <id>https://www.mahaofei.com/post/4add66b0.html</id>
    <published>2023-05-15T09:05:05.000Z</published>
    <updated>2023-05-15T09:05:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、安装问题"><a href="#一、安装问题" class="headerlink" title="一、安装问题"></a>一、安装问题</h1><h2 id="ROS安装时rosdep-init与rosdep-update问题解决方法"><a href="#ROS安装时rosdep-init与rosdep-update问题解决方法" class="headerlink" title="ROS安装时rosdep_init与rosdep_update问题解决方法"></a>ROS安装时rosdep_init与rosdep_update问题解决方法</h2><p><strong>解决方法</strong></p><p>使用下面的命令替代上面两行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3-pip</span><br><span class="line">sudo pip3 install rosdepc</span><br><span class="line">sudo rosdepc init</span><br><span class="line">rosdepc update</span><br></pre></td></tr></table></figure><h1 id="二、环境问题"><a href="#二、环境问题" class="headerlink" title="二、环境问题"></a>二、环境问题</h1><h2 id="Unable-to-find-either-executable-‘empy’-or-Python-module-‘em’…-try-installing-the-package-‘python3-empy’"><a href="#Unable-to-find-either-executable-‘empy’-or-Python-module-‘em’…-try-installing-the-package-‘python3-empy’" class="headerlink" title="Unable to find either executable ‘empy’ or Python module ‘em’…  try  installing the package ‘python3-empy’"></a>Unable to find either executable ‘empy’ or Python module ‘em’…  try  installing the package ‘python3-empy’</h2><p><strong>（1）问题原因</strong></p><p>Anaconda使用的是Python3版本，但是ROS使用的Python2</p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="Could-not-find-a-package-configuration-file-provided-by-“某某包”-with-any-of-the-following-names"><a href="#Could-not-find-a-package-configuration-file-provided-by-“某某包”-with-any-of-the-following-names" class="headerlink" title="Could not find a package configuration file provided by “某某包” with any of  the following names"></a>Could not find a package configuration file provided by “某某包” with any of  the following names</h2><p><strong>（1）问题原因</strong></p><p>缺少<code>某某包</code></p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ros-noetic-某某包</span><br></pre></td></tr></table></figure><h1 id="三、配置问题"><a href="#三、配置问题" class="headerlink" title="三、配置问题"></a>三、配置问题</h1><h2 id="ERROR-cannot-launch-node-of-type-robot-state-publisher-state-publisher-Cannot-locate-node-of-type-state-publisher-in-package-robot-state-publisher-Make-sure-file-exists-in-package-path-and-permission-is-set-to-executable-chmod-x）"><a href="#ERROR-cannot-launch-node-of-type-robot-state-publisher-state-publisher-Cannot-locate-node-of-type-state-publisher-in-package-robot-state-publisher-Make-sure-file-exists-in-package-path-and-permission-is-set-to-executable-chmod-x）" class="headerlink" title="ERROR: cannot launch node of type [robot_state_publisher/state_publisher]: Cannot locate node of type [state_publisher] in package [robot_state_publisher]. Make sure file exists in package path and permission is set to executable (chmod +x）"></a>ERROR: cannot launch node of type [robot_state_publisher/state_publisher]: Cannot locate node of type [state_publisher] in package [robot_state_publisher]. Make sure file exists in package path and permission is set to executable (chmod +x）</h2><p><strong>（1）问题原因</strong></p><p>使用launch文件启动某个节点时出现这个问题，是因为launch文件中name、pkg、type不统一导致的。</p><p><strong>（2）解决方法</strong></p><p>检查launch文件，确保name、pkg、type一样，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot; /&gt;</span><br></pre></td></tr></table></figure><h2 id="joint-state-publisher-gui没有显示"><a href="#joint-state-publisher-gui没有显示" class="headerlink" title="joint state publisher gui没有显示"></a>joint state publisher gui没有显示</h2><p><strong>（1）问题描述</strong></p><p>使用ROS进行仿真，想用joint state publisher进行机械臂控制，但是启动launch文件后没有报错信息，但也没有joint state publisher gui。</p><p><strong>（2）解决方法</strong></p><p>2020年开始，gui已经移出了 joint state publisher, 并且成为了一个新的package：joint state publisher gui. 之前那种使用gui参数的方式调用joint state publisher 是仍然可行的，但是不会调用gui。</p><p>在launch文件中，将joint state publisher 替换成joint<strong>state</strong>publisher_gui。</p>]]></content>
    
    
    <summary type="html">在使用ROS系统进行机器人实验中，遇到的各种错误信息汇总，不定时更新。</summary>
    
    
    
    <category term="机器人" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="ros" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/ros/"/>
    
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
  </entry>
  
  <entry>
    <title>ROS Gazebo 6D机械臂抓取仿真实验</title>
    <link href="https://www.mahaofei.com/post/7fec171b.html"/>
    <id>https://www.mahaofei.com/post/7fec171b.html</id>
    <published>2023-05-15T07:23:56.000Z</published>
    <updated>2023-05-15T07:23:56.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、基础知识"><a href="#一、基础知识" class="headerlink" title="一、基础知识"></a>一、基础知识</h1><h2 id="1-1-URDF"><a href="#1-1-URDF" class="headerlink" title="1.1 URDF"></a>1.1 URDF</h2><p>URDF是ROS中机器人模型的描述格式，包括机器人的外观、物理属性、关节类型等方面。</p><ul><li><code>&lt;robot&gt;</code>：最顶层标签</li><li><code>&lt;link&gt;</code>：描述刚提的外观形状、碰撞几何、颜色、惯性矩阵等</li><li><code>&lt;joint&gt;</code>：描述两个link之间的关系，有6种类型，最常用的是<code>revolute</code>类型，有关节位置限制的旋转关节</li></ul><p>xacro模型可以将部分URDF打包成一个”类”，在其他模型中调用。</p><p>功能包中一般包括以下四个部分</p><ol><li><code>cfg</code>：配置文件</li><li><code>launch</code>：加载urdf模型，并在rviz中展示</li><li><code>meshes</code>：urdf用到的外观模型</li><li><code>urdf</code>：urdf模型定义</li></ol><h2 id="1-2-Gazebo"><a href="#1-2-Gazebo" class="headerlink" title="1.2 Gazebo"></a>1.2 Gazebo</h2><h2 id="1-3-Moveit控制"><a href="#1-3-Moveit控制" class="headerlink" title="1.3 Moveit控制"></a>1.3 Moveit控制</h2><p><strong>（1）Moveit!大致功能</strong></p><ul><li>运动学计算</li><li>运动规划</li><li>碰撞检测</li></ul><p>最重要的节点是<code>move_group</code>，输入可以是RVIZ中的数据或点云和深度图。路径规划一般使用的OMPL库，碰撞检测使用FCL库。最后发送个机械臂让机械臂执行轨迹。</p><h2 id="1-4-机械臂运动规划"><a href="#1-4-机械臂运动规划" class="headerlink" title="1.4 机械臂运动规划"></a>1.4 机械臂运动规划</h2><h2 id="1-5-基于深度学习的视觉避障"><a href="#1-5-基于深度学习的视觉避障" class="headerlink" title="1.5 基于深度学习的视觉避障"></a>1.5 基于深度学习的视觉避障</h2><h1 id="二、实验环境搭建"><a href="#二、实验环境搭建" class="headerlink" title="二、实验环境搭建"></a>二、实验环境搭建</h1><h2 id="1-1-安装ROS"><a href="#1-1-安装ROS" class="headerlink" title="1.1 安装ROS"></a>1.1 安装ROS</h2><p>参考<a href="https://www.mahaofei.com/post/b278544f.html">Ubuntu20.04安装ROS Noetic</a>文章</p><p>![[01-Ubuntu20.04安装ROS Noetic#二、安装ROS]]</p><h2 id="1-2-安装Moveit"><a href="#1-2-安装Moveit" class="headerlink" title="1.2 安装Moveit!"></a>1.2 安装Moveit!</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ros-noetic-moveit</span><br></pre></td></tr></table></figure><h2 id="1-3-安装UR机器人及驱动"><a href="#1-3-安装UR机器人及驱动" class="headerlink" title="1.3 安装UR机器人及驱动"></a>1.3 安装UR机器人及驱动</h2><p>复制代码后，修改下面的内容，使其能在noetic版本的ros上运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/srv/SetPayload.srv</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">float32 payload</span><br><span class="line">geometry_msgs/Vector3 center_of_gravity</span><br><span class="line">-----------------------</span><br><span class="line">bool success</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/CMakeLists.txt</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8.3)</span><br><span class="line">project(ur_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Find catkin macros and libraries</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># is used, also find other catkin packages</span></span></span><br><span class="line">find_package(catkin REQUIRED COMPONENTS message_generation std_msgs geometry_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate messages in the &#x27;msg&#x27; folder</span></span></span><br><span class="line">add_message_files(</span><br><span class="line">   FILES</span><br><span class="line">   Analog.msg</span><br><span class="line">   Digital.msg</span><br><span class="line">   IOStates.msg</span><br><span class="line">   RobotStateRTMsg.msg</span><br><span class="line">   MasterboardDataMsg.msg</span><br><span class="line">   RobotModeDataMsg.msg</span><br><span class="line">   ToolDataMsg.msg</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate services in the &#x27;srv&#x27; folder</span></span></span><br><span class="line">add_service_files(</span><br><span class="line">   FILES</span><br><span class="line">   SetPayload.srv</span><br><span class="line">   SetSpeedSliderFraction.srv</span><br><span class="line">   SetIO.srv</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate added messages and services with any dependencies listed here</span></span></span><br><span class="line">generate_messages(</span><br><span class="line">   DEPENDENCIES</span><br><span class="line">   std_msgs</span><br><span class="line">   geometry_msgs</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># catkin specific configuration ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line">catkin_package(</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> INCLUDE_DIRS include</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> LIBRARIES ur_msgs</span></span><br><span class="line">   CATKIN_DEPENDS message_runtime std_msgs geometry_msgs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> DEPENDS system_lib</span></span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Build ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Install ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Testing ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/package.xml</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;package format=&quot;2&quot;&gt;</span><br><span class="line">  &lt;name&gt;ur_msgs&lt;/name&gt;</span><br><span class="line">  &lt;version&gt;1.2.5&lt;/version&gt;</span><br><span class="line">  &lt;description&gt;The ur_msgs package&lt;/description&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;author&gt;Andrew Glusiec&lt;/author&gt;</span><br><span class="line">  &lt;author&gt;Felix Messmer&lt;/author&gt;</span><br><span class="line">  &lt;maintainer email=&quot;g.a.vanderhoorn@tudelft.nl&quot;&gt;G.A. vd. Hoorn&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;miguel.prada@tecnalia.com&quot;&gt;Miguel Prada Sarasola&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;nhg@ipa.fhg.de&quot;&gt;Nadia Hammoudeh Garcia&lt;/maintainer&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;license&gt;BSD&lt;/license&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt;</span><br><span class="line">  &lt;build_depend&gt;message_generation&lt;/build_depend&gt;</span><br><span class="line">  &lt;depend&gt;std_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;depend&gt;geometry_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;</span><br><span class="line"></span><br><span class="line">  &lt;export&gt;</span><br><span class="line">  &lt;/export&gt;</span><br><span class="line">&lt;/package&gt;</span><br></pre></td></tr></table></figure><p>完成之后，就可以编译了。<code>source</code>之后使用<code>roslaunch ur5_moveit_config demo.launch</code></p><h2 id="1-4-简单测试"><a href="#1-4-简单测试" class="headerlink" title="1.4 简单测试"></a>1.4 简单测试</h2><p><strong>（1）Rviz打开UR5模型</strong></p><p>机械臂夹爪模型路径为<code>universal_robot/urdf/ur5_gripper_joint_limited_robot.urdf.xacro</code>。</p><p>可以通过<code>universal_robot/ur_description/launch/view_ur5_with_gripper.launch</code>启动，从Rviz中查看模型情况，并使用<code>joint_state_publisher_gui</code>对机械臂模型拖动控制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5_with_gripper.launch</span><br></pre></td></tr></table></figure><p><strong>（2）Rviz中Moveit测试</strong></p><p>使用下面的程序可以在 Rviz 中进行 Moveit 轨迹规划测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur5_moveit_config demo.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur5_gripper_moveit_config demo.launch</span><br></pre></td></tr></table></figure><p><strong>（3）Gazebo中Moveit测试</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5.launch</span><br><span class="line">roslaunch ur5_moveit_config ur5_moveit_planning_execution.launch sim:=true</span><br><span class="line">roslaunch ur5_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><code>ur5.launch</code>：用于启动 gazebo 仿真环境。具体包括以下几个部分，启动空环境、定义 robot_description 参数服务器、发送到gazebo中生成机器人、启动并加载控制器。</p><p><code>ur5_moveit_planning_execution.launch</code>：用于启动 MoveIt 相关组件。具体包括以下几个部分：设置 sim参数， 根据 sim 参数重映射 follow_joint_trajectory 话题，启动MoveIt。</p><p><code>moveit_rviz.launch</code>：用于启动 Rviz 相关组件。具体包括以下几个部分：加载配置参数，启动Rviz。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5_with_gripper.launch</span><br><span class="line">roslaunch ur5_single_arm_moveit_config ur5_moveit_planning_execution.launch</span><br><span class="line">roslaunch ur5_gripper_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202308041008966.png" alt=""></p><h2 id="1-5-导入自定义物体"><a href="#1-5-导入自定义物体" class="headerlink" title="1.5 导入自定义物体"></a>1.5 导入自定义物体</h2><p><strong>（1）网络方法</strong></p><p>使用 MeshLab 加载自己的物体模型。</p><p>点击【Filters -&gt; Normals … -&gt; Compute normals for points sets】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Remeshing -&gt; Surface Reconstruction: Screened Poisson】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Trivial Per-Triangle】，按照如下设置，重要的是Method。</p><p><img src="https://img.mahaofei.com/img/202305242228283.png" alt="image.png"></p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】，按照如下设置，重要的是Source Mesh和Target Mesh。</p><p><img src="https://img.mahaofei.com/img/202305242231021.png" alt="image.png"></p><p><strong>（2）摸索方法</strong></p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Flat Plane】</p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】</p>]]></content>
    
    
    <summary type="html">使用Gazebo搭建6D机械臂仿真环境，并添加相机，然后实现抓取仿真实验。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E5%AE%9E%E9%AA%8C/"/>
    
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>【目标检测算法】YOLOV8代码复现</title>
    <link href="https://www.mahaofei.com/post/16b5f6b3.html"/>
    <id>https://www.mahaofei.com/post/16b5f6b3.html</id>
    <published>2023-05-06T06:13:12.000Z</published>
    <updated>2023-05-06T06:13:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、算法笔记"><a href="#一、算法笔记" class="headerlink" title="一、算法笔记"></a>一、算法笔记</h1><h1 id="二、代码复现"><a href="#二、代码复现" class="headerlink" title="二、代码复现"></a>二、代码复现</h1><h2 id="2-1-搭建环境"><a href="#2-1-搭建环境" class="headerlink" title="2.1 搭建环境"></a>2.1 搭建环境</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolov8 python=3.7</span><br><span class="line">conda activate yolov8</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>下载作者开源的程序，并安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/ultralytics</span><br><span class="line">cd ultralytics</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-命令行使用教程"><a href="#2-2-命令行使用教程" class="headerlink" title="2.2 命令行使用教程"></a>2.2 命令行使用教程</h2><p><strong>（1）语法规则</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yolo TASK MODE ARGS</span><br><span class="line"></span><br><span class="line">Where   TASK (optional) is one of [detect, segment, classify]</span><br><span class="line">        MODE (required) is one of [train, val, predict, export, track]</span><br><span class="line">        ARGS (optional) are any number of custom &#x27;arg=value&#x27; pairs like &#x27;imgsz=320&#x27; that override defaults.</span><br></pre></td></tr></table></figure><p><strong>（2）训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01</span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo predict model=yolov8n-seg.pt source=&#x27;https://youtu.be/Zgi9g1ksQHc&#x27; imgsz=320</span><br></pre></td></tr></table></figure><p><strong>（4）评价</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640</span><br></pre></td></tr></table></figure><h2 id="2-3-Python使用教程"><a href="#2-3-Python使用教程" class="headerlink" title="2.3 Python使用教程"></a>2.3 Python使用教程</h2><p><strong>（1）训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n.pt&#x27;</span>) <span class="comment"># 从预训练模型开始</span></span><br><span class="line">model.train(epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><strong>（2）评价</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line">model.val()  <span class="comment"># 使用model.pt的data yaml进行评价</span></span><br><span class="line">model.val(data=<span class="string">&#x27;coco128.yaml&#x27;</span>)  <span class="comment"># 或指定数据进行评价</span></span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><p>获取预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line"><span class="comment"># 接受所有类型 - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam</span></span><br><span class="line"><span class="comment"># 从摄像头</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;0&quot;</span>)</span><br><span class="line"><span class="comment"># 从文件夹</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>, show=<span class="literal">True</span>) <span class="comment"># Display preds. Accepts all YOLO predict arguments</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL图像</span></span><br><span class="line">im1 = Image.<span class="built_in">open</span>(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im1, save=<span class="literal">True</span>)  <span class="comment"># save plotted images</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从ndarray</span></span><br><span class="line">im2 = cv2.imread(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im2, save=<span class="literal">True</span>, save_txt=<span class="literal">True</span>)  <span class="comment"># save predictions as labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL/ndarray的列表</span></span><br><span class="line">results = model.predict(source=[im1, im2])</span><br></pre></td></tr></table></figure><p>预测结果分析（results会包含预测所有结果的列表，当有很多图像的时候要注意避免内存溢出，特别是在实例分割时）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. return as a list</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  return as a generator (stream=True)</span></span><br><span class="line">results = model.predict(source=<span class="number">0</span>, stream=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># Detection</span></span><br><span class="line">    result.boxes.xyxy   <span class="comment"># box with xyxy format, (N, 4)</span></span><br><span class="line">    result.boxes.xywh   <span class="comment"># box with xywh format, (N, 4)</span></span><br><span class="line">    result.boxes.xyxyn  <span class="comment"># box with xyxy format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.xywhn  <span class="comment"># box with xywh format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.conf   <span class="comment"># confidence score, (N, 1)</span></span><br><span class="line">    result.boxes.cls    <span class="comment"># cls, (N, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Segmentation</span></span><br><span class="line">    result.masks.data      <span class="comment"># masks, (N, H, W)</span></span><br><span class="line">    result.masks.xy        <span class="comment"># x,y segments (pixels), List[segment] * N</span></span><br><span class="line">    result.masks.xyn       <span class="comment"># x,y segments (normalized), List[segment] * N</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Classification</span></span><br><span class="line">    result.probs     <span class="comment"># cls prob, (num_class, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Each result is composed of torch.Tensor by default, </span></span><br><span class="line"><span class="comment"># in which you can easily use following functionality:</span></span><br><span class="line">result = result.cuda()</span><br><span class="line">result = result.cpu()</span><br><span class="line">result = result.to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">result = result.numpy()</span><br></pre></td></tr></table></figure><h2 id="2-3-数据集制作（实例分割）"><a href="#2-3-数据集制作（实例分割）" class="headerlink" title="2.3 数据集制作（实例分割）"></a>2.3 数据集制作（实例分割）</h2><p><strong>（1）使用Labelme创建实例分割数据集</strong></p><p>安装labelme</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme</span><br></pre></td></tr></table></figure><p>安装完成后直接在命令行输入<code>labelme</code>即可打开。</p><p>使用label进行标注，将生成的json文件和原始图像jpg，放入同一个文件夹中。</p><p><strong>（2）Labelme格式转COCO格式</strong></p><p>参考<a href="https://pypi.org/project/labelme2coco/">pypi的labelme2coco包</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme2coco</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者使用清华源</span></span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple labelme2coco</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labelme2coco path/to/labelme/dir --train_split_rate 0.85</span><br></pre></td></tr></table></figure><p><strong>（3）COCO格式转YOLO格式</strong></p><p>使用<code>labelme</code>制作实例分割的coco格式数据集，然后使用<a href="https://github.com/ultralytics/JSON2YOLO">ultralytics/JSON2YOLO</a>项目将json文件转换成yolo的训练格式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/JSON2YOLO.git</span><br><span class="line">cd JSON2YOLO</span><br></pre></td></tr></table></figure><p>创建一个虚拟环境，然后使用下面的命令安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第387行，设置为刚才得到的COCO注释的位置，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> source == <span class="string">&#x27;COCO&#x27;</span>:</span><br><span class="line">convert_coco_json(<span class="string">&#x27;datasets/20230223_Phone_4Obj_Coco/annotations&#x27;</span>,  <span class="comment"># directory with *.json</span></span><br><span class="line">  use_segments=<span class="literal">True</span>,</span><br><span class="line">  cls91to80=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第289行，因为不是coco80中的物体类型，是自己设置的，因此需要修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cls = coco80[ann[&#x27;category_id&#x27;] - 1] if cls91to80 else ann[&#x27;category_id&#x27;] - 1  # class</span></span><br><span class="line">cls = ann[<span class="string">&#x27;category_id&#x27;</span>]  <span class="comment"># class</span></span><br></pre></td></tr></table></figure><p>运行程序将COCO格式json文件转换为YOLO格式txt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python general_json2yolo.py</span><br></pre></td></tr></table></figure><p>结果保存在new_dir中，需要手动把images复制过去。最后得到的数据集如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_root</span><br><span class="line">├── images</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">└── labels</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br></pre></td></tr></table></figure><p><strong>（4）创建数据集的YAML文件</strong></p><p>打开目录<code>ultralytics/datasets</code>，复制一份其中的<code>coco128-seg.yaml</code>，重命名为<code>custom-seg.yaml</code>，然后根据自己的数据集进行修改。</p><p>例如：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span></span><br><span class="line"><span class="attr">path:</span> <span class="string">/media/mahaofei/OneTouch/Dataset/Program_data/image_processing/ultralytics/20230223_Phone_4Obj_YOLO</span>  <span class="comment"># dataset root dir</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">images/train2017</span>  <span class="comment"># train images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">images/train2017</span>  <span class="comment"># val images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">test:</span>  <span class="comment"># test images (optional)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classes</span></span><br><span class="line"><span class="attr">names:</span></span><br><span class="line">  <span class="attr">0:</span> <span class="string">ammeter</span></span><br><span class="line">  <span class="attr">1:</span> <span class="string">coffeebox</span></span><br><span class="line">  <span class="attr">2:</span> <span class="string">realsensebox</span></span><br><span class="line">  <span class="attr">3:</span> <span class="string">sucker</span></span><br></pre></td></tr></table></figure></p><h2 id="2-4-开始训练"><a href="#2-4-开始训练" class="headerlink" title="2.4 开始训练"></a>2.4 开始训练</h2><p>新建一个python文件如<code>train.py</code>，添加内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a model</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;)  # build a new model from YAML</span></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n-seg.pt&#x27;</span>)  <span class="comment"># load a pretrained model (recommended for training)</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;).load(&#x27;yolov8n.pt&#x27;)  # build from YAML and transfer weights</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.train(data=<span class="string">&#x27;custom-seg.yaml&#x27;</span>, epochs=<span class="number">100</span>, imgsz=<span class="number">3904</span>, batch=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="2-5-结果预测"><a href="#2-5-结果预测" class="headerlink" title="2.5 结果预测"></a>2.5 结果预测</h2>]]></content>
    
    
    <summary type="html">使用经典的YOLO算法进行实例分割</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【抓取姿态估计算法】RGB Matters论文笔记与复现</title>
    <link href="https://www.mahaofei.com/post/a1b0a01b.html"/>
    <id>https://www.mahaofei.com/post/a1b0a01b.html</id>
    <published>2023-05-04T00:30:31.000Z</published>
    <updated>2023-05-04T00:30:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、论文笔记"><a href="#一、论文笔记" class="headerlink" title="一、论文笔记"></a>一、论文笔记</h1><p><strong>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</strong></p><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="1-1-目标问题"><a href="#1-1-目标问题" class="headerlink" title="1.1 目标问题"></a>1.1 目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="1-2-方法"><a href="#1-2-方法" class="headerlink" title="1.2 方法"></a>1.2 方法</h2><p>大致流程为：</p><ol><li>使用Angle View Net网络生成图像不同位置的抓取器方向 $P_{img}=(u,v,r_x,r_y,r_z,c)$，即图像中坐标的位置，其对应的夹爪旋转姿态，以及置信度。</li><li>对置信度高的预测，结合深度图计算距离和夹爪宽度$P_{cam}=x,y,z,rx,ry,rz,w$</li></ol><p><strong>（0）定义</strong></p><p>抓握姿势定义为 (x, y, z, rx, ry, rz, w)，其中(x, y, z)代表夹持器的位置，(rx, ry, rz)代表夹持器的旋转，w代表夹持器的宽度。</p><p>夹持器本文仅考虑平行夹爪，使用 (h, l, wmax) 定义，三个参数分别代表夹具的 高度、长度和最大宽度。</p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒（因为同一个位置进行抓取有不止一个可行的旋转）。</p><p>可以使用下面的模型，将方向解耦为接近方向和绕平面的旋转，将夹持器旋转预测作为一个分类问题进行预测，共有VxA类方向。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>网络通过将RGB图像栅格化，对于每一个网格，AVN预测一个1维VxA个元素的向量，包含每个方向的置信度。最终得到(VxA)xGHxGW的tensor。AVN最终的输出表示为每个角度的heatmap。</p><p>作者在代码中给出的是V=60,A=6的测试。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="1-3-思考"><a href="#1-3-思考" class="headerlink" title="1.3 思考"></a>1.3 思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1 id="二、复现过程"><a href="#二、复现过程" class="headerlink" title="二、复现过程"></a>二、复现过程</h1><h2 id="2-1-环境搭建"><a href="#2-1-环境搭建" class="headerlink" title="2.1 环境搭建"></a>2.1 环境搭建</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n rgb_matters python=3.7</span><br><span class="line">conda activate rgb_matters</span><br></pre></td></tr></table></figure><p>下载程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/GouMinghao/rgb_matters</span><br><span class="line">cd rgb_matters</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-测试Demo"><a href="#2-2-测试Demo" class="headerlink" title="2.2 测试Demo"></a>2.2 测试Demo</h2><p>下载作者训练好的模型：<a href="https://drive.google.com/drive/folders/1upW4gvQk5ftXfpLHtvCogudpP4kNyoGq?usp=sharing">Google Drive</a></p><p>在代码目录创建一个<code>weights</code>的目录，然后将下载的模型放入其中，完成后文件夹结构如下</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rgbd_graspnet/</span><br><span class="line">├── check_label_integrity.py</span><br><span class="line">├── train.py</span><br><span class="line">├── train.sh</span><br><span class="line">├── vis_label.py</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">└── weights</span><br><span class="line">    ├── kn_jitter_79200.pth</span><br><span class="line">    ├── kn_no_norm_76800.pth</span><br><span class="line">    ├── kn_norm_63200.pth</span><br><span class="line">    ├── kn_norm_only_73600.pth</span><br><span class="line">    └── rs_norm_56400.pth</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230424162408.png" alt=""></p><h1 id="三、代码分析"><a href="#三、代码分析" class="headerlink" title="三、代码分析"></a>三、代码分析</h1><h2 id="3-1-输出结果分析"><a href="#3-1-输出结果分析" class="headerlink" title="3.1 输出结果分析"></a>3.1 输出结果分析</h2><p><strong>（1）热力图获取</strong></p><p>使用下面的代码进行预测热力图</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = RGBNormalNet(num_layers=args.num_layers, use_normal=args.use_normal, normal_only=args.normal_only)</span><br><span class="line">state_dict = torch.load(weights_path)</span><br><span class="line">net.load_state_dict(state_dict[&quot;net&quot;], strict=False)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net.eval()</span><br><span class="line"></span><br><span class="line">rgb, _ = load_data(rgb_path, depth_path)</span><br><span class="line"></span><br><span class="line">rgb = rgb.unsqueeze(0).to(device)</span><br><span class="line"></span><br><span class="line">prob_map = net(rgb)</span><br></pre></td></tr></table></figure><p>其中的<code>prob_mat</code>是预测的热力图<code>shape=(batch_size, 360, h, w)</code>一共360张热力图（360张包括接近方向v=60和平面内旋转A=6）</p><p><strong>（2）夹爪姿态获取</strong></p><p>使用<code>convert_grasp()</code>函数从360张热力图中提取夹爪姿态。</p><p>夹爪姿态为<code>Grasp</code>实例，包括以下几个参数：</p><ul><li><code>score</code>：float类型，抓取得分</li><li><code>width</code>：float类型，夹爪宽度</li><li><code>height</code>：float类型，夹爪高度</li><li><code>depth</code>：float类型，夹爪深度</li><li><code>rotation_matrix</code>：shape(3, 3)数组，旋转矩阵</li><li><code>translation</code>：shape(3)数组，平移向量</li><li><code>object_id</code>：int类型，抓取物体类别</li></ul><p>具体参考下面两张图：</p><p><img src="https://img.mahaofei.com/img/202305142107952.png" alt="image.png"></p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p>]]></content>
    
    
    <summary type="html">复现上交提出的RGB Matters算法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>抓取姿态估计算法调研</title>
    <link href="https://www.mahaofei.com/post/791dd0f5.html"/>
    <id>https://www.mahaofei.com/post/791dd0f5.html</id>
    <published>2023-04-23T07:16:08.000Z</published>
    <updated>2023-04-23T07:16:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hybrid-Physical-Metric-For-6-DoF-Grasp-Pose-Detection"><a href="#Hybrid-Physical-Metric-For-6-DoF-Grasp-Pose-Detection" class="headerlink" title="Hybrid Physical Metric For 6-DoF Grasp Pose Detection"></a>Hybrid Physical Metric For 6-DoF Grasp Pose Detection</h1><blockquote><p><strong>标题</strong>：用于6D抓取检测的混合物理度量<br><strong>作者团队</strong>：清华大学（王生进）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/luyh20/FGC-GraspNet">https://github.com/luyh20/FGC-GraspNet</a></p></blockquote><h2 id="一、目标问题"><a href="#一、目标问题" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>单个物理指标会导致离散的抓取置信度分数，在百万抓取数据训练时会导致预测结果不准确。</p><p>本文定义了一种新的度量方式，基于力封闭度量、物体平面度、重力和碰撞测量。</p><p>本文设计了平面重力碰撞FGC-GraspNet，适用于多任务多分辨率学习体系。</p><h2 id="二、混合物理度量"><a href="#二、混合物理度量" class="headerlink" title="二、混合物理度量"></a>二、混合物理度量</h2><p><img src="https://img.mahaofei.com/img/202304231518654.png" alt="image.png"></p><p><strong>（1）平面度</strong></p><p>平面度越高的抓的越稳。利用点的局部法向量的相似性计算平坦度得分。</p><p><strong>（2）重心度量</strong></p><p>夹持力更接近物体重心的更稳定。计算物体重心到两个接触点的连线的距离作为重力得分。</p><p><strong>（3）碰撞扰动度量</strong></p><p>当夹爪接近物体时容易发生碰撞，因此取夹爪两个最大行程端点与物体接触点的欧氏距离最小值作为碰撞扰动得分。</p><p><strong>（4）混合物理度量</strong></p><p>混合物理度量是上面度量的加权组合。</p><h2 id="三、-FGC-GraspNet"><a href="#三、-FGC-GraspNet" class="headerlink" title="三、 FGC-GraspNet"></a>三、 FGC-GraspNet</h2><p>通过最远点采样FPS得到20000x3对点云输入，网络由PointNet++，FA分支、RD分支组成。</p><p><img src="https://img.mahaofei.com/img/202304231530817.png" alt=""></p><ul><li>PointNEt++用于提取点特征</li><li>低分辨率的特征进入FA分支进行前景分割和逐点逼近方向得分回归</li><li>高分辨率的特征用于RD旋转分支。</li></ul><h2 id="四、思考"><a href="#四、思考" class="headerlink" title="四、思考"></a>四、思考</h2><p>将混合物理度量纳入LOSS的计算过程确实有意义。而且具有一定的复用性，其它算法也可借鉴此设计。</p><h1 id="Volumetric-Grasping-Network-Real-time-6-DOF-Grasp-Detection-in-Clutter"><a href="#Volumetric-Grasping-Network-Real-time-6-DOF-Grasp-Detection-in-Clutter" class="headerlink" title="Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter"></a>Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter</h1><blockquote><p><strong>标题</strong>：基于体素的抓取网络：杂乱场景中实时6D抓取检测<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：CoRL2020<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/vgn">https://github.com/ethz-asl/vgn</a></p></blockquote><h2 id="一、目标问题-1"><a href="#一、目标问题-1" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>本文提出了一种网络从深度相机中获得场景信息，预测6D抓取的网络。</p><h2 id="二、论文方法"><a href="#二、论文方法" class="headerlink" title="二、论文方法"></a>二、论文方法</h2><p><strong>（1）网络架构</strong></p><p>由滤波器、卷积层组成的感知模块将输入体素映射为特征图，然后进行卷积、上采样操作，最后是三个独立的分支用于预测抓取质量、旋转和夹爪宽度。</p><p><strong>（2）抓取检测</strong></p><p>使用一些方法去除不可能的抓取姿势，然后应用非极大抑制来获得候选的抓取列表。</p><h2 id="三、思考"><a href="#三、思考" class="headerlink" title="三、思考"></a>三、思考</h2><p>非常基础的方法，已经有人在此基础上进行了扩展并发表了顶会。</p><h1 id="Efficient-Learning-of-Goal-Oriented-Push-Grasping-Synergy-in-Clutter"><a href="#Efficient-Learning-of-Goal-Oriented-Push-Grasping-Synergy-in-Clutter" class="headerlink" title="Efficient Learning of Goal-Oriented Push-Grasping Synergy in Clutter"></a>Efficient Learning of Goal-Oriented Push-Grasping Synergy in Clutter</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：浙江大学（熊蓉）<br><strong>期刊会议</strong>：RAL<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy">https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy</a></p></blockquote><h2 id="一、目标问题-2"><a href="#一、目标问题-2" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>在混乱场景中抓取物体时，有时需要一些预抓取动作，例如推动。使机械臂能够分离目标对象并稳定的实现抓取。</p><h2 id="二、方法"><a href="#二、方法" class="headerlink" title="二、方法"></a>二、方法</h2><p><img src="https://img.mahaofei.com/img/202304231536100.png" alt=""></p><p>环境准备：固定的RGBD相机拍摄工作空间，将RGBD投影到重力方向，使用颜色高度图和深度高度图表示每个状态。</p><p><strong>（1）有目标的抓取训练</strong></p><p>训练一个有目标条件下的抓取网络，当有足够的训练之后，成功抓取的Q值稳定。</p><p><strong>（2）有目标的推动训练</strong></p><p>训练一个有目标条件下的推动网络，推动的奖励函数是基于抓取网络反向训练设计的。</p><p><strong>（3）交替训练</strong></p><p>利用交替训练来解决物体分布不匹配的问题，进一步提高混乱环境中抓取策略性能。</p><h2 id="三、思考-1"><a href="#三、思考-1" class="headerlink" title="三、思考"></a>三、思考</h2><p>推物体再抓取物体相当于一个两阶段方法，可以不用在底层进行训练，而是在高层的规划决策层来进行判断，发布任务是推物体还是抓物体。</p><h1 id="TransGrasp-Grasp-Pose-Estimation-ofaCategory-ofObjects-byTransferring-Grasps-fromOnlyOne-Labeled-Instance"><a href="#TransGrasp-Grasp-Pose-Estimation-ofaCategory-ofObjects-byTransferring-Grasps-fromOnlyOne-Labeled-Instance" class="headerlink" title="TransGrasp: Grasp Pose Estimation ofaCategory ofObjects byTransferring Grasps fromOnlyOne Labeled Instance"></a>TransGrasp: Grasp Pose Estimation ofaCategory ofObjects byTransferring Grasps fromOnlyOne Labeled Instance</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：大连理工大学（孙怡）<br><strong>期刊会议</strong>：ECCV<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/yanjh97/TransGrasp">https://github.com/yanjh97/TransGrasp</a></p></blockquote><h2 id="一、目标问题-3"><a href="#一、目标问题-3" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>现有大多数方法需要大量的抓取数据来训练，为了解决这个问题，本文实现只标记一个对象预测一类对象的抓取姿态。</p><h2 id="二、方法-1"><a href="#二、方法-1" class="headerlink" title="二、方法"></a>二、方法</h2><p><strong>（1）学习类别的对应关系</strong></p><ol><li>Shape Encoder和DIFDecoder组成神经网络，训练得到对象变形到模板的密集对应关系</li></ol><p><strong>（2）抓取姿态估计</strong></p><ol><li>点云首先从相机坐标系转换到对象坐标系</li><li>生成对象实例的变形到模板</li><li>将带有抓取注释的模型输入到DeformNet中获得模型的变形</li><li>由两者的共同模板见你对应关系，通过对准物体表面上的抓握点来引导抓握姿势的变换</li><li>通过refine模块进行优化</li><li>将优化后的抓握知识转换为相机坐标进行抓取</li></ol><h2 id="三、思考-2"><a href="#三、思考-2" class="headerlink" title="三、思考"></a>三、思考</h2><p>这种算法只能实现与模板形状相似的物体进行抓取，而且每个类别要先手工标记1000个抓握姿势。</p><h1 id="Contact-GraspNet-Efficient-6-DoF-Grasp-Generation-in-Cluttered-Scenes"><a href="#Contact-GraspNet-Efficient-6-DoF-Grasp-Generation-in-Cluttered-Scenes" class="headerlink" title="Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes"></a>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</h1><blockquote><p><strong>标题</strong>：ContactGraspNet：在杂乱场景中高效生成6-DoF抓取<br><strong>作者团队</strong>：NVIDIA<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a></p></blockquote><h2 id="1-目标问题"><a href="#1-目标问题" class="headerlink" title="1 目标问题"></a>1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/202304231545643.png" alt=""></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/202304231546650.png" alt=""></p><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1 id="RGB-Matters-Learning-7-DoF-Grasp-Poses-on-Monocular-RGBD-Images"><a href="#RGB-Matters-Learning-7-DoF-Grasp-Poses-on-Monocular-RGBD-Images" class="headerlink" title="RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images"></a>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</h1><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="一、目标问题-4"><a href="#一、目标问题-4" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="二、方法-2"><a href="#二、方法-2" class="headerlink" title="二、方法"></a>二、方法</h2><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒。可以使用下面的模型，将夹持器旋转预测作为一个分类问题进行预测。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>AVN最终的输出表示为角视图热图。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="三、思考-3"><a href="#三、思考-3" class="headerlink" title="三、思考"></a>三、思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1 id="CaTGrasp-Learning-Category-Level-Task-Relevant-Grasping-in-Clutter-from-Simulation"><a href="#CaTGrasp-Learning-Category-Level-Task-Relevant-Grasping-in-Clutter-from-Simulation" class="headerlink" title="CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation"></a>CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation</h1><blockquote><p><strong>标题</strong>：CaTGrasp：从仿真中学习杂乱场景的类别级抓取<br><strong>作者团队</strong>：Rutgers University（美国罗格斯大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/wenbowen123/catgrasp">https://github.com/wenbowen123/catgrasp</a></p></blockquote><h2 id="一、目标问题-5"><a href="#一、目标问题-5" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>提出了一个框架学习工业对象的抓取，不需要真实的数据或手动注释</p><h2 id="二、方法-3"><a href="#二、方法-3" class="headerlink" title="二、方法"></a>二、方法</h2><p>给定同一类别的3D模型的数据库，该方法学习</p><ul><li>以对象为中心的NUNOCS表示</li><li>hotmap：抓握过程中手-对象接触区域的任务实现成功的可能性</li><li>抓取姿势的编码本</li></ul><p><strong>（1）类别级标准NUNOCS表示</strong></p><p>将同一个类别的不同实例对象转换到标准空间，并缩放为标准大小。</p><p><strong>（2）稳定抓取学习</strong></p><p>首先给定从当前实例到规范模型的9D变换，将相同的变换应用于抓取来得到抓取建议。</p><p>将生成的抓取用于训练基于PointNet构建的网络，预测抓取质量。</p><p><strong>（3）实例分割</strong></p><p>使用了3D U-Net，将整个场景的点云作为输入，预测每点偏移到物体中心，将偏移点聚类为实例段。</p><p><strong>（4）仿真中生成训练数据</strong></p><p>利用PyBullet模拟生成合成数据。</p><h2 id="三、思考-4"><a href="#三、思考-4" class="headerlink" title="三、思考"></a>三、思考</h2><p>该方法提出了一种使用仿真数据进行训练，减少人工标注的方法。抓取的方法没有太多创新，仍然需要每个类别提供多个预先的实例以及抓取姿态用于训练。</p><h1 id="Closed-Loop-Next-Best-View-Planning-for-Target-Driven-Grasping"><a href="#Closed-Loop-Next-Best-View-Planning-for-Target-Driven-Grasping" class="headerlink" title="Closed-Loop Next-Best-View Planning for Target-Driven Grasping"></a>Closed-Loop Next-Best-View Planning for Target-Driven Grasping</h1><blockquote><p><strong>标题</strong>：闭环次优视图规划用于目标驱动的抓取<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/active_grasp">https://github.com/ethz-asl/active_grasp</a></p></blockquote><h2 id="一、目标问题-6"><a href="#一、目标问题-6" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>从密集遮挡环境中抓取物体</p><h2 id="二、方法-4"><a href="#二、方法-4" class="headerlink" title="二、方法"></a>二、方法</h2><p>该方法具有以下前提条件</p><ul><li>机械臂末端连接深度相机</li><li>相机光学中心和手抓中心已经校准</li><li>已知物体的部分视图和3D边界框</li></ul><p><img src="https://img.mahaofei.com/img/202304231603042.png" alt=""></p><p>首先将点云观测yt和相机姿态xt继承，重建为体素图。计算体素的可抓取性，以及可能的抓取姿态。如果可抓取性不满足要求，就调整机械臂位置计算下一张图</p><p><strong>（1）抓取检测</strong></p><p>使用体积抓取网络VGN进行抓取。该网络将体素网格M映射到抓握质量分数Q、平行抓握方向R、开口宽度W。</p><p>过滤掉指尖不在目标边界框的抓取姿态、无法找到反向运动学解的抓取姿态。</p><p><strong>（2）次优视图规划器</strong></p><p><strong>世界表示</strong>：使用TSDF（截断有符号距离函数）表示大小为 l 的立方体体素。</p><p><strong>视图生成</strong>：将候选视图生成在目标边界上半球内。</p><p><strong>信息增益</strong>：TSDF 重建的完整性对于抓取的检测和预测准确性有很大影响。因此使用了后侧体素 IG 公式的变体，对被遮挡具有负距离的体素使用光线投影来计算隐藏对象体素的数量。</p><ol><li>在策略更新的最大数量中加入时间预算</li><li>如果抓取分数低于给定的阈值，就会停止算法，因为获取不到有用信息</li><li>如果VGN在几帧内保持稳定的抓取配置，就停止</li></ol><h2 id="三、思考-5"><a href="#三、思考-5" class="headerlink" title="三、思考"></a>三、思考</h2><p>该方法是在位姿估计的基础上进行的抓取预测，可以将该方法与Gen6D结合起来，获得物体的6D抓取位姿。</p><h1 id="Edge-Grasp-Network-A-Graph-Based-SE-3-invariant-Approach-to-Grasp-Detection"><a href="#Edge-Grasp-Network-A-Graph-Based-SE-3-invariant-Approach-to-Grasp-Detection" class="headerlink" title="Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection"></a>Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection</h1><blockquote><p><strong>标题</strong>：边缘抓取网络：一种基于图的SE(3)不变的抓取检测方法<br><strong>作者团队</strong>：Northeastern University（美国东北大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/HaojHuang/Edge-Grasp-Network">https://github.com/HaojHuang/Edge-Grasp-Network</a></p></blockquote><h2 id="一、目标问题-7"><a href="#一、目标问题-7" class="headerlink" title="一、目标问题"></a>一、目标问题</h2><p>以单个视角观察到的点云为输入，得到一组抓取姿态</p><h2 id="二、方法-5"><a href="#二、方法-5" class="headerlink" title="二、方法"></a>二、方法</h2><p><strong>（1）裁剪点云</strong></p><p>给定一个点云p和接近点pa，只有接近点pa的相邻点会影响抓取，因此以pa为中心裁剪一个球。</p><p><strong>（2）PointNet卷积</strong></p><p>使用PointNet计算接近点与最近邻点，每个点的特征。</p><p><strong>（3）计算全局特征</strong></p><p>将逐点特征传递给MLP，用最大池化层生成一级全局特征。这些全局特征再与点特征相连传递到第二个MLP计算全局特征。</p><p>对于每个抓取，通过将全局特征与点特征连接来计算边缘特征，用分类器表示边缘抓取。</p><p><strong>（4）抓取评估</strong></p><p>使用sigmoid函数的四层MLP来预测抓取成功率，以边缘特征为输入计算抓取是否成功。</p><h2 id="三、思考-6"><a href="#三、思考-6" class="headerlink" title="三、思考"></a>三、思考</h2><p>该方法类似于DenseFusion的思想，即提取逐点特征和全局特征，进行特征融合，本文得到的融合特征即边缘特征，利用该特征再使用分类器得到抓取位姿。</p>]]></content>
    
    
    <summary type="html">调研近三年顶会顶刊上的抓取姿态估计的论文</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="抓取姿态估计" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>CVPR2022高被引论文笔记</title>
    <link href="https://www.mahaofei.com/post/6ec34466.html"/>
    <id>https://www.mahaofei.com/post/6ec34466.html</id>
    <published>2023-04-23T02:45:17.000Z</published>
    <updated>2023-04-23T02:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、目标检测"><a href="#一、目标检测" class="headerlink" title="一、目标检测"></a>一、目标检测</h1><h2 id="1-1-视频目标检测"><a href="#1-1-视频目标检测" class="headerlink" title="1.1 视频目标检测"></a>1.1 视频目标检测</h2><h3 id="Video-Swin-Transformer"><a href="#Video-Swin-Transformer" class="headerlink" title="Video Swin Transformer"></a>Video Swin Transformer</h3><blockquote><p><strong>标题</strong>：视频 Swin Transformer<br><strong>作者团队</strong>：Microsoft Research Asia<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>现今大多数的视觉识别模型都是基于Transformer建立的，本文在此基础上进行调整，得到更好的速度和精度。</p><p><strong>（2）方法</strong></p><ol><li>总体架构</li></ol><p>视频定义为TxHxWx3，patch为2x4x4x3的块，每个patch有96个特征维度。该架构的主要组件是Video Swin Transformer模块，通过将标准的Transformer的Multihead self-attention(MSA)模块替换为基于3D Shift Window的MSA模块，来实现。</p><p><img src="https://img.mahaofei.com/img/202305071021361.png" alt="image.png"></p><ol><li>3D MSA模块</li></ol><p>由于视频有时间维度，全局自注意模块会导致巨大的计算和内存成本。MSA模块就比传统的全局自注意模块要高效。</p><p>更进一步，基于Swin Transformer的2D移位窗口扩展到3D，实现了跨窗口链接，保证了体系结构的表达能力。</p><p><img src="https://img.mahaofei.com/img/202305071033856.png" alt="image.png"></p><h1 id="二、图像分割"><a href="#二、图像分割" class="headerlink" title="二、图像分割"></a>二、图像分割</h1><h1 id="三、图像处理"><a href="#三、图像处理" class="headerlink" title="三、图像处理"></a>三、图像处理</h1><h2 id="3-1-图像合成"><a href="#3-1-图像合成" class="headerlink" title="3.1 图像合成"></a>3.1 图像合成</h2><h3 id="High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models"><a href="#High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models" class="headerlink" title="High-Resolution Image Synthesis with Latent Diffusion Models"></a>High-Resolution Image Synthesis with Latent Diffusion Models</h3><blockquote><p><strong>标题</strong>：具有潜在扩散模型的高分辨率图像合成<br><strong>作者团队</strong>：海德堡大学；Runway ML<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>扩散模型已经在包括图像数据在内的很多数据上，实现了很好的数据合成效果。但这些模型由于直接操作像素，需要昂贵的GPU资源。</p><p>本文提出的潜在扩散模型，达到了降低复杂性和保留细节的平衡点。</p><p><strong>（2）方法</strong></p><p>主要方法是：使用自动编码模型，学习一个在感知上与图像空间等效的空间，压缩学习阶段和生成学习阶段来减少资源需求。</p><ol><li>感知压缩模型<br>利用了结合perceptual loss, patch-based, adversarial objective的自动编码器。</li><li>潜在扩散模型<br>扩散模型是概率模型，通过逐渐对正态分布变量去噪来学习数据分布。<br>通过由自动编码器得到的高效、低维的空间，与高维像素空间相比更适合生成模型。</li><li>调节机制<br>通过使用交叉注意力机制增强基础网络UNet，能够处理各种模态的输入。</li></ol><p><strong>（3）思考</strong></p><p>将需要高运算量的像素操作，通过自动编码转换为了低维空间的操作，节省了计算量。</p><h1 id="四、三维视觉"><a href="#四、三维视觉" class="headerlink" title="四、三维视觉"></a>四、三维视觉</h1><h1 id="五、位姿估计"><a href="#五、位姿估计" class="headerlink" title="五、位姿估计"></a>五、位姿估计</h1><h1 id="六、机器人"><a href="#六、机器人" class="headerlink" title="六、机器人"></a>六、机器人</h1><h1 id="七、神经网络"><a href="#七、神经网络" class="headerlink" title="七、神经网络"></a>七、神经网络</h1><h2 id="7-1-神经网络结构设计"><a href="#7-1-神经网络结构设计" class="headerlink" title="7.1 神经网络结构设计"></a>7.1 神经网络结构设计</h2><h3 id="A-ConvNet-for-the-2020s"><a href="#A-ConvNet-for-the-2020s" class="headerlink" title="A ConvNet for the 2020s"></a>A ConvNet for the 2020s</h3><blockquote><p><strong>标题</strong>：2020s的ConvNet<br><strong>作者团队</strong>：Facebook AI<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/facebookresearch/ConvNeXt">https://github.com/facebookresearch/ConvNeXt</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>20年以来，由于Vision Transformers的引入，它开始快速取代卷积神经网络。但只使用Transformers也有些问题，因此后来又出现了hierarchical Transformers，其中加入了几个卷积神经网络作为先验。但这些方法都可以归结为Transformers的优势。</p><p>本文想要探讨纯卷积神经网络所能实现的极限。</p><p><strong>（2）最佳方法</strong></p><ol><li><strong>训练技术</strong>：使用AdamW优化器、数据增强、随机擦除、正则化等方法可以显著提高训练模型的性能</li><li><strong>宏观设计</strong>：<ul><li>阶段比例：ResNet中各阶段的比例很大程度是经验获得的，SwinTransformer的比例是1:1:3:1，传统的ResNet比例是(3,4,6,3)，此处调整为(3,3,9,3)与SwinT相同，发现也提高了模型准确率</li><li>模块设计：标准的ResNet模块包括一个7x7步长2的卷积层，然后是一个最大池化层。此处模仿SwinT，设计为4x4步长为4的卷积层作为基础模块。</li></ul></li><li><strong>使用分组卷积技术</strong>，可以有效提高网络性能</li><li><strong>反向瓶颈</strong>：使MLP的隐藏维度比输入维度宽4倍，这在几个ConvNet中以及Transformer中设计思路相同。</li><li><strong>更大的卷积核</strong>：尽管堆叠小卷积核可以有效利用硬件，但测试证明，总体上大卷积核能够提高模型性能</li><li><strong>微观设计</strong>：<ul><li>更少的归一化层</li><li>使用层归一化LN代替批归一化BatchNorm</li><li>分离下采样层：ResNet中，下采样是通过每个阶段开始的残差块实现的，在层和层之间加入单独的下采样层发现可以提高准确率</li></ul></li></ol><p><strong>（3）总结</strong></p><ol><li>尽可能丰富数据，增大随机化程度：使用AdamW优化器、数据增强、随机擦除、正则化等方法</li><li>使用更优化的网络结构：调整各阶段卷积比例、使用反向瓶颈设计、更少的归一化层、更大的卷积核、在每个阶段之间加入下采样层。</li></ol>]]></content>
    
    
    <summary type="html">阅读CVPR的高被印论文，开拓视野。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="顶会顶刊" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E9%A1%B6%E4%BC%9A%E9%A1%B6%E5%88%8A/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>如何使用Git管理项目代码</title>
    <link href="https://www.mahaofei.com/post/dd16f220.html"/>
    <id>https://www.mahaofei.com/post/dd16f220.html</id>
    <published>2023-04-17T07:00:50.000Z</published>
    <updated>2023-04-17T07:00:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、注册Github并创建仓库"><a href="#一、注册Github并创建仓库" class="headerlink" title="一、注册Github并创建仓库"></a>一、注册Github并创建仓库</h1><p>这一步不细说了，需要科学上网，参考<a href="https://www.mahaofei.com/post/96c83ac9.html">这篇文章</a>，[[03_科学上网方法（如何访问Google, ChatGPT）|Google学术访问方法]]。</p><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><h1 id="二、下载Git并配置"><a href="#二、下载Git并配置" class="headerlink" title="二、下载Git并配置"></a>二、下载Git并配置</h1><h2 id="2-1-Git安装"><a href="#2-1-Git安装" class="headerlink" title="2.1 Git安装"></a>2.1 Git安装</h2><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><p>在资源管理器内右键，选择<code>Git bash here</code>打开Git界面。</p><h2 id="2-2-Git配置"><a href="#2-2-Git配置" class="headerlink" title="2.2 Git配置"></a>2.2 Git配置</h2><p>输入下面的代码，按下回车，生成ssh密钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;邮箱地址&quot;</span><br></pre></td></tr></table></figure><p>出现要求设置密码，可以不用设置，连续回车两次就可以。</p><p>打开<code>C:\Users\用户名\.ssh</code>，可以看到有一个<code>id_rsa.pub</code>文件，这就是刚才生成的密钥。</p><p>使用记事本打开此文件，复制里面的密钥内容。</p><h2 id="2-3-Github添加ssh-key"><a href="#2-3-Github添加ssh-key" class="headerlink" title="2.3 Github添加ssh key"></a>2.3 Github添加ssh key</h2><p>进入<a href="https://github.com/">Github官网</a>，点击右上角【setting —&gt; SSH and GPG keys —&gt; New SSH key】，在这里添加密钥，其中</p><ul><li>Title：自己写一个ssh key的名字，用于区分多个ssh key</li><li>Key：刚刚复制的密钥<br>填写完成后点击Add SSH key添加。</li></ul><p>然后在git bash中输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>如果连接成功，会让你输入<code>yes/no</code>，输入yes即可。</p><h2 id="2-4-配置用户名和邮箱"><a href="#2-4-配置用户名和邮箱" class="headerlink" title="2.4 配置用户名和邮箱"></a>2.4 配置用户名和邮箱</h2><p>输入下面的代码配置自己的用户名和邮箱，两个信息都要和Github账号的信息一致</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;用户名&quot;</span><br><span class="line">git config --global user.email &quot;邮箱&quot;</span><br></pre></td></tr></table></figure><h1 id="三、代码管理"><a href="#三、代码管理" class="headerlink" title="三、代码管理"></a>三、代码管理</h1><h2 id="3-1-代码上传"><a href="#3-1-代码上传" class="headerlink" title="3.1 代码上传"></a>3.1 代码上传</h2><p><strong>（1）初始化</strong></p><p>创建一个文件夹，在这个文件夹内，右键<code>git bash here</code>，然后输入<code>git init</code>完成初始化。</p><p>可以看到目录中出现了一个<code>.git</code>隐藏文件夹，这说明已经完成了初始化。</p><p><strong>（2）链接远程仓库</strong></p><p>在刚刚的<code>git bash</code>窗口，输入下面的命令同步到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p>如果出现fatal: remote origin already exists.可按以下步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br><span class="line">git pull git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p><strong>（3）上传本地文件</strong></p><p>添加本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add.</span><br></pre></td></tr></table></figure><p>提交本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;说明信息，一般说明本次提交更新了什么&quot;</span><br></pre></td></tr></table></figure><p>推送到远端仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push git@github.com:用户名/仓库名.git</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><h2 id="3-2-拉取代码"><a href="#3-2-拉取代码" class="headerlink" title="3.2 拉取代码"></a>3.2 拉取代码</h2><p>从项目中拉取代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><p>如果出现<code>fatal: refusing to merge unrelated histories</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br></pre></td></tr></table></figure><h2 id="3-3-分支管理"><a href="#3-3-分支管理" class="headerlink" title="3.3 分支管理"></a>3.3 分支管理</h2><p><strong>（1）查看分支</strong></p><p>在命令行窗口的光标处，输入git branch命令，查看 Git 仓库的分支情况。分支前有*表示是当前所在的分支。</p><p><strong>（2）创建分支</strong></p><p>使用下面的命令创建一个名为a的分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch a</span><br></pre></td></tr></table></figure><p><strong>（3）分支切换</strong></p><p>在命令行窗口的光标处，输入git checkout a命令，切换到a分支。</p><p><strong>（4）合并分支</strong></p><p>切换到master分支，然后输入git merge a命令，将a分支合并到master分支。</p><p><strong>（5）删除分支</strong></p><p>在命令行窗口的光标处，输入git branch -d a命令，删除a分支。</p><p><strong>（6）为分支添加标签</strong></p><p>在命令行窗口的光标处，输入git tag test_tag命令，为当前分支添加标签test_tag</p><h2 id="3-4-修改分支名称"><a href="#3-4-修改分支名称" class="headerlink" title="3.4 修改分支名称"></a>3.4 修改分支名称</h2><p>假设分支名称为oldName，想要修改为 newName</p><ol><li>本地分支重命名(还没有推送到远程)</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><ol><li>远程分支重命名 (已经推送远程-假设本地分支和远程对应分支名称相同)</li></ol><p>重命名远程分支对应的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><p>到github修改默认分支的分支名。</p><p>上传新命名的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin newName</span><br></pre></td></tr></table></figure><p>把修改后的本地分支与远程分支关联</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --set-upstream-to origin/newName</span><br></pre></td></tr></table></figure><p>注意：如果本地分支已经关联了远程分支，需要先解除原先的关联关系：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --unset-upstream </span><br></pre></td></tr></table></figure><h2 id="3-5-报错信息"><a href="#3-5-报错信息" class="headerlink" title="3.5 报错信息"></a>3.5 报错信息</h2><p><strong>（1）error: src refspec master does not match any. error: failed to push some refs to</strong></p><p>仔细检查push的是<code>master</code>分支还是<code>main</code>分支。</p>]]></content>
    
    
    <summary type="html">当有多台设备，或者同一个项目有多个版本的代码时，利用git管理项目代码就十分必要了。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/tags/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    <category term="Git" scheme="https://www.mahaofei.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>【抓取算法】Contact GraspNet</title>
    <link href="https://www.mahaofei.com/post/c18d351e.html"/>
    <id>https://www.mahaofei.com/post/c18d351e.html</id>
    <published>2023-04-07T07:52:27.000Z</published>
    <updated>2023-04-07T07:52:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、论文笔记"><a href="#一、论文笔记" class="headerlink" title="一、论文笔记"></a>一、论文笔记</h1><blockquote><p><strong>标题</strong>：Contact-GraspNet: 在杂乱场景中高效生成6-DoF抓取<br><strong>期刊会议</strong>：ICRA2021<br><strong>作者团队</strong>：Martin Sundermeyer（NVIDIA）<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a><br><strong>数据集</strong>：</p></blockquote><h2 id="1-1-目标问题"><a href="#1-1-目标问题" class="headerlink" title="1.1 目标问题"></a>1.1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="1-2-方法"><a href="#1-2-方法" class="headerlink" title="1.2 方法"></a>1.2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/20230404152359.png" alt="image.png"></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/20230404153946.png" alt="image.png"></p><h2 id="1-3-思考"><a href="#1-3-思考" class="headerlink" title="1.3 思考"></a>1.3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1 id="二、算法复现"><a href="#二、算法复现" class="headerlink" title="二、算法复现"></a>二、算法复现</h1><h2 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h2><p><strong>（1）环境搭建</strong></p><p>下载代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/NVlabs/contact_graspnet.git</span><br></pre></td></tr></table></figure><p>创建虚拟环境<br>（这个环境是没问题的，如果出现依赖不满足要求的情况，可以先删掉那项，创建完环境后再手动安装）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f contact_graspnet_env.yml</span><br></pre></td></tr></table></figure><p>重新编译<code>pointnet_tfops</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh compile_pointnet_tfops.sh</span><br></pre></td></tr></table></figure><p><strong>（2）模型和数据准备</strong></p><p>从作者给出的连接下载<a href="https://drive.google.com/drive/folders/1tBHKf60K8DLM5arm-Chyf7jxkzOr5zGl?usp=sharing">trained models</a>，并将它们放到<code>./checkpoints/</code>，下载<a href="https://drive.google.com/drive/folders/1v0_QMTUIEOcu09Int5V6N2Nuq7UCtuAA?usp=sharing">test data</a>，并将它们放到<code>./test_data</code></p><h2 id="2-2-预测抓取"><a href="#2-2-预测抓取" class="headerlink" title="2.2 预测抓取"></a>2.2 预测抓取</h2><p>给定一个深度图(.npy文件/单位m)，相机内参，2D实例分割图，运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/inference.py \</span><br><span class="line">       --np_path=test_data/0.npy \</span><br><span class="line">       --local_regions --filter_grasps</span><br></pre></td></tr></table></figure><p><code>--np_path</code>：输入的.npz/.npy文件，带有深度、内参、实力分割图、RGB信息<br><code>--ckpt_dir</code>：checkpoint目录，默认为<code>checkpoint/scene_test_2048_bs3_hor_sigma_001</code>，非常干净的深度数据使用<code>scene_2048_bs3_rad2_32</code>，非常混乱的深度数据使用<code>scene_test_2048_bs3_hor_sigma_0025</code><br><code>--local_regions</code>：裁剪的3D实例分割<br><code>--filter_grasps</code>：筛选抓取点，使他们只为于对象的表面<br><code>--skip_border_objects</code>：忽略碰到深度图边缘的实例分割<br><code>--forward_passes</code>：前向计算的次数，增加可以提高抓取的采样点<br><code>--z_range</code>：[min, max]的z值来裁剪输入点云<br><code>--arg_configs TEST.second_thres:0.19 TEST.first_thres:0.23</code>：覆盖抓取的配置置信度来获得更多或更少的抓取候选</p><h2 id="2-3-训练网络"><a href="#2-3-训练网络" class="headerlink" title="2.3 训练网络"></a>2.3 训练网络</h2><p><strong>（1）下载数据集</strong></p><ul><li>下载<a href="https://drive.google.com/file/d/1zcPARTCQx2oeiKk7a-wdN_CN-RUVX56c/view?usp=sharing">Acronym</a>数据集</li><li>从<a href="https://www.shapenet.org/">https://www.shapenet.org/</a>下载ShapeNet meshe</li><li>创建watertiget<ul><li>下载并构建<a href="https://github.com/hjwdzh/Manifold">https://github.com/hjwdzh/Manifold</a></li><li>创建watertight mesh，假设物体路径为model.obj：<code>manifold model.obj temp.watertight.obj -s</code></li><li>简化：<code>simplify -i temp.watertight.obj -o model.obj -m -r 0.02</code></li></ul></li></ul><p>下载10000个带有Contact抓取信息的桌面训练场景<a href="https://drive.google.com/drive/folders/1eeEXAISPaStZyjMX8BHR08cdQY4HF4s0">Google Drive</a>，解压为下面的格式</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">acronym</span><br><span class="line">├── grasps</span><br><span class="line">├── meshes</span><br><span class="line">├── scene_contacts</span><br><span class="line">└── splits</span><br></pre></td></tr></table></figure><p><strong>（2）训练Contact-GraspNet</strong></p><p>如果在没有外设的服务器上训练，设置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYOPENGL_PLATFORM=&#x27;egl&#x27;</span><br></pre></td></tr></table></figure><p>使用配置文件<code>contact_graspnet/config.yaml</code>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/train.py --ckpt_dir checkpoints/your_model_name \</span><br><span class="line">                                 --data_path /path/to/acronym/data</span><br></pre></td></tr></table></figure><p><strong>（3）生成自己的Contact Grasps和场景</strong></p><p>所下载的<code>scene_contacts</code>是从Acronym数据集生成的，要生成自己的数据集，下载安装<a href="https://github.com/NVlabs/acronym">acronym_tools</a>。</p><p>第一步，对象的6D抓取被映射到保存在<code>mesh_contacts</code>的接触点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_contact_infos.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>根据生成的<code>mesh_contacts</code>，可以创建桌面场景保存到<code>scene_contacts</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>一个线程大约花费3天，可以多次运行命令在多个核上并行处理。</p><p>可视化显示创建的桌面场景和抓取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym \</span><br><span class="line">       --load_existing scene_contacts/000000.npz -vis</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">复现NVIDIA提出的抓取估计算法Contact GraspNet</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>使用内网穿透SakuraFrp远程连接服务器</title>
    <link href="https://www.mahaofei.com/post/9ed2c32f.html"/>
    <id>https://www.mahaofei.com/post/9ed2c32f.html</id>
    <published>2023-04-05T05:59:35.000Z</published>
    <updated>2023-04-05T05:59:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux端配置"><a href="#Linux端配置" class="headerlink" title="Linux端配置"></a>Linux端配置</h1><p><strong>（1）ssh配置</strong></p><p>安装ssh服务器与客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt -y install openssh-server</span><br><span class="line">sudo apt -y install openssh-client</span><br></pre></td></tr></table></figure><p>配置ssh客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><ul><li>​将<code>PermitRootLogin prohibt-password</code> 修改为 <code>PermitRootLogin yes</code></li><li>将<code>PasswordAuthentication yes</code> 前的#删除，取消注释</li></ul><p>重启ssh服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh restart</span><br></pre></td></tr></table></figure><p>查看ssh服务运行状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh status</span><br></pre></td></tr></table></figure><p><strong>（2）Sakura配置</strong></p><p><a href="https://www.natfrp.com/user/">SakuraFrp</a></p><p>进入隧道列表新建隧道</p><ul><li>尽量选择国内节点</li><li>隧道类型为TCP隧道</li><li>本机端口为SSH</li><li>主机ip默认127.0.0.1即可(代指内网穿透本机)</li></ul><p><img src="https://img.mahaofei.com/img/20230405140317.png" alt=""></p><p>在官网下载对应版本的frpc，复制下载链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget -O frpc &lt;下载地址&gt;</span><br><span class="line">chmod 755 frpc</span><br><span class="line">ls -ls frpc</span><br><span class="line">md5sum frpc</span><br><span class="line">frpc -v</span><br></pre></td></tr></table></figure><p>隧道配置文件中复制隧道密钥</p><p>Ubuntu中使用下面的命令开启隧道</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frpc -f &lt;复制的密钥&gt;</span><br></pre></td></tr></table></figure><h1 id="Windows端配置"><a href="#Windows端配置" class="headerlink" title="Windows端配置"></a>Windows端配置</h1><p>打开【设置-应用-添加功能】，添加OpenSSH 服务器和OpenSSH 客户端。</p><p>打开服务，找到 OpenSSH SSH Server 和 OpenSSH Authentication Agent -&gt; 启动服务并设为自动。</p><p>打开 power shell，使用以下命令检查安装和运行情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-Service sshd</span><br></pre></td></tr></table></figure><p>打开Sakura官网，打开隧道列表，点击要连接的隧道，点击一键认证，下载exe认证程序并运行。</p><p>然后使用<code>ssh -p &lt;端口号&gt; &lt;用户名&gt;@&lt;地址&gt;</code>进行远程连接</p><h1 id="VSCode远程ssh开发环境"><a href="#VSCode远程ssh开发环境" class="headerlink" title="VSCode远程ssh开发环境"></a>VSCode远程ssh开发环境</h1><p>安装插件 <code>Remote - SSH</code></p>]]></content>
    
    
    <summary type="html">不想使用向日葵和todesk等工具远程连接桌面，而且个人电脑和服务器也不在一个局域网下，想要远程连接服务器，因此考虑使用内网穿透。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】Gen6D算法</title>
    <link href="https://www.mahaofei.com/post/76335f84.html"/>
    <id>https://www.mahaofei.com/post/76335f84.html</id>
    <published>2023-03-29T13:22:39.000Z</published>
    <updated>2023-03-29T13:22:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="1-1-目标问题"><a href="#1-1-目标问题" class="headerlink" title="1.1 目标问题"></a>1.1 目标问题</h3><p>现有的位姿估计算法要么需要高质量的物体模型，要么需要提供额外的深度图或物体掩码图，这对于位姿估计的实际应用有很大的限制。本文提出的方法只需要一些物体的姿态图像，就能够在任意环境中预测物体位姿。</p><p>作者认为一个位姿估计器应该具有以下特点：</p><ul><li>通用性：可以应用于任意物体，而无需对对象或类别进行训练</li><li>无模型：用于一个未见过的物体时，只需要一些已知姿态的参考图像来定义物体参考坐标系即可</li><li>输入简单：仅输入RGB图像来估计位姿，而不需要深度图或物体掩码图</li></ul><p><strong>（1）如何设计视角选择器，从参考图像中找到与查询图像视角最接近的</strong></p><p>本文使用神经网络对查询图像和参考图像进行逐像素比较，产生相似性得分，并选择具有最高相似性得分的参考图像。并添加了全局归一化层和自注意层来共享不同参考图像之间的相似性信息，为选择最相似的参考图像提供了上下文信息。</p><p><strong>（2）实现没有模型的姿态优化</strong></p><p>本文提出了一种新的基于三维空间的姿态优化方法，给定一个查询图像和一个输入姿态，找到几个接近输入姿态的参考图像，将这些参考图像投影回3D空间中，构建特征空间，通过3D的CNN将构建的特征空间与查询图像的特征相匹配，来优化姿态。</p><h3 id="1-2-现有工作"><a href="#1-2-现有工作" class="headerlink" title="1.2 现有工作"></a>1.2 现有工作</h3><p>现有位姿估计方法大都是基于特定实例的，不能推广到未见过的物体，通常都需要根据物体3D模型来渲染大量图像进行训练。有一些方法可以推广到类别级，也不需要对象的模型，但仍然无法预测没见过的类别的物体。</p><h2 id="2-实现方法"><a href="#2-实现方法" class="headerlink" title="2. 实现方法"></a>2. 实现方法</h2><p><strong>数据规范化</strong>：对于每个物体，通过对参考图像中的点进行三角测量等方法估计物体的大致大小，然后对物体坐标系进行归一化，使物体中心位于原点，大小为1，此时物体位于原点的单位球体内。</p><p>Gen6D包括一个物体检测器，一个视角选择器，一个姿态优化器。</p><p><img src="https://img.mahaofei.com/img/20230403203020.png" alt="image.png"></p><p>物体检测其首先利用查询图像和参考图像来检测物体所在区域。然后视角选择器将查询图像于参考图像相匹配，产生粗略的初始姿态。最后由姿态优化器进一步细化以得到精确的对象姿态。</p><h3 id="2-1-物体检测"><a href="#2-1-物体检测" class="headerlink" title="2.1 物体检测"></a>2.1 物体检测</h3><p>将检测问题分解成两部分</p><ol><li>找到对象中心的2D投影点q</li><li>估计包围单位球体的正方形边界框。</li></ol><p><img src="https://img.mahaofei.com/img/20230403204751.png" alt="image.png"></p><p>物体中心的深度可以使用$d=2f/S_q$求得，其中2是单位球体的直径，f是虚拟焦距（将主点设为投影点q），$S_q$是边界框边长。这就是物体的初始平移。</p><blockquote><p>问题：这里将物体归一化之后求出的深度d还是真实深度吗？虚拟焦距又是如何确定的？</p></blockquote><p>检测器使用了VGG网络提取参考图像和查询图像的特征图，然后将所有参考图像的特征图作为卷积核与查询图像的特征图卷积，得到分数图。考虑尺度差异，设置再多个预定义尺度上进行卷积，最后得到热力图和比例图。选择热力图上的最大值位置作为对象中心2D投影，使用比例图上相同比例的比例作为边界框的大小$S_q=S_r*s$。</p><blockquote><p>问题：这里将所有参考图像的特征图都进行卷积，那么参考图像上物体特征和背景特征是如何区分的？</p></blockquote><h3 id="2-2-视角选择"><a href="#2-2-视角选择" class="headerlink" title="2.2 视角选择"></a>2.2 视角选择</h3><p>将查询图像与每个参考图像比较，计算相似性得分。计算每个参考图像和查询图像的元素乘积，获得得分图，并计算相似性参数。</p><p><img src="https://img.mahaofei.com/img/20230404192903.png" alt="image.png"></p><p><strong>（1）平面内旋转</strong><br>为了考虑平面内旋转，本文将参考图像旋转Na个预定义角度，查询时使用所有旋转版本进行逐元素乘积。</p><p><strong>（2）全局归一化</strong><br>使用参考图像的所有特征图计算的均值和方差，对相似度网络生成的特征图进行归一化。这样做可以用特征图的分布来编码上下文相似性，并放大不同图像之间的相似性差异。</p><p><strong>（3）参考视角变换</strong><br>在所有参考图像的相似性特征向量上应用变换，包括它们的视角、注意力层。这样的变换器使得特征向量相互通信以编码上下文信息，有助于确定最相似的参考图像。</p><h3 id="2-3-姿态优化"><a href="#2-3-姿态优化" class="headerlink" title="2.3 姿态优化"></a>2.3 姿态优化</h3><p>经过上面两个步骤，我们已经有了粗略的物体位姿。本步骤对位姿进行优化。</p><p>选择接近输入姿态的6个参考图像，通过CNN提取特征图，然后将特征图投影到3D空间中，并计算特征的均值和方差作为空间顶点的特征。<br>对于查询图像，使用同样的CNN提取特征图，将特征图投影到3D空间中，并将查询特征与参考图像特征的均值和方差连接起来。</p><p>最后在空间特征上使用3DCNN预测残差来更新输入姿态。</p><p><img src="https://img.mahaofei.com/img/20230404195340.png" alt="image.png"></p><h1 id="3-实验分析"><a href="#3-实验分析" class="headerlink" title="3. 实验分析"></a>3. 实验分析</h1><h1 id="二、算法复现"><a href="#二、算法复现" class="headerlink" title="二、算法复现"></a>二、算法复现</h1><h2 id="2-1-环境搭建"><a href="#2-1-环境搭建" class="headerlink" title="2.1 环境搭建"></a>2.1 环境搭建</h2><h3 id="2-1-1-Python环境"><a href="#2-1-1-Python环境" class="headerlink" title="2.1.1 Python环境"></a>2.1.1 Python环境</h3><p>创建[[02_Anaconda的基本使用与在Pycharm中调用|Anaconda虚拟环境]]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gen6d python=3.7</span><br><span class="line">conda activate gen6d</span><br></pre></td></tr></table></figure><p>安装pytorch环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 -c pytorch</span><br></pre></td></tr></table></figure><p>安装依赖，打开<code>requirements.txt</code>，删除其中的pytorch, torchvision, cudatoolkit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="2-1-2-自制数据集工具"><a href="#2-1-2-自制数据集工具" class="headerlink" title="2.1.2 自制数据集工具"></a>2.1.2 自制数据集工具</h3><p><strong>（1）COLMAP</strong></p><p>参考官网教程：<a href="https://colmap.github.io/install.html">https://colmap.github.io/install.html</a></p><p>安装依赖库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-regex-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev \</span><br><span class="line">    libcgal-qt5-dev</span><br></pre></td></tr></table></figure><p>下载COLMAP源代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/colmap/colmap</span><br><span class="line">cd colmap</span><br></pre></td></tr></table></figure><p>修改<code>CMakeLists.txt</code>文件，添加下面的内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_CUDA_ARCHITECTURES &quot;70&quot;)</span><br></pre></td></tr></table></figure><p>开始编译、安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -GNinja</span><br><span class="line">ninja</span><br><span class="line">sudo ninja install</span><br></pre></td></tr></table></figure><p><strong>（2）CloudCompare</strong></p><p>安装Flatpak</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install flatpak</span><br></pre></td></tr></table></figure><p>安装Software Flatpak plugin</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-software-plugin-flatpak</span><br></pre></td></tr></table></figure><p>添加Flathub repository</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo</span><br></pre></td></tr></table></figure><p>安装CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak install flathub org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p>运行CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><strong>（3）安装ffmpeg</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ffmpeg</span><br></pre></td></tr></table></figure><h2 id="2-2-数据集准备"><a href="#2-2-数据集准备" class="headerlink" title="2.2 数据集准备"></a>2.2 数据集准备</h2><h3 id="2-2-1-官方数据集"><a href="#2-2-1-官方数据集" class="headerlink" title="2.2.1 官方数据集"></a>2.2.1 官方数据集</h3><p><strong>（1）下载数据集</strong></p><p>从<a href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EkWESLayIVdEov4YlVrRShQBkOVTJwgK0bjF7chFg2GrBg?e=Y8UpXu">原作者给出的链接</a>中下载预训练模型，GenMOP数据集和processed LINEMOD数据集。</p><p><strong>（2）组织数据集</strong></p><p>将下载的文件按照下面的格式进行整理。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- model</span><br><span class="line">        |-- detector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- selector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- refiner_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br></pre></td></tr></table></figure><h3 id="2-2-2-自制数据集"><a href="#2-2-2-自制数据集" class="headerlink" title="2.2.2 自制数据集"></a>2.2.2 自制数据集</h3><p><strong>（1）视频录制</strong></p><p>使用手机录制目标物体的参考视频和测试视频。注意：参考视频需要满足以下条件</p><ul><li>参考视频中对象是静态的</li><li>参考视频中背景尽可能纹理丰富且平整，摄像角度要尽可能覆盖每个角度，以便COLMAP恢复相机姿态</li></ul><p><strong>（2）组织文件</strong></p><p>将视频按照下面的路径进行组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- video</span><br><span class="line">           |-- mouse-ref.mp4</span><br><span class="line">           |-- mouse-test.mp4</span><br></pre></td></tr></table></figure><p><strong>（3）将参考视频拆分为图像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 每10帧保存一张图像，最大图像边长为960</span></span></span><br><span class="line">python prepare.py --action video2image \</span><br><span class="line">                  --input data/custom/video/ref/coffeebox-ref.mp4 \</span><br><span class="line">                  --output data/custom/coffeebox/images \</span><br><span class="line">                  --frame_inter 10 \</span><br><span class="line">                  --image_size 960 \</span><br><span class="line">                  --transpose</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 或者</span></span></span><br><span class="line">python prepare.py --action video2image --input data/custom/video/ammeter-ref.mp4 --output data/custom/ammeter/images --frame_inter 10 --image_size 960 --transpose</span><br></pre></td></tr></table></figure><p>拆分后的视频保存在<code>data/custom/mouse/images</code>中。</p><p><strong>（4）运行COLMAP SfM恢复相机姿态</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action sfm --database_name custom/ammeter --colmap &lt;path-to-your-colmap-exe&gt;</span><br></pre></td></tr></table></figure><p>注：<code>&lt;path-to-your-colmap-exe&gt;</code>可以通过命令<code>which colmap</code>来查找，一般ubuntu路径为<code>/usr/local/bin/colmap</code>，windows路径为<code>E:/Programming/COLMAP-3.8-windows-cuda/COLMAP.bat</code></p><p><strong>（5）手动处理点云</strong></p><p>通过裁减对象点云来手动确定对象所在区域。例如使用<a href="https://www.cloudcompare.org/">CloudCompare</a>来可视化处理COLMAP重建的点云，重建的点云位于<code>data/custom/mouse/colmap/pointcloud.ply</code>中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230327215520.png" alt=""></p><p>导出裁剪后的点云为<code>data/custom/mouse/object_point_cloud.ply</code>。</p><p><img src="https://img.mahaofei.com/img/20230327220042.png" alt=""></p><p><strong>（6）手动确定对象的X轴正方向和Y轴正方向</strong></p><p><img src="https://img.mahaofei.com/img/20230327220221.png" alt=""></p><p><img src="https://img.mahaofei.com/img/20230327220225.png" alt=""></p><p>编辑一个<code>data/custom/mouse/meta_info.txt</code>文件来保存你的X+和Z+信息，例如</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.297052 0.350839 -0.000593</span><br><span class="line">0.973488 0.054352 -0.222188</span><br></pre></td></tr></table></figure><p><strong>（7）确保您具有以下文件，这些文件由上述步骤生成</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- mouse</span><br><span class="line">           |-- object_point_cloud.ply  ## object point cloud</span><br><span class="line">           |-- meta_info.txt           ## meta information about z+/x+ directions</span><br><span class="line">           |-- images                  ## images</span><br><span class="line">           |-- colmap                  ## colmap project</span><br></pre></td></tr></table></figure><p><strong>（8）从处理后的参考图像中预测姿势</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python predict.py --cfg configs/gen6d_pretrain.yaml \</span><br><span class="line">                  --database custom/coffeebox_lied \</span><br><span class="line">                  --video data/custom/video/coffeebox-test.mp4 \</span><br><span class="line">                  --resolution 960 \</span><br><span class="line">                  --transpose \</span><br><span class="line">                  --output data/custom/ammeter_processed/test \</span><br><span class="line">                  --ffmpeg &lt;path-to-ffmpeg-exe&gt;</span><br></pre></td></tr></table></figure><h2 id="2-3-训练与评估"><a href="#2-3-训练与评估" class="headerlink" title="2.3 训练与评估"></a>2.3 训练与评估</h2><p>将文件按照下面的形式组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br><span class="line">    |-- shapenet</span><br><span class="line">        |-- shapenet_cache</span><br><span class="line">        |-- shapenet_render</span><br><span class="line">        |-- shapenet_render_v1.pkl</span><br><span class="line">    |-- co3d_256_512</span><br><span class="line">        |-- apple</span><br><span class="line">            ...</span><br><span class="line">    |-- google_scanned_objects</span><br><span class="line">        |-- 06K3jXvzqIM</span><br><span class="line">            ...</span><br><span class="line">    |-- coco</span><br><span class="line">        |-- train2017</span><br></pre></td></tr></table></figure><h3 id="2-3-1-训练detector"><a href="#2-3-1-训练detector" class="headerlink" title="2.3.1 训练detector"></a>2.3.1 训练detector</h3><p>修改<code>train_meta_info.py</code>的第86行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;genmop_train&#x27;</span>: [<span class="string">f&#x27;genmop/<span class="subst">&#123;name&#125;</span>-test&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">&#x27;ammeter&#x27;</span>, <span class="string">&#x27;coffeebox&#x27;</span>, <span class="string">&#x27;realsensebox&#x27;</span>]],</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第109行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">genmop_meta_info=&#123;</span><br><span class="line">    <span class="string">&#x27;ammeter&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0222805</span>, -<span class="number">0.409031</span>, <span class="number">0.912248</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.401556</span>, <span class="number">0.773825</span>, <span class="number">0.340199</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;coffeebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0718405</span>, -<span class="number">0.471545</span>, <span class="number">0.878911</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.582604</span>, -<span class="number">0.490501</span>, -<span class="number">0.219265</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;realsensebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.103463</span>, -<span class="number">0.521284</span>, <span class="number">0.847088</span>],np.float32), <span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">1.690831</span>, <span class="number">0.688506</span>, <span class="number">0.590004</span>],np.float32)&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第212行，修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras, images, points3d = read_model(<span class="string">f&#x27;<span class="subst">&#123;GenMOP_ROOT&#125;</span>/<span class="subst">&#123;seq_name&#125;</span>/colmap/sparse/0&#x27;</span>)</span><br></pre></td></tr></table></figure><p>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/detector/detector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-2-训练selector"><a href="#2-3-2-训练selector" class="headerlink" title="2.3.2 训练selector"></a>2.3.2 训练selector</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/selector/selector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-3-训练refiner"><a href="#2-3-3-训练refiner" class="headerlink" title="2.3.3 训练refiner"></a>2.3.3 训练refiner</h3><p>为refiner训练进行数据准备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database linemod/cat \</span><br><span class="line">                  --que_split linemod_val \</span><br><span class="line">                  --ref_database linemod/cat \</span><br><span class="line">                  --ref_split linemod_val</span><br><span class="line"></span><br><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database genmop/tformer-test \</span><br><span class="line">                  --que_split all \</span><br><span class="line">                  --ref_database genmop/tformer-ref \</span><br><span class="line">                  --ref_split all </span><br></pre></td></tr></table></figure><p>该命令会在<code>data/val</code>生成信息，该信息会被用于生成refiner的有效数据</p><p>训练refiner</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/refiner/refiner_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-4-评估所有组件"><a href="#2-3-4-评估所有组件" class="headerlink" title="2.3.4 评估所有组件"></a>2.3.4 评估所有组件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object TFormer from the GenMOP dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name genmop/tformer</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object <span class="built_in">cat</span> from the LINEMOD dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name linemod/cat</span><br></pre></td></tr></table></figure><h1 id="三、现存问题"><a href="#三、现存问题" class="headerlink" title="三、现存问题"></a>三、现存问题</h1><p><strong>优点</strong></p><ol><li>只需要对给定物体录制1-2分钟的视频，使用程序1-2小时<strong>添加数据集</strong>，即可实现新物体的位姿估计，不需要再训练网络</li><li><strong>精度</strong>还可以</li></ol><p><strong>缺点</strong></p><ol><li>对于<strong>方形凸形物体识别较好，对于物体内部存在中空区域</strong>，例如圆环等物体识别效果较差</li><li>由于<strong>参考视频要求物体静止，因此无法录到物体底面的特征</strong>，对于物体底面识别效果较差（可考虑物体正反放置录制两次，对于同一个物体使用两个参考视频进行预测，选择置信度高的位姿）</li><li>当进行识别时，如果<strong>图像中不存在物体也会生成一个估计位姿</strong>（可以考虑根据置信度判断输出，或者在位姿估计前使用yolo等算法预判断物体位置）</li><li>当存在<strong>遮挡时位姿估计效果较差</strong>，可能会出现只框处未被遮挡的部分，或者在遮挡物体上强行进行位姿估计。</li><li>当要同时识别的物体很多时，对于显卡显存要求比较大，而且计算会很慢，服务器1.5s/it。如果每次只对某个特定物体进行识别，速度还可以。</li></ol>]]></content>
    
    
    <summary type="html">算法复现</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="Gen6D" scheme="https://www.mahaofei.com/tags/Gen6D/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】GDRNPP算法</title>
    <link href="https://www.mahaofei.com/post/250dc866.html"/>
    <id>https://www.mahaofei.com/post/250dc866.html</id>
    <published>2023-03-27T01:23:53.000Z</published>
    <updated>2023-04-01T01:23:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation"><a href="#一、GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation" class="headerlink" title="一、GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation"></a>一、GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</h1><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Gu Wang,  Tsinghua University, BNRist<br><strong>关键词</strong>：位姿估计；端到端<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/THU-DA-6D-Pose-Group/GDR-Net">https://github.com/THU-DA-6D-Pose-Group/GDR-Net</a></p></blockquote><h2 id="1-目标问题"><a href="#1-目标问题" class="headerlink" title="1 目标问题"></a>1 目标问题</h2><p>提出一种端到端的位姿估计算法。</p><h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><p><img src="https://img.mahaofei.com/img/20230316104020.png" alt=""></p><p><strong>（1）网络架构</strong></p><p>首先向GDR-Net提供256x256的ROI图，预测出三个64x64的中间特征图</p><ul><li>稠密对应图$M_{2D-3D}$：将稠密坐标映射$M_{XYZ}$对跌倒2D像素坐标上得到，反映了对象的几何形状信息。</li><li>表面区域注意图$M_{SRA}$：采用最远点采样从$M_{XYZ}$中到处，代表了对象的对称性。</li><li>可见对象掩码$M_{vis}$</li></ul><p>使用一个简单的2D卷积Patch Pnp模块直接从特征图中回归6D对象姿态。Patch PnP模块由三个卷积层组成，然后用两个全连接层用于扁平化特征，最后连个全连接层输出R6D旋转和tSITE平移。</p><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h2><p>本文专注于图像的特征提取和处理工作，实现从单一图片预测位姿的功能。</p><h1 id="二、算法复现"><a href="#二、算法复现" class="headerlink" title="二、算法复现"></a>二、算法复现</h1><h2 id="2-1-数据集准备"><a href="#2-1-数据集准备" class="headerlink" title="2.1 数据集准备"></a>2.1 数据集准备</h2><p>下载<a href="https://bop.felk.cvut.cz/datasets/">BOP数据集</a>和<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">VOC2012数据集</a>，从<a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EgOQzGZn9A5DlaQhgpTtHBwB2Bwyx8qmvLauiHFcJbnGSw?e=EZ60La">Onedrive (password: groupji)</a>或者<a href="https://pan.baidu.com/s/1FzTO4Emfu-DxYkNG40EDKw">百度网盘(密码: vp58)</a>下载test_boxes，完成后datasets文件夹的结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">datasets/</span><br><span class="line">├── BOP_DATASETS   # https://bop.felk.cvut.cz/datasets/</span><br><span class="line">    ├──tudl</span><br><span class="line">    ├──lmo</span><br><span class="line">    ├──ycbv</span><br><span class="line">    ├──icbin</span><br><span class="line">    ├──hb</span><br><span class="line">    ├──itodd</span><br><span class="line">    └──tless</span><br><span class="line">└──VOCdevkit</span><br></pre></td></tr></table></figure><p>从<a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EgOQzGZn9A5DlaQhgpTtHBwB2Bwyx8qmvLauiHFcJbnGSw?e=EZ60La">Onedrive (password: groupji)</a>或者<a href="https://pan.baidu.com/s/1LhXblEic6pYf1i6hOm6Otw">百度网盘(密码10t3)</a>下载预训练模型，并将其放到<code>./output</code>文件夹内。</p><h2 id="2-2-环境准备"><a href="#2-2-环境准备" class="headerlink" title="2.2 环境准备"></a>2.2 环境准备</h2><p>要求Ubuntu 18.04/20.04, CUDA 10.1/10.2/11.6, python &gt;= 3.7, PyTorch &gt;= 1.9, torchvision</p><p><strong>（1）创建虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gdrnpp python=3.7</span><br><span class="line">conda activate grdnpp</span><br></pre></td></tr></table></figure><p><strong>（2）安装依赖</strong></p><p>打开<code>requirements/requirement.txt</code>，修改第48行为<code>pytorch-lightning==1.6.0</code></p><p>运行<code>sh scripts/install_deps.sh</code></p><p><strong>（3）从<a href="https://github.com/facebookresearch/detectron2">源码</a>安装detectron2</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install &#x27;git+https://github.com/facebookresearch/detectron2.git&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">(add --user <span class="keyword">if</span> you don<span class="string">&#x27;t have permission)</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Or, to install it from a local clone:</span></span></span><br><span class="line">git clone https://github.com/facebookresearch/detectron2.git</span><br><span class="line">python -m pip install -e detectron2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">On macOS, you may need to prepend the above commands with a few environment variables:</span></span></span><br><span class="line">CC=clang CXX=clang++ ARCHFLAGS=&quot;-arch x86_64&quot; python -m pip install ...</span><br></pre></td></tr></table></figure><p><strong>（4）编译 fps 的cpp扩展</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh core/csrc/compile.sh</span><br></pre></td></tr></table></figure><p><strong>（5）编译egl_renderer的cpp扩展</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh lib/egl_renderer/compile_cpp_egl_renderer.sh</span><br></pre></td></tr></table></figure><h2 id="2-3-目标检测算法"><a href="#2-3-目标检测算法" class="headerlink" title="2.3 目标检测算法"></a>2.3 目标检测算法</h2><p>从 <a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EkCTrRfHUZVEtD7eHwLkYSkBCTXlh9ekDteSzK6jM4oo-A?e=m0aNCy">Onedrive</a> (password: groupji) or <a href="https://pan.baidu.com/s/1AU7DGCmZWsH9VgQnbTRjow">BaiDuYunPan</a>(password: aw68)中下载预训练模型。</p><p><strong>（1）训练</strong></p><p><code>./det/yolox/tools/train_yolox.sh &lt;config_path&gt; &lt;gpu_ids&gt; (other args)</code></p><p><strong>（2）测试</strong></p><p><code>./det/yolox/tools/test_yolox.sh &lt;config_path&gt; &lt;gpu_ids&gt; &lt;ckpt_path&gt; (other args)</code></p><h2 id="2-4-位姿估计算法"><a href="#2-4-位姿估计算法" class="headerlink" title="2.4 位姿估计算法"></a>2.4 位姿估计算法</h2><p><strong>（1）训练</strong></p><p>打开<code>core/gdrn_modeling/datasets/lm_pbr.py</code>，注释第190行<code>assert osp.exists(xyz_path), xyz_path</code></p><p><code>./core/gdrn_modeling/train_gdrn.sh &lt;config_path&gt; &lt;gpu_ids&gt; (other args)</code></p><p>例如：</p><p><code>./core/gdrn_modeling/train_gdrn.sh configs/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv.py 0</code></p><p><strong>（2）测试</strong></p><p><code>./core/gdrn_modeling/test_gdrn.sh &lt;config_path&gt; &lt;gpu_ids&gt; &lt;ckpt_path&gt; (other args)</code></p><p>例如：</p><p><code>./core/gdrn_modeling/test_gdrn.sh configs/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv.py 0 output/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv/model_final_wo_optim.pth</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation&quot;&gt;&lt;a href=&quot;#一、GDR-Net-Geometry-Guided-Dir</summary>
      
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="GDRNPP" scheme="https://www.mahaofei.com/tags/GDRNPP/"/>
    
  </entry>
  
  <entry>
    <title>TensorBoard的使用丨深度学习曲线生成</title>
    <link href="https://www.mahaofei.com/post/6db9da8f.html"/>
    <id>https://www.mahaofei.com/post/6db9da8f.html</id>
    <published>2023-03-23T14:14:20.000Z</published>
    <updated>2023-03-23T14:14:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TensorBoard的安装"><a href="#TensorBoard的安装" class="headerlink" title="TensorBoard的安装"></a>TensorBoard的安装</h1><p>要求Pytorch版本必须在1.2.0以上。</p><p>使用下面的命令安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br></pre></td></tr></table></figure><h1 id="TensorBoard的代码调用"><a href="#TensorBoard的代码调用" class="headerlink" title="TensorBoard的代码调用"></a>TensorBoard的代码调用</h1><p><strong>（1）导入包，并创建TensorBoard回调对象</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> TensorBoard</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs/learning_rate_scheduler&quot;</span>) <span class="comment">#指定TensorBoard日志目录</span></span><br></pre></td></tr></table></figure><p><strong>（2）在模型的训练过程中导入回调</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">global_step = <span class="number">0</span> <span class="comment"># 初始化 global_step 为 0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># 训练过程</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将学习率和训练损失添加到 TensorBoard</span></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Loss&#x27;</span>, loss, global_step=global_step)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Learning_Rate&#x27;</span>, lr, global_step=global_step)</span><br><span class="line">global_step += <span class="number">1</span>  <span class="comment"># 为每个batch更新 global_step 计数器</span></span><br></pre></td></tr></table></figure><h1 id="查看曲线"><a href="#查看曲线" class="headerlink" title="查看曲线"></a>查看曲线</h1><p>训练开始后，打开一个终端，输入下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir logs/learning_rate_scheduler</span><br></pre></td></tr></table></figure><p>然后打开浏览器的<a href="http://localhost:6006/">http://localhost:6006/</a>页面，就可以看到曲线。</p><p><img src="https://img.mahaofei.com/img/20230323221410.png" alt=""></p>]]></content>
    
    
    <summary type="html">在深度学习训练过程中，我们必定会需要观察系统的Loss、Learning_rate等参数的变化，因此实时绘制曲线图是十分有必要的。本文就介绍了如何利用Pytorch的TensorBoard绘制曲线图。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="深度学习基础" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】ECCV2020-2022 6D位姿估计相关论文</title>
    <link href="https://www.mahaofei.com/post/f0f72f50.html"/>
    <id>https://www.mahaofei.com/post/f0f72f50.html</id>
    <published>2023-03-21T11:20:13.000Z</published>
    <updated>2023-03-21T11:20:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ECCV2020"><a href="#ECCV2020" class="headerlink" title="ECCV2020"></a>ECCV2020</h1><h2 id="01-CosyPose-Consistent-multi-view-multi-object-6D-pose-estimation"><a href="#01-CosyPose-Consistent-multi-view-multi-object-6D-pose-estimation" class="headerlink" title="01. CosyPose: Consistent multi-view multi-object 6D pose estimation"></a>01. CosyPose: Consistent multi-view multi-object 6D pose estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2020<br><strong>作者 / 机构</strong>：Yann Labbe,  Ecole normale superieure, CNRS, PSL Research University, Paris, France<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ylabbe/cosypose">https://github.com/ylabbe/cosypose</a></p></blockquote><h4 id="1-目标问题"><a href="#1-目标问题" class="headerlink" title="1 目标问题"></a>1 目标问题</h4><p>在相机位置未知的情况下，利用多视角信息来提高物体姿态估计的准确性和鲁棒性</p><h4 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h4><p><strong>（1）建立6D姿态初始候选对象</strong></p><p>给定一组具有已知3D模型的对象和场景的单个图像，我们为每个对象输出一组候选检测，并为每个检测输出对象相对于与图像相关联的相机的6D姿态。</p><p><strong>（2）对象候选匹配</strong></p><p>匹配多个视图中可见的对象，以获得单个一致的场景。</p><p><strong>（3）全局场景细化</strong></p><p>所有物体和相机的6D姿态都经过了优化，以最大限度地减少全局重投影误差。</p><h4 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h4><p><strong>（1）创新点</strong></p><ul><li>提出了一种基于渲染和比较的单视角单物体6D姿态估计方法，用于生成每个图像中的物体姿态假设。</li><li>开发了一种基于RANSAC的鲁棒方法，用于匹配不同图像中的单个物体姿态假设，并利用这些对象级别的对应关系来恢复相机之间的相对位置。</li><li>开发了一种基于对象级别捆绑调整（object-level bundle adjustment）的全局优化方法，用于在所有视角下最小化重投影误差，并改善噪声单视角物体姿态。</li></ul><p><strong>（2）实用性</strong></p><p>从多个视图中推测物体的6D位姿，对于抓取场景实用性较差。</p><h1 id="ECCV2022"><a href="#ECCV2022" class="headerlink" title="ECCV2022"></a>ECCV2022</h1><h2 id="01-DProST-Dynamic-Projective-Spatial-Transformer-Network-for-6D-Pose-Estimation"><a href="#01-DProST-Dynamic-Projective-Spatial-Transformer-Network-for-6D-Pose-Estimation" class="headerlink" title="01. DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation"></a>01. DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：<br><strong>关键词</strong>：<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/parkjaewoo0611/DProST">https://github.com/parkjaewoo0611/DProST</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-1"><a href="#1-目标问题-1" class="headerlink" title="1 目标问题"></a>1 目标问题</h4><p>提出了一种新的基于投影网格的姿态估计系统。</p><p><img src="https://img.mahaofei.com/img/20230402093533.png" alt=""></p><h4 id="2-方法-1"><a href="#2-方法-1" class="headerlink" title="2 方法"></a>2 方法</h4><ul><li>使用深度神经网络从RGB图像中提取特征，并预测物体位置和大小。</li><li>在相机空间上根据预测位置和大小生成一个锥形光束网格，并将其反向变换到物体空间。</li><li>使用参考图像和掩码从物体模型或重建特征中提取纹理特征，并将其映射到变换后的网格上。</li><li>使用双线性插值从映射后的纹理特征中采样得到重建图像，并与输入图像进行比较。</li><li>使用基于网格距离和网格匹配损失函数来优化网络参数和姿态参数。</li></ul><h4 id="3-思考-1"><a href="#3-思考-1" class="headerlink" title="3 思考"></a>3 思考</h4><p>深度神经网络提取特征，投影网格重建图像，使用损失函数优化参数。</p><h2 id="02-DCL-Net-Deep-Correspondence-Learning-Network-for6D-Pose-Estimation"><a href="#02-DCL-Net-Deep-Correspondence-Learning-Network-for6D-Pose-Estimation" class="headerlink" title="02. DCL-Net: Deep Correspondence Learning Network for6D Pose Estimation"></a>02. DCL-Net: Deep Correspondence Learning Network for6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Hongyang Li, South China University of Technology, Guangzhou, China<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/Gorilla-Lab-SCUT/DCL-Net">https://github.com/Gorilla-Lab-SCUT/DCL-Net</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-2"><a href="#1-目标问题-2" class="headerlink" title="1 目标问题"></a>1 目标问题</h4><p>从点对应关系中直接估计6D物体姿态，而不是使用间接的对应学习目标</p><h4 id="2-方法-2"><a href="#2-方法-2" class="headerlink" title="2 方法"></a>2 方法</h4><p>这篇论文的主要方法是提出了一种新的深度对应学习网络（DCL-Net），它利用双重特征解耦和对齐（FDA）模块，在特征空间中建立相机坐标系和物体坐标系之间的部分到部分对应关系和完整到完整对应关系。具体步骤如下：</p><ul><li>首先，对于部分物体观测和其CAD模型，分别提取它们的点特征图；</li><li>然后，设计两个FDA模块，分别建立部分到部分对应关系和完整到完整对应关系。具体来说，每个FDA模块将两个点特征图作为输入，并将每个特征图解耦为独立的姿态特征图和匹配特征图；然后利用匹配特征图学习一个注意力图来建立深度对应关系；最后，根据注意力图将两个系统的姿态特征图和匹配特征图进行对齐和配对，得到姿态特征对和匹配特征对；</li><li>接着，将两个FDA模块得到的两组对应关系进行融合，因为它们具有互补优势；然后利用融合后的匹配特征对学习置信度得分来衡量深度对应关系的质量；同时利用置信度得分加权融合后的姿态特征对来直接回归物体姿态；</li><li>最后，提出了一个基于置信度的姿态优化网络来进一步迭代地提高姿态精度。</li></ul><h4 id="3-思考-2"><a href="#3-思考-2" class="headerlink" title="3 思考"></a>3 思考</h4><p>点特征方法。</p><h2 id="03-Perspective-Flow-Aggregation-for-Data-Limited-6D-Object-Pose-Estimation"><a href="#03-Perspective-Flow-Aggregation-for-Data-Limited-6D-Object-Pose-Estimation" class="headerlink" title="03. Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation"></a>03. Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Yinlin Hu, EPFL CVLab, Lausanne, Switzerland<br><strong>关键词</strong>：位姿估计；少数据情况<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/cvlab-epfl/perspective-flow-aggregation">https://github.com/cvlab-epfl/perspective-flow-aggregation</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-3"><a href="#1-目标问题-3" class="headerlink" title="1 目标问题"></a>1 目标问题</h4><p>在数据有限的情况下，使用合成图像或少量真实图像来训练一个6D物体姿态估计的模型</p><h4 id="2-方法-3"><a href="#2-方法-3" class="headerlink" title="2 方法"></a>2 方法</h4><ul><li>首先，利用合成图像和真实图像（如果有的话）来训练一个基于深度学习的特征提取器，用于从输入图像中提取出与物体姿态相关的特征。</li><li>然后，利用合成图像和真实图像（如果有的话）来训练一个基于透视流（perspective flow）的模块，用于将输入图像中的特征点映射到目标物体模型上。透视流是指由于相机和物体之间相对运动而导致的特征点在不同视角下的位移。</li><li>最后，利用一种称为透视流聚合（perspective flow aggregation）的技术，将多个透视流进行融合，并通过最小二乘法求解出最优的6D物体姿态。</li></ul><h4 id="3-思考-3"><a href="#3-思考-3" class="headerlink" title="3 思考"></a>3 思考</h4><p>投影透视方法。</p><h2 id="04-Learning-Based-Point-Cloud-Registration-for-6D-Object-Pose-Estimation-in-the-Real-World"><a href="#04-Learning-Based-Point-Cloud-Registration-for-6D-Object-Pose-Estimation-in-the-Real-World" class="headerlink" title="04. Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World"></a>04. Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Zheng Dang, CVLab, EPFL, Lausanne, Switzerland<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/AnsonYanxin/MatchNorm">https://github.com/AnsonYanxin/MatchNorm</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-4"><a href="#1-目标问题-4" class="headerlink" title="1 目标问题"></a>1 目标问题</h4><h4 id="2-方法-4"><a href="#2-方法-4" class="headerlink" title="2 方法"></a>2 方法</h4><ul><li>首先，它提出了一种基于深度学习的点云匹配模块，用于从源点云和目标点云中提取特征，并计算两个点云之间的相似度矩阵。</li><li>然后，它提出了一种基于归一化的点云对齐模块，用于根据相似度矩阵找到最佳的刚性变换矩阵，使得源点云和目标点云之间的距离最小化</li></ul><h4 id="3-思考-4"><a href="#3-思考-4" class="headerlink" title="3 思考"></a>3 思考</h4><p>代码不完全。</p>]]></content>
    
    
    <summary type="html">检索阅读近三年ECCV6D位姿估计相关论文并进行记录</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="ECCV" scheme="https://www.mahaofei.com/tags/ECCV/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】CVPR2020-2022 6D位姿估计相关论文</title>
    <link href="https://www.mahaofei.com/post/afed67af.html"/>
    <id>https://www.mahaofei.com/post/afed67af.html</id>
    <published>2023-03-15T01:29:33.000Z</published>
    <updated>2023-03-15T01:29:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CVPR-2020"><a href="#CVPR-2020" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h1><h2 id="01-HybridPose-6D-Object-Pose-Estimation-under-Hybrid-Representations"><a href="#01-HybridPose-6D-Object-Pose-Estimation-under-Hybrid-Representations" class="headerlink" title="01. HybridPose: 6D Object Pose Estimation under Hybrid Representations"></a>01. HybridPose: 6D Object Pose Estimation under Hybrid Representations</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Chen Song, The University of Texas at Austin<br><strong>关键词</strong>：位姿估计；混合特征<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/chensong1995/HybridPose">https://github.com/chensong1995/HybridPose</a></p></blockquote><h3 id="1-目标问题"><a href="#1-目标问题" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>6D位姿估计</p><h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h3><p><img src="https://img.mahaofei.com/img/20230315170643.png" alt=""></p><p>算法由中间特征预测网络和姿态回归网络组成：</p><p><strong>（1）预测模块</strong></p><p>将图像作为输入，用三个预测网络输出预测的关键点、边缘向量和对称对应关系</p><ul><li>关键点：利用PVNet的关键点预测方法</li><li>边缘向量：每两个关键点之间的向量</li><li>对称对应关系：扩展了FlowNet网络，结合了像素流和语义掩码</li></ul><p><strong>（2）姿态回归模块</strong></p><p>姿态回归网络：包括初始化子模块和优化子模块</p><ul><li>初始化子模块：使用中间特征回归初始姿态</li><li>优化子模块：使用GM鲁棒范数并优化，获得最终姿态</li></ul><h3 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h3><p>方法比较直观，使用关键点、关键点向量和对称关系进行姿态预测。</p><p>但是实际应用比较困难，训练前需要使用FSP生成关键点标签、使用SymSeg生成对称性标签，并且还要提供分割模板。而且还需要PVNet的融合数据。</p><hr><h2 id="02-Single-Stage-6D-Object-Pose-Estimation"><a href="#02-Single-Stage-6D-Object-Pose-Estimation" class="headerlink" title="02. Single-Stage 6D Object Pose Estimation"></a>02. Single-Stage 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Yinlin Hu, CVLab, EPFL, Switzerland<br><strong>关键词</strong>：位姿估计；单阶段<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/cvlab-epfl/single-stage-pose">https://github.com/cvlab-epfl/single-stage-pose</a></p></blockquote><h3 id="1-目标问题-1"><a href="#1-目标问题-1" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>提出一种单阶段框架，解决两阶段框架（先建立3D对象关键点和2D图像的对应关系然后回归）的缺点，加快训练速度。</p><h3 id="2-方法-1"><a href="#2-方法-1" class="headerlink" title="2 方法"></a>2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316095154.png" alt=""></p><p>通过一些实例分割网络建立了3D物体和2D图像的关系后，使用三个主要模块来直接从这些对应聚类预测位姿：</p><ul><li>局部特征提取模块</li><li>特征聚合模块：在不同聚类中聚合特征</li><li><p>全局推理模块：有全连接层组成，用于将最终姿态估计为四元数和平移</p></li><li><p>提出了一种新颖的<strong>投影分布</strong>（Projection Distribution）表示法，将三维物体关键点在二维图像上的投影建模为一个概率分布，而不是一个确定的位置。</p></li><li>设计了一个<strong>单阶段6D姿态估计网络</strong>（Single-Stage 6D Pose Estimation Network），利用卷积神经网络和全连接层来预测每个物体关键点在图像上的投影分布参数。</li><li>采用了一种<strong>最大似然估计</strong>（Maximum Likelihood Estimation）方法，根据预测的投影分布参数和已知的三维物体模型来直接计算物体在相机坐标系下的旋转矩阵和平移向量。</li></ul><h3 id="3-思考-1"><a href="#3-思考-1" class="headerlink" title="3 思考"></a>3 思考</h3><p>似乎需要与其它网络结合，从其他网络的中间层进行特征提取。</p><p>Github资料较少。</p><hr><h2 id="03-G2L-Net-Global-to-Local-Network-for-Real-time-6D-Pose-Estimation-with-Embedding-Vector-Features"><a href="#03-G2L-Net-Global-to-Local-Network-for-Real-time-6D-Pose-Estimation-with-Embedding-Vector-Features" class="headerlink" title="03. G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features"></a>03. G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Wei Chen, School of Computer Science, University of Birmingham<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/DC1991/G2L_Net">https://github.com/DC1991/G2L_Net</a></p></blockquote><h3 id="1-目标问题-2"><a href="#1-目标问题-2" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>提高位姿估计算法的准确度和速度。</p><h3 id="2-方法-2"><a href="#2-方法-2" class="headerlink" title="2 方法"></a>2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316101524.png" alt=""></p><p><strong>（1）全局定位</strong></p><p>使用2D检测器（例如yolo）来预测目标的边界框和标签，并将得到的概率图中的最大概率位置作为球体中心（结合深度图的3D坐标），来获得一个球体空间，减少后续3D搜索空间。</p><p><strong>（2）平移定位</strong></p><p>进行3D分割和平移残差预测，并将对象点的坐标系转换为局部规范坐标系。</p><p><strong>（3）旋转定位</strong></p><p>使用逐点嵌入向量特征提取器来提取嵌入向量特征，然后输入解码器回归出输入点云的旋转。</p><h3 id="3-思考-2"><a href="#3-思考-2" class="headerlink" title="3 思考"></a>3 思考</h3><p>相当于将DenseFusion的实例分割先验步骤进行了替换，使用了yolo+点云分割来代替。最后的特征还是逐点特征。</p><h1 id="CVPR2021"><a href="#CVPR2021" class="headerlink" title="CVPR2021"></a>CVPR2021</h1><h2 id="01-GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation"><a href="#01-GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation" class="headerlink" title="01. GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation"></a>01. GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Gu Wang,  Tsinghua University, BNRist<br><strong>关键词</strong>：位姿估计；端到端<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/THU-DA-6D-Pose-Group/GDR-Net">https://github.com/THU-DA-6D-Pose-Group/GDR-Net</a></p></blockquote><h3 id="1-目标问题-3"><a href="#1-目标问题-3" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>提出一种端到端的位姿估计算法。</p><h3 id="2-方法-3"><a href="#2-方法-3" class="headerlink" title="2 方法"></a>2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316104020.png" alt=""></p><p><strong>（1）网络架构</strong></p><p>首先向GDR-Net提供256x256的ROI图，预测出三个64x64的中间特征图</p><ul><li>稠密对应图$M_{2D-3D}$：将稠密坐标映射$M_{XYZ}$对跌倒2D像素坐标上得到，反映了对象的几何形状信息。</li><li>表面区域注意图$M_{SRA}$：采用最远点采样从$M_{XYZ}$中到处，代表了对象的对称性。</li><li>可见对象掩码$M_{vis}$</li></ul><p>使用一个简单的2D卷积Patch Pnp模块直接从特征图中回归6D对象姿态。Patch PnP模块由三个卷积层组成，然后用两个全连接层用于扁平化特征，最后连个全连接层输出R6D旋转和tSITE平移。</p><h3 id="3-思考-3"><a href="#3-思考-3" class="headerlink" title="3 思考"></a>3 思考</h3><p>本文专注于图像的特征提取和处理工作，实现从单一图片预测位姿的功能。方法不够直观。</p><h2 id="02-FS-Net-Fast-Shape-based-Network-for-Category-Level-6D-Object-Pose-Estimation-with-Decoupled-Rotation-Mechanism"><a href="#02-FS-Net-Fast-Shape-based-Network-for-Category-Level-6D-Object-Pose-Estimation-with-Decoupled-Rotation-Mechanism" class="headerlink" title="02. FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism"></a>02. FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Wei Chen, School of Computer Science, University of Birmingham<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/DC1991/FS_Net">https://github.com/DC1991/FS_Net</a></p></blockquote><h3 id="1-目标问题-4"><a href="#1-目标问题-4" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>解决以往类别级姿态特征提取效率低，精度和推理速度低的问题。</p><h3 id="2-方法-4"><a href="#2-方法-4" class="headerlink" title="2 方法"></a>2 方法</h3><p>设计了一种具有3D图卷积的方向感知自动编码器，用于潜在特征提取。</p><p>提出解耦旋转机制，利用两个解码器互补的访问旋转信息。</p><p>使用两个残差来估计平移。</p><p>提出一种在线box-cage的三维变形机制来增强训练数据。</p><p><img src="https://img.mahaofei.com/img/20230316151555.png" alt=""></p><ol><li>输入RGB图像。</li><li>使用yolov3检测对象的2D位置、类别标签、类概率图，并将最大概率的位置作为3D球体的中心。从而得到目标3D球体点云。</li><li>使用三维变形机制进行数据扩充。</li><li>使用基于形状的3DGC自动编码器来进行点云分割，用于旋转的潜在特征学习。<br>3DGC由m个单位向量组成，卷积值是核向量和n个最近向量之间的余弦相似度之和。</li><li>从潜在特征中将旋转信息解码为两个垂直向量。</li><li>利用残差估计网络预测平移。</li></ol><h3 id="3-思考-4"><a href="#3-思考-4" class="headerlink" title="3 思考"></a>3 思考</h3><p>提出的使用三维变形机制进行数据扩充很有意思，或许后续很多方法都可以加上这个步骤，使得算法更具有鲁棒性。</p><p>需要训练yolo模型和FS_Net模型。</p><p><strong>NOCS数据集</strong></p><h1 id="CVPR2022"><a href="#CVPR2022" class="headerlink" title="CVPR2022"></a>CVPR2022</h1><h2 id="01-OVE6D-Object-Viewpoint-Encoding-for-Depth-based-6D-Object-Pose-Estimation"><a href="#01-OVE6D-Object-Viewpoint-Encoding-for-Depth-based-6D-Object-Pose-Estimation" class="headerlink" title="01. OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation"></a>01. OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Dingding Cai, Tampere University<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/dingdingcai/OVE6D-pose">https://github.com/dingdingcai/OVE6D-pose</a></p></blockquote><h3 id="1-目标问题-5"><a href="#1-目标问题-5" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>已知物体的分割掩码，物体的三维mesh模型，预测从物体坐标系到相机坐标系的变换R+T。</p><h3 id="2-方法-5"><a href="#2-方法-5" class="headerlink" title="2 方法"></a>2 方法</h3><p><strong>（1）训练阶段</strong></p><p>使用ShapeNet中的3D物体模型来训练OVE6D模型，这个阶段只进行一次，得到的模型参数在后续保持固定。</p><p><strong>（2）编码阶段</strong></p><p>将目标物体的3D网络模型转换为view points编码本，这个阶段对每个物体只进行一次。（view points编码本是一个特征向量的集合）</p><p><strong>（3）推理阶段</strong></p><p>从输入的物体深度图像和物体分割掩码中推理物体的6D姿态</p><ol><li>视角估计：将输入图像和物体ID作为输入，通过与view points编码本中的特征向量进行余弦相似度匹配找到最接近的预定义视角，并输出索引和置信度。</li><li>平面旋转估计：输入图像、ID、预定视角索引、置信度，通过卷积神经网络回归出相对于相机坐标系的旋转。</li><li>平移估计：输入图像、ID、预定义视角索引、置信度、平面旋转角度，通过另一个卷积神经网络输出物体的3D位置。</li></ol><h3 id="3-思考-5"><a href="#3-思考-5" class="headerlink" title="3 思考"></a>3 思考</h3><p>算法需要预先训练好ShapeNet，然后确定一个view points编码本，过程较复杂不够简洁直观。</p><h2 id="02-OnePose-One-Shot-Object-Pose-Estimation-without-CAD-Models"><a href="#02-OnePose-One-Shot-Object-Pose-Estimation-without-CAD-Models" class="headerlink" title="02. OnePose: One-Shot Object Pose Estimation without CAD Models"></a>02. OnePose: One-Shot Object Pose Estimation without CAD Models</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Jiaming Sun, Zhejiang University<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/zju3dv/OnePose">https://github.com/zju3dv/OnePose</a></p></blockquote><h3 id="1-目标问题-6"><a href="#1-目标问题-6" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>实现不依赖于CAD模型的位姿估计</p><h3 id="2-方法-6"><a href="#2-方法-6" class="headerlink" title="2 方法"></a>2 方法</h3><p>借鉴了视觉定位的思想，只需要一个简单的RGB视频扫描物体，就可以构建一个稀疏的SfM模型。然后，利用一个通用的特征匹配网络将这个模型与新的查询图像对齐，从而得到物体姿态。</p><p>提出了一种新的图注意力网络（GATs），可以将同一个SfM点对应的2D特征聚合成3D特征，并与查询图像中的2D特征进行自注意力和交叉注意力匹配。</p><p><img src="https://img.mahaofei.com/img/20230316201824.png" alt=""></p><ol><li>对于每一个物体，使用视频扫描得到一组相机姿态以及物体的3D边界框。</li><li>利用SFM重建一个稀疏的点云模型</li><li>SfM的2D-3D对应关系被建立起来</li><li>使用注意力聚合层，将2D描述符聚合到3D描述符</li><li>通过PnP回归计算出物体位姿</li></ol><p><strong>总体实现流程如下</strong></p><ul><li>使用一些AR工具捕获物体数据，包括物体中心位置，尺寸，绕Z州的旋转角，相机姿态等。</li><li>从捕获的视频中提取图像，使用SfM重建稀疏点云，所有的对应图中提取2D关键点和描述符。</li><li>定位时，实时捕获一系列图像，提取2D关键点和描述符进行匹配，从数据库中查询候选图像，从而找到相机姿态。</li></ul><h3 id="3-思考-6"><a href="#3-思考-6" class="headerlink" title="3 思考"></a>3 思考</h3><p>大概就是创建一个数据库，包括2D图像和重建出的点云，以及相应的2D-3D关键点和描述符，对每一个输入图像提取特征后进行匹配查询。</p><h2 id="03-Focal-Length-and-Object-Pose-Estimation-via-Render-and-Compare"><a href="#03-Focal-Length-and-Object-Pose-Estimation-via-Render-and-Compare" class="headerlink" title="03. Focal Length and Object Pose Estimation via Render and Compare"></a>03. Focal Length and Object Pose Estimation via Render and Compare</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Georgy Ponimatkin, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://ponimatkin.github.io/focalpose">https://ponimatkin.github.io/focalpose</a></p></blockquote><h3 id="1-目标问题-7"><a href="#1-目标问题-7" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>估计相机参数未知的照片中物体的6D姿态</p><h3 id="2-方法-7"><a href="#2-方法-7" class="headerlink" title="2 方法"></a>2 方法</h3><ol><li>从一个3D模型库中选择一个与输入图像中物体最匹配的3D模型。</li><li>用一个CNN编码器将输入图像编码成一个特征向量。</li><li>用一个CNN解码器将特征向量解码成一个初始的6D姿态和焦距。</li><li>用渲染引擎根据初始的6D姿态和焦距渲染出一个虚拟视图，并与输入图像进行比较。</li><li>用一个损失函数计算虚拟视图和输入图像之间的差异，并反向传播更新6D姿态和焦距。</li><li>重复步骤4和5直到收敛或达到最大迭代次数。</li></ol><h3 id="3-思考-7"><a href="#3-思考-7" class="headerlink" title="3 思考"></a>3 思考</h3><p>对于网络图像中物体的位姿估计，不知道实际应用场景是什么。</p><h2 id="04-ES6D-A-Computation-Efficient-and-Symmetry-Aware-6D-Pose-Regression-Framework"><a href="#04-ES6D-A-Computation-Efficient-and-Symmetry-Aware-6D-Pose-Regression-Framework" class="headerlink" title="04. ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework"></a>04. ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Ningkai Mo, ShenZhen Key Lab of Computer Vision and Pattern Recognition<br><strong>关键词</strong>：位姿估计；对称<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/GANWANSHUI/ES6D">https://github.com/GANWANSHUI/ES6D</a></p></blockquote><h3 id="1-目标问题-8"><a href="#1-目标问题-8" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>主要解决如何利用RGB-D数据来估计刚体物体的6D姿态，特别是对于具有对称性的物体</p><h3 id="2-方法-8"><a href="#2-方法-8" class="headerlink" title="2 方法"></a>2 方法</h3><ul><li>设计了一个全卷积的特征提取网络，叫做XYZNet，它可以高效地从RGB和深度图中提取点云特征，并将不同模态的特征融合起来。</li><li>提出了一种新的形状表示方法，叫做分组基元（GP），它只与物体的对称性有关，而忽略了形状的细节。</li><li>基于GP，设计了一种新的姿态距离度量，叫做平均（最大）分组基元距离，或者A(M)GPD。这种度量可以作为损失函数来训练回归网络，并保证网络收敛到正确的姿态。</li></ul><p><img src="https://img.mahaofei.com/img/20230316211632.png" alt=""></p><ol><li>从RGB-D图像生成RGB-XYZ数据。RGB-XYZ数据被馈送到CNN模块以提取局部特征，该局部特征对颜色和几何信息进行编码</li><li>点云特征是通过类似PointNet的CNN模块获得的，并填充到与局部特征相同的大小</li><li>将局部特征和点云特征连接为用于姿态估计的逐点特征</li><li>选择具有最大置信度的姿势作为最终结果</li></ol><h3 id="3-思考-8"><a href="#3-思考-8" class="headerlink" title="3 思考"></a>3 思考</h3><p>同样是逐点特征，这篇论文提出了XYZNet，可以更高效的提取提取点云和RGB特征，不需要提供掩码图。</p><p>代码只提供T-LESS数据集方法。</p><h2 id="05-GPV-Pose-Category-level-Object-Pose-Estimation-via-Geometry-guided-Point-wise-Voting"><a href="#05-GPV-Pose-Category-level-Object-Pose-Estimation-via-Geometry-guided-Point-wise-Voting" class="headerlink" title="05.  GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting"></a>05.  GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：YanDi, Technical University of Munich<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/lolrudy/GPV_Pose">https://github.com/lolrudy/GPV_Pose</a></p></blockquote><h3 id="1-目标问题-9"><a href="#1-目标问题-9" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>主要解决了现有方法在处理未见过的物体实例时存在的不确定性和不稳定性的问题</p><h3 id="2-方法-9"><a href="#2-方法-9" class="headerlink" title="2 方法"></a>2 方法</h3><p><img src="https://img.mahaofei.com/img/20230317093148.png" alt=""></p><ol><li>给定一副RGB-D图像，先使用如Maks-RCNN等方法将物体从深度图中分割出来。</li><li>然后从深度三维点云中抽取1028个点，并将它们输入到GPV-Pose位姿估计网络中。</li><li>由于3DGC对于点云的移动和缩放不敏感，所以以3DGC为主干提取全局和每个点的特征，凭借附加了三个并行分支，用于姿态预测，对称性，和逐点包围盒。</li></ol><p>注：</p><ul><li>3DGC方法首先将输入点云转换为一个k近邻图（kNN graph），其中每个点与其最近的k个邻居相连。使用多层图卷积（Graph Convolution）来提取每个点的局部特征，并使用最大池化（Max Pooling）来提取全局特征。3DGC方法将全局和逐点特征拼接起来，形成一个混合特征向量，用于后续的姿态估计、对称感知重建和点投票模块。</li></ul><h3 id="3-思考-9"><a href="#3-思考-9" class="headerlink" title="3 思考"></a>3 思考</h3><p><strong>（1）创新点</strong></p><ul><li>引入了一种解耦的置信度驱动旋转表示，允许几何感知恢复相关旋转矩阵</li><li>提出了一种基于点投票的位移估计模块，利用几何约束来生成可靠和精确的位移预测</li><li>在一个端到端可训练的网络中整合这两个模块，并通过一个多任务损失函数进行优化</li></ul><p><strong>（2）与DenseFusion相比</strong></p><p>它们都使用了3D图卷积网络（3DGC）来从输入点云中提取每个点的局部特征，并将其与全局特征拼接起来，形成一个混合特征向量。</p><p>GPV-Pose使用了一种解耦的置信度驱动的旋转表示，可以通过几何关系恢复旋转矩阵，而DenseFusion使用了一种直接预测四元数的方法。</p><p><strong>使用NOCS数据集。</strong></p><h2 id="06-DGECN-A-Depth-Guided-Edge-Convolutional-Network-for-End-to-End-6D-Pose-Estimation"><a href="#06-DGECN-A-Depth-Guided-Edge-Convolutional-Network-for-End-to-End-6D-Pose-Estimation" class="headerlink" title="06. DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation"></a>06. DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Tuo Cao, School of Computer Science, Wuhan University, Wuhan, Hubei, China<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/maplect/DGECN_CVPR2022">https://github.com/maplect/DGECN_CVPR2022</a></p></blockquote><h3 id="1-目标问题-10"><a href="#1-目标问题-10" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>从单目RGB图像中进行位姿估计。</p><h3 id="2-方法-10"><a href="#2-方法-10" class="headerlink" title="2 方法"></a>2 方法</h3><p><strong>（1）深度细化网络DRN</strong></p><p>两个不同的深度估计网络分别输出深度图DA和DB，计算两个深度图之间的差异，并将差异超过阈值的区域定义为不确定区域。</p><p><strong>（2）特征提取</strong></p><ul><li>深度估计：将彩色图像作为输入，并执行深度图预测</li><li>对象分割：利用分割的掩码，将深度图转换为3D点云，并利用3D特征提取器来提取几何特征</li></ul><p><strong>（3）2D关键点定位</strong></p><p>采用最远点采样（FPS）算法来选择物体表面上的关键点。</p><p><strong>（4）从2D-3D对应关系学习6D位姿</strong></p><p>使用动态图PnP（DG-PnP）算法，通过边缘卷积构建一个图结构，利用2D-3D对应关系中的拓扑信息来直接学习6D姿态</p><h3 id="3-思考-10"><a href="#3-思考-10" class="headerlink" title="3 思考"></a>3 思考</h3><p><strong>（1）创新点</strong></p><ul><li>用一个深度引导网络同时预测分割和深度图，并用一个深度优化网络（DRN）提高深度图的质量</li><li>根据分割和深度图建立2D-3D对应关系，即将图像上的关键点与3D模型上的点匹配</li><li>提出一个动态图PnP（DG-PnP）算法，通过边缘卷积构建一个图结构，利用2D-3D对应关系中的拓扑信息来直接学习6D姿态</li></ul><p><strong>（2）实用性</strong></p><p>从单目RGB图像进行位姿估计，通过网络回归深度图。</p><h2 id="07-Templates-for-3D-Object-Pose-Estimation-Revisited-Generalization-to-New-Objects-and-Robustness-to-Occlusions"><a href="#07-Templates-for-3D-Object-Pose-Estimation-Revisited-Generalization-to-New-Objects-and-Robustness-to-Occlusions" class="headerlink" title="07. Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions)"></a>07. Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions)</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Van Nguyen Nguyen, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/nv-nguyen/template-pose">https://github.com/nv-nguyen/template-pose</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h3 id="1-目标问题-11"><a href="#1-目标问题-11" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>提出了一种方法，只需要物体的CAD模型，就可以将输入对象匹配到一组模板，即使在部分遮挡的情况下也可以估计3D姿态。</p><h3 id="2-方法-11"><a href="#2-方法-11" class="headerlink" title="2 方法"></a>2 方法</h3><p><img src="https://img.mahaofei.com/img/20230322150858.png" alt=""></p><p>训练时，使用由真实图像和合成模板组成的对，来计算局部特征，预测两幅图像的相似性。</p><p>然后对于未看到的图像，计算其局部特征，将图像与模板数据库匹配来检索对象姿态。</p><h3 id="3-思考-11"><a href="#3-思考-11" class="headerlink" title="3 思考"></a>3 思考</h3><p>编码本思想，实用性较强，可尝试。</p><h2 id="08-Coupled-Iterative-Refinement-for-6D-Multi-Object-Pose-Estimation"><a href="#08-Coupled-Iterative-Refinement-for-6D-Multi-Object-Pose-Estimation" class="headerlink" title="08. Coupled Iterative Refinement for 6D Multi-Object Pose Estimation"></a>08. Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Lahav Lipson, Princeton University<br><strong>关键词</strong>：位姿估计；迭代优化<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">https://github.com/princeton-vl/Coupled-Iterative-Refinement</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h3 id="1-目标问题-12"><a href="#1-目标问题-12" class="headerlink" title="1 目标问题"></a>1 目标问题</h3><p>给定一组已知的RGBD输入，检测每个对象的6D姿态。</p><h3 id="2-方法-12"><a href="#2-方法-12" class="headerlink" title="2 方法"></a>2 方法</h3><p>算法复杂，迭代优化方法。</p><h3 id="3-思考-12"><a href="#3-思考-12" class="headerlink" title="3 思考"></a>3 思考</h3><p>代码效果最好，但是代码较为复杂。</p>]]></content>
    
    
    <summary type="html">检索阅读近三年CVPR6D位姿估计相关论文并进行记录</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】DenseFusion被引论文丨无代码</title>
    <link href="https://www.mahaofei.com/post/d3bc13be.html"/>
    <id>https://www.mahaofei.com/post/d3bc13be.html</id>
    <published>2023-03-14T07:23:31.000Z</published>
    <updated>2023-03-14T07:23:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-6D-Pose-Estimation-for-Bin-Picking-based-on-Improved-Mask-R-CNN-and-DenseFusion"><a href="#01-6D-Pose-Estimation-for-Bin-Picking-based-on-Improved-Mask-R-CNN-and-DenseFusion" class="headerlink" title="01. 6D Pose Estimation for Bin-Picking based on Improved Mask R-CNN and DenseFusion"></a>01. 6D Pose Estimation for Bin-Picking based on Improved Mask R-CNN and DenseFusion</h1><blockquote><p><strong>期刊 / 会议</strong>：26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)<br><strong>作者 / 机构</strong>：Hesheng Wang, 上海交通大学<br><strong>关键词</strong>：位姿估计, 实例分割, MaskRCNN, DenseFusion<br><strong>时间</strong>：2021<br><strong>代码</strong>：无</p></blockquote><h2 id="1-目标问题"><a href="#1-目标问题" class="headerlink" title="1 目标问题"></a>1 目标问题</h2><p>将实例分割算法和位姿估计算法应用到工业机器人抓取中。</p><h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><p>使用两级神经网络，将输入的RGB-D图像回归出6D位姿。</p><p><img src="https://img.mahaofei.com/img/20230314153950.png" alt=""></p><p><strong>（1）数据集生成</strong></p><p>由于基于学习的算法需要大量的已经标注的数据集，因此本文给出了一种工业零件的虚拟数据集的生成过程。使用Blender进行物理模拟，调整渲染参数，生成多种高质量的RGB图像及分割掩码和6D姿态标签。</p><p><strong>（2）实例分割</strong></p><p>使用ResNeXt与MaskRCNN完成目标检测与实例分割。</p><p><strong>（3）姿态估计</strong></p><p>基于DenseFusion来预测6D位姿，为了提高性能，增加了NonLocal模块，(图中的绿色块)，使得网络可以从提取的特征中学习空间结构特征，并且可以有效地在点特征之间建立连接。（好像没什么用）</p><p><img src="https://img.mahaofei.com/img/1678779913.png" alt=""></p><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h2><p>虚拟数据集可辅助提高训练结果的鲁棒性，可尝试。</p><h1 id="02-A-Lightweight-Two-End-Feature-Fusion-Network-for-Object-6D-Pose-Estimation"><a href="#02-A-Lightweight-Two-End-Feature-Fusion-Network-for-Object-6D-Pose-Estimation" class="headerlink" title="02. A Lightweight Two-End Feature Fusion Network for Object 6D Pose Estimation"></a>02. A Lightweight Two-End Feature Fusion Network for Object 6D Pose Estimation</h1><blockquote><p><strong>期刊 / 会议</strong>：Machines<br><strong>作者 / 机构</strong>：Ligang Zuo, 北京科技大学<br><strong>关键词</strong>：位姿估计, 特征融合<br><strong>时间</strong>：2022<br><strong>代码</strong>：无代码</p></blockquote><h2 id="1-目标问题-1"><a href="#1-目标问题-1" class="headerlink" title="1 目标问题"></a>1 目标问题</h2><p>提出一种轻量化的位姿估计模型，用于部署在移动设备上。</p><h2 id="2-方法-1"><a href="#2-方法-1" class="headerlink" title="2 方法"></a>2 方法</h2><ol><li>使用PointnoProblemNet网络提取点云特征</li><li>将点云特征与图像特征进行像素级融合</li><li>利用CNN进行特征提取</li><li>对每个特征进行位姿估计，选择置信度最高的作为最终结果<br>（这不就是DenseFusion的思路吗，只不过把PointNet换成了PointnoProblem）</li></ol><p>使用深度可分离卷积代替标准卷积，即将ResNet特征提取部分换为MobileNetv2，来减少模型参数的数量。</p><h2 id="3-思考-1"><a href="#3-思考-1" class="headerlink" title="3 思考"></a>3 思考</h2><p>可以考虑尝试使用MobileNetv2特征提取网络，因为所使用的也是移动设备。</p>]]></content>
    
    
    <summary type="html">检索了DenseFusion的被引论文，但是因为太多都没有开源代码，看了两篇就转向只关注顶会论文了。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="DenseFusion" scheme="https://www.mahaofei.com/tags/DenseFusion/"/>
    
  </entry>
  
  <entry>
    <title>Windows设置共享文件夹给Ubuntu或其它设备</title>
    <link href="https://www.mahaofei.com/post/76e8a448.html"/>
    <id>https://www.mahaofei.com/post/76e8a448.html</id>
    <published>2023-03-11T04:11:46.000Z</published>
    <updated>2023-03-11T04:11:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Windows系统设置"><a href="#一、Windows系统设置" class="headerlink" title="一、Windows系统设置"></a>一、Windows系统设置</h1><p>在需要的位置新建一个文件夹，然后【右键-共享】，将其设置为共享文件夹。</p><p><img src="https://img.mahaofei.com/img/202303111048877.png" alt=""></p><p>然后打开【设置-网络和Internet-高级网络设置-高级共享设置】，设置为如下的形式。<strong>主要是公用网络的两个都要打开，所有网络的共享打开，密码保护关闭</strong></p><p><img src="https://img.mahaofei.com/img/202303111048312.png" alt=""></p><h1 id="二、Ubuntu系统设置"><a href="#二、Ubuntu系统设置" class="headerlink" title="二、Ubuntu系统设置"></a>二、Ubuntu系统设置</h1><h2 id="2-1-临时挂载"><a href="#2-1-临时挂载" class="headerlink" title="2.1 临时挂载"></a>2.1 临时挂载</h2><p>挂载方式如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t cifs //192.168.3.67/home/Share ~/Share -o username=&#x27;admin&#x27;,password=&#x27;123456&#x27;,dir_mode=0777,file_mode=0777,vers=2.0</span><br></pre></td></tr></table></figure><h2 id="2-2-自动挂载"><a href="#2-2-自动挂载" class="headerlink" title="2.2 自动挂载"></a>2.2 自动挂载</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/fstab</span><br></pre></td></tr></table></figure><p>在最后面按照下面的格式添加</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//192.168.1.143/home/Share ~/share cifs defaults,username=admin,password=123456,dir_mode=0777,file_mode=0777,vers=2.0 0 2</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">局域网内，共享win电脑上的某个文件夹，使局域网内其它系统可以直接访问</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Windows工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Windows%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu使用OneDrive</title>
    <link href="https://www.mahaofei.com/post/ba007624.html"/>
    <id>https://www.mahaofei.com/post/ba007624.html</id>
    <published>2023-03-10T07:31:33.000Z</published>
    <updated>2023-03-10T07:31:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p><strong>（1）安装依赖</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt install build-essential </span><br><span class="line">apt install libcurl4-openssl-dev -y</span><br><span class="line">apt install libsqlite3-dev -y</span><br><span class="line">apt install pkg-config -y</span><br><span class="line">apt install libnotify-dev -y</span><br><span class="line">curl -fsS https://dlang.org/install.sh | bash -s dmd</span><br></pre></td></tr></table></figure><p>激活DMD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/dlang/dmd-2.082.0/activate</span><br></pre></td></tr></table></figure><p><strong>（2）安装onedrive客户端</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/abraunegg/onedrive.git</span><br><span class="line">cd onedrive</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>输入以下命令登录onedrive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onedrive</span><br></pre></td></tr></table></figure><p>下载config文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/.config/onedrive</span><br><span class="line">wget https://raw.githubusercontent.com/abraunegg/onedrive/master/config -O ~/.config/onedrive/config</span><br><span class="line">nano ~/.config/onedrive/config</span><br></pre></td></tr></table></figure><p>打开config文件中的下面几行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sync_dir = &quot;~/disk/ubuntu/onedrive&quot;</span><br><span class="line">monitor_interval = &quot;60&quot;</span><br></pre></td></tr></table></figure><h1 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h1><p>第一次同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onedrive --synchronize</span><br></pre></td></tr></table></figure><p>实时同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onedrive --monitor</span><br></pre></td></tr></table></figure><blockquote><p>参考链接</p><ol><li><a href="https://github.com/abraunegg/onedrive">abraunegg/onedrive</a></li><li><a href="https://www.moerats.com/archives/740/">Rat’s. 适用于Linux的OneDrive客户端，支持VPS和OneDrive之间实时同步/备份</a></li></ol></blockquote><h1 id="替代方法"><a href="#替代方法" class="headerlink" title="替代方法"></a>替代方法</h1><p>此方法不太推荐，同步速度较慢，而且一旦取消同步本地文件都会清空。</p><h2 id="1-安装OneDriver"><a href="#1-安装OneDriver" class="headerlink" title="1. 安装OneDriver"></a>1. 安装OneDriver</h2><p>参考项目：<a href="https://github.com/jstaf/onedriver">https://github.com/jstaf/onedriver</a></p><p>根据作者的说明，对于Ubuntu系统，可以直接下载deb文件安装，下载链接为：<a href="https://software.opensuse.org/download.html?project=home%3Ajstaf&amp;package=onedriver">https://software.opensuse.org/download.html?project=home%3Ajstaf&amp;package=onedriver</a></p><p>选择Ubuntu，找到自己的Ubuntu版本，以及amd64/arm64，下载deb安装包并安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i onedriver_0.13.0-1_amd64.deb</span><br></pre></td></tr></table></figure><h1 id="2-OneDriver使用"><a href="#2-OneDriver使用" class="headerlink" title="2. OneDriver使用"></a>2. OneDriver使用</h1><p>点击左上角<code>+</code>，选择本地同步文件夹。</p><p>然后输入用户名密码登录自己的OneDrive网盘。</p><p>点击右面的√，勾选系统登录时启动OneDriver。</p>]]></content>
    
    
    <summary type="html">之前一直使用坚果云做多设备同步，但是坚果云每个月只有1G流量，不够用。因此改到OneDrive，本文记录在Ubuntu上进行OneDrive的配置过程。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【浏览器插件】iTab新建标签页</title>
    <link href="https://www.mahaofei.com/post/999804d5.html"/>
    <id>https://www.mahaofei.com/post/999804d5.html</id>
    <published>2023-03-10T00:28:23.000Z</published>
    <updated>2023-03-10T00:28:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本人之前很长一段时间都是实用的默认主页，并且只保留一个搜索框使主页尽可能简洁。</p><p>但是后来随着要用的网页越来越多，很多网站记不住经常需要搜索才能找到。</p><p>收藏夹又因为网站太多，我的收藏夹都是文件夹套文件夹套网址。</p><p>正巧看到了这个新建标签页，感觉还不错，有兴趣的可以体验一下，有点像之前用的infinite和青柠，但个人觉得这个更好一些。</p><h1 id="主页"><a href="#主页" class="headerlink" title="主页"></a>主页</h1><p>主页侧面是不同的分区，每个分区内可以添加很多不同的图标。</p><p><img src="https://img.mahaofei.com/img/20230310082742.png" alt=""></p><p>有官方给的图标，也可以自定义添加。</p><p><img src="https://img.mahaofei.com/img/20230310082814.png" alt=""></p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>而且itab支持网页版主页，也就是说，手机edge等无法安装浏览器扩展的应用，可以在设置里将主页改成<a href="https://go.itab.link/">https://go.itab.link/</a>就可以实现手机电脑使用同样的主页了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;本人之前很长一段时间都是实用的默认主页，并且只保留一个搜索框使主页尽可能简洁。&lt;/p&gt;
&lt;p&gt;但是后来随着要用的网页越来越多，很多网站记不住</summary>
      
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="浏览器插件" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8F%92%E4%BB%B6/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
</feed>
