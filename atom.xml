<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>马浩飞丨博客</title>
  
  <subtitle>无限进步！！！</subtitle>
  <link href="https://www.mahaofei.com/atom.xml" rel="self"/>
  
  <link href="https://www.mahaofei.com/"/>
  <updated>2023-11-02T02:01:55.000Z</updated>
  <id>https://www.mahaofei.com/</id>
  
  <author>
    <name>马浩飞</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【强化学习笔记】基础</title>
    <link href="https://www.mahaofei.com/post/21a41133.html"/>
    <id>https://www.mahaofei.com/post/21a41133.html</id>
    <published>2023-11-02T02:01:55.000Z</published>
    <updated>2023-11-02T02:01:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、基本概念</h1><h2 id="1-1-专业术语">1.1 专业术语</h2><p><strong>（1）state/状态</strong>：状态可以暂时理解为周围环境的状态<br><strong>（2）action/动作</strong>：agent智能体的动作<br><strong>（3）policy/策略</strong>：针对观测到的状态，作出决策，控制agent运动。$\pi(a|s)=P(A=a|S=s)$，$a$ 是动作action，$s$ 是状态，强化学习的目标就是学习策略policy，一般是概率密度函数。<br><strong>（4）reward/奖励</strong>：定义奖励的方式很影响强化学习的结果。<br><strong>（5）state transition/状态转移</strong>：在原本的状态，agent执行一个动作后状态会发生转移。$P(S’=s’|S=s,A=a)$<br><strong>（6）Return/回报</strong>：也被称为以后的奖励的累积，$U_t=R_t+R_{t+1}+R_{t+2}\dots$<br><strong>（7）Discounted return/折扣回报</strong>：未来的奖励比当前的奖励回报小，设 $\gamma$ 为折扣率，则 $U_t=R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}\dots$<br><strong>（8）Action-Value Function/动作价值函数</strong>：在t时刻其实我们并不知道 $U_t$ ，因为$U_t$是个随机变量，取决于未来的状态和动作，但我们可以用期望计算$U_t$。$Q_\pi(s_t,a_t)=E[U_t|S_t=s_t,A_t=a_t]$，用不同的策略函数就会有不同的价值结果。也可以用最大值获得不同策略函数的最大分散$Q^*(s_t,a_t)=max_\pi Q_\pi(s_t,a_t)$，使用动作价值函数可以判断当前的动作好不好。<br><strong>（9）State-value function/状态价值函数</strong>：$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]$，状态价值函数可以告诉我们当前的局势好不好。<br><strong>（10）cross entropy/交叉熵</strong>：衡量两个向量之间的区别有多大，$H(\textbf p, \textbf q)=-\sum^m_{j=1}p_j\cdot ln q_j$，两个概率分布一致时，交叉熵最小。</p><h2 id="1-2-强化学习的随机性">1.2 强化学习的随机性</h2><p><strong>（1）Action动作的随机性</strong></p><p>因为动作是根据策略函数随机抽样得到的，因此agent有可能做策略中的任何一种动作，虽然这些动作的概率有大有小，但是动作本身是随机的。</p><p><strong>（2）State transitions状态转移的随机性</strong></p><p>假定agent作出了一个动作，环境会用概率随机抽样，给出下一个状态。</p><h2 id="1-3-强化学习如何控制agent">1.3 强化学习如何控制agent</h2><p><strong>（1）如果有策略函数 $\pi(a|s)$</strong></p><ol><li>给定一个观测状态 $s_t$</li><li>利用策略函数从所有可能的动作中随机采样 $a_t~\pi(\cdot|s_t)$</li></ol><p><strong>（2）如果有最优的动作价值函数 $Q^<em>(s,a)</em>$</strong></p><ol><li>给定一个观测状态 $s_t$</li><li>最大化 $a_t=argmax_a Q^*(s_t,a)$ 来选择动作</li></ol>]]></content>
    
    
    <summary type="html">强化学习基础部分笔记</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="Python" scheme="https://www.mahaofei.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>【论文复现】MimicPlay从人类演示中学习机器人技能</title>
    <link href="https://www.mahaofei.com/post/21b38b7b.html"/>
    <id>https://www.mahaofei.com/post/21b38b7b.html</id>
    <published>2023-10-21T07:00:02.000Z</published>
    <updated>2023-10-21T07:00:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1>论文笔记</h1><h2 id="1-目标问题-23">1 目标问题</h2><p>从人类演示中学习，是教授机器人操作技能的一种很有前途的方法。</p><p>目前大多数模仿学习算法仍然局限于学习短期的操作，例如开门或抓取特定物品。</p><p>而关于长期任务的研究，目前有两个方向：分层模仿学习和从演示数据中学习。分层学习旨在通过端到端实现高级规划到低级运动控制的学习。从演示数据中学习是指人类通过遥控机器人于环境互动来收集数据。</p><p>本文提出了一个分层学习框架，从大量人类演示数据中学习潜在的计划，来指导机器人在少量演示中实现视觉运动控制。</p><h2 id="2-主要方法">2 主要方法</h2><p><img src="https://img.mahaofei.com/img/202310231710757.png" alt="image.png"></p><h3 id="2-1-收集人类数据">2.1 收集人类数据</h3><p>人类在用手与环境互动的过程中，创造了一个手的轨迹。本文使用两台经过校准的相机来跟踪人类演示数据中的3D手轨迹，手部位置检测使用现有的库。</p><h3 id="2-2-从人类数据中学习3D潜在规划">2.2 从人类数据中学习3D潜在规划</h3><p>问题：给定一个由目标图像表示的长期任务，策略产生以目标为条件的行动。</p><p>将该问题转化为分层学习策略，其中高级规划器从目标图像中提取关键特征，并转化成低维的规划，利用这些规划引导运动控制器动作。为了训练高级规划器，本文使用廉价的数据源（人类演示数据）</p><p><strong>（1）多模式潜在计划学习</strong></p><p>利用收集的人类演示数据和对应的3D手部轨迹，将学习规程转化为目标条件的3D轨迹生成任务。即将人类演示图像，目标图像处理为低维特征，利用MLP编码为潜在计划向量，利用潜在计划向量和手的位置，利用MLP解码为3D手部轨迹的预测。</p><p>为了解决不同人演示同一任务的差异，使用基于MLP的高斯混合模型来对潜在计划的轨迹分布进行建模。</p><p><strong>（2）处理人类演示数据和机器人之间的视觉差异</strong></p><p>本文考虑人与机器人在同一环境中，需要解决机器人与人类外观不同导致的视觉差异。通过计算人类和机器人的视觉编码器的特征嵌入的分布，最小化两者距离（此步骤机器人与人类视频不需要是对应的）</p><h3 id="2-3-多任务模仿学习">2.3 多任务模仿学习</h3><p><strong>（1）用于潜在计划生成的视频提示</strong></p><p>使用单镜头视频作为目标指定提示，发送给训练好的潜在规划器，生成机器人可执行的潜在计划。</p><p>规划器将视频分成多个帧，每个时间步长，规划器从序列中取一个图像帧作为目标图像，生成潜在规划引导机器人动作。</p><p><strong>（2）基于Transformer的计划引导与模仿</strong></p><p>在执行复杂任务时，仅使用高层规划是不够的，还需要考虑底层的细节。因此考虑将机器人腕部相机和本体感觉都转换成低维特征向量，与潜在计划进行结合，利用Trasformer架构（因为其擅长管理长期运动生成）进行处理。</p><p><strong>（3）多任务</strong></p><p>在同一环境中的所有任务中共享相同的规划器和策略模型。</p><h1>算法复现</h1><h2 id="1-环境搭建">1 环境搭建</h2><h3 id="1-1-代码准备">1.1 代码准备</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/j96w/MimicPlay.git</span><br></pre></td></tr></table></figure><h3 id="1-2-conda环境配置">1.2 conda环境配置</h3><p><strong>（1）进入所下载代码环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay</span><br></pre></td></tr></table></figure><p><strong>（2）创建 conda 环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mimicplay python=3.8 -y</span><br><span class="line">conda activate mimicplay</span><br></pre></td></tr></table></figure><p><strong>（3）安装 MuJoCo</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mujoco==2.3.0</span><br></pre></td></tr></table></figure><blockquote><p><strong>如果出现问题</strong>：<code>imgaug 0.4.0 requires XXXXXX, which is not installed.</code><br><strong>解决方法如下</strong>：</p><ol><li>安装报错提示的imgaug所需的依赖项：<code>pip install imageio matplotlib Pillow scikit-image six opencv-python</code></li><li>重新安装mujoco：<code>pip install mujoco</code></li></ol></blockquote><p><strong>（4）安装robosuite</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ARISE-Initiative/robosuite.git</span><br><span class="line">cd robosuite</span><br><span class="line">git checkout v1.4.1_libero</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -r requirements-extra.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><blockquote><p>第一次安装失败，然后<code>git checkout v1.4.1_libero</code>之后才安装成功</p></blockquote><p><strong>（5）安装BDDL</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/StanfordVL/bddl.git</span><br><span class="line">cd bddl</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><strong>（6）安装LIBERO</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git</span><br><span class="line">cd LIBERO</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><strong>（7）安装robomimic</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/ARISE-Initiative/robomimic</span><br><span class="line">cd robomimic</span><br><span class="line">git checkout mimicplay-libero</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><blockquote><p>第一次安装失败，然后<code>git checkout mimicplay-libero</code>之后才安装成功</p></blockquote><p><strong>（8）安装MimicPlay</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/j96w/MimicPlay.git</span><br><span class="line">cd MimicPlay</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h2 id="2-数据准备（从虚拟机器人动作数据中学习机器人动作）">2 数据准备（从虚拟机器人动作数据中学习机器人动作）</h2><h3 id="2-1-官方数据集">2.1 官方数据集</h3><p>训练集和测试视频在<a href="https://drive.google.com/drive/folders/1FUKd3vr-KBiYRnKIymNmGClmVx9U45XG">此处</a>下载。训练集是一系列没有指定特定任务（没有标签）的人类演示视频。</p><p>作者推荐下载原始数据<code>demo.hdf5</code>，然后在本地电脑上将其处理为具有图像观察的训练数据集<code>demo_image.hdf5</code>，因为这样可以很好的检查环境库是否安装正确，具体步骤如下：</p><p><strong>（1）将下载的数据集移动到<code>mimicplay/datasets</code></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">mv mimicplay_release_dataset your_installation_path/mimicplay/datasets</span><br></pre></td></tr></table></figure><p><strong>例如：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">mkdir -p datasets/playdata</span><br><span class="line">mv ~/Downloads/demo.hdf5 ./datasets/playdata/</span><br></pre></td></tr></table></figure><p><strong>（2）将原始数据转换为图像数据集</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">python scripts/preprocess_hdf5.py -i ./datasets/playdata/demo.hdf5 -o ./datasets/playdata/demo_modified.hdf5</span><br><span class="line">python scripts/dataset_states_to_obs.py --dataset &#x27;datasets/playdata/demo_modified.hdf5&#x27; --done_mode 0 --camera_names agentview robot0_eye_in_hand --camera_height 84 --camera_width 84 --output_name image_demo_local.hdf5 --exclude-next-obs</span><br></pre></td></tr></table></figure><blockquote><p><strong>如果出现问题</strong>：<code>ileNotFoundError: [Errno 2] No such file or directory: 'patchelf'</code><br><strong>解决方法如下</strong>：<code>sudo apt-get install patchelf</code></p></blockquote><p><strong>（3）提取末端轨迹用于上层规划器的训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/dataset_extract_traj_plans.py --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27;</span><br></pre></td></tr></table></figure><p><strong>（4）检查数据：重新播放数据集中的图像，保存成视频</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/playback_robomimic_dataset.py --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27; --use-obs --render_image_names agentview_image --video_path image_demo_local_replay.mp4</span><br></pre></td></tr></table></figure><h3 id="2-2-自制数据集">2.2 自制数据集</h3><p><strong>（1）使用BDDL文件收集数据</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/collect_playdata.py --bddl-file &#x27;scripts/bddl_files/KITCHEN_SCENE9_playdata.bddl&#x27; --device &#x27;keyboard&#x27;</span><br></pre></td></tr></table></figure><p>收集的原始数据可以在<code>robosuite/robosuite/models/assets/demonstrations/</code>路径下找到。</p><p><strong>（2）将原始数据转换成robomimic格式</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/convert_playdata_to_robomimic_dataset.py --dataset &#x27;path_to_your_data&#x27;</span><br></pre></td></tr></table></figure><p><strong>（3）现在有了robomimic格式的数据，按照#1.3 公共数据集中的步骤生成特定任务的视频提示</strong></p><h2 id="3-数据准备（从人类演示中学习机器人动作）">3 数据准备（从人类演示中学习机器人动作）</h2><h3 id="3-1-配置人手检测模型">3.1 配置人手检测模型</h3><p><strong>（1）配置开源的 hand_object_detector</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda create --name handobj python=3.6</span><br><span class="line">conda activate handobj</span><br><span class="line">conda install pytorch=1.0.1 torchvision cudatoolkit=10.0 -c pytorch</span><br><span class="line">cd mimicplay/scripts/human_playdata_process</span><br><span class="line">git clone https://github.com/ddshan/hand_object_detector &amp;&amp; cd hand_object_detector</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">cd lib</span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></table></figure><p><strong>（2）下载fast_rcnn模型，并放置在指定位置</strong></p><p>从Google Drive中下载<a href="https://drive.google.com/file/d/1H2tWsZkS7tDF8q1-jdjx6V9XrK25EDbE/view">faster_rcnn_1_8_132028.pth (361M)</a>，移动到下面的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd hand_object_detector</span><br><span class="line">mkdir -p models/res101_handobj_100K/pascal_voc</span><br><span class="line">mv faster_rcnn_1_8_132028.pth models/res101_handobj_100K/pascal_voc/.</span><br></pre></td></tr></table></figure><p><strong>（3）将mimicplay的python脚本放入人手检测器的目录下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd mimicplay/scripts/human_playdata_process/</span><br><span class="line">cp demo_mp4.py hand_object_detector/</span><br></pre></td></tr></table></figure><h3 id="3-2-从人类演示生成数据集">3.2 从人类演示生成数据集</h3><p><strong>（1）复制两个示例视频</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp vis_1.mp4 hand_object_detector/</span><br><span class="line">cp vis_2.mp4 hand_object_detector/</span><br></pre></td></tr></table></figure><p><strong>（2）生成hdf5数据文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd hand_object_detector/</span><br><span class="line">python demo_mp4.py</span><br></pre></td></tr></table></figure><p><strong>（3）可视化数据集</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd scripts/human_playdata_process/</span><br><span class="line">python vis_processed_human_play_data.py</span><br></pre></td></tr></table></figure><h2 id="4-训练">4 训练</h2><h3 id="4-1-训练高级规划器">4.1 训练高级规划器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">python scripts/train.py --config configs/highlevel.json --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27;</span><br></pre></td></tr></table></figure><p>训练结束后，选择评估分数最高的checkpoint，将其路径作为</p><h3 id="4-2-训练低级机器人控制器">4.2 训练低级机器人控制器</h3>]]></content>
    
    
    <summary type="html">斯坦福大学李飞飞团队的通过观看人类动作进行长期模仿学习方法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>Nerf(instant-ngp)快速实现三维重建</title>
    <link href="https://www.mahaofei.com/post/ce3c8324.html"/>
    <id>https://www.mahaofei.com/post/ce3c8324.html</id>
    <published>2023-10-10T12:59:32.000Z</published>
    <updated>2023-10-10T12:59:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1>搭建环境</h1><p><strong>（1）创建conda环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n nerf-ngp python=3.8</span><br><span class="line">conda activate nerf-ngp</span><br><span class="line">pip install commentjson imageio numpy opencv-python-headless pybind11 pyquaternion scipy tqdm</span><br></pre></td></tr></table></figure><p><strong>（2）下载instant-ngp应用</strong></p><blockquote><p>项目地址：<a href="https://github.com/NVlabs/instant-ngp">https://github.com/NVlabs/instant-ngp</a></p></blockquote><p>快速使用可以下载官方提供的<code>instant-ngp.exe</code>应用，根据自己的显卡版本下载即可：</p><ul><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-RTX-3000-and-4000.zip"><strong>RTX 3000 &amp; 4000 series, RTX A4000–A6000</strong>, and other Ampere &amp; Ada cards</a></li><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-RTX-2000.zip"><strong>RTX 2000 series, Titan RTX, Quadro RTX 4000–8000</strong>, and other Turing cards</a></li><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-GTX-1000.zip"><strong>GTX 1000 series, Titan Xp, Quadro P1000–P6000</strong>, and other Pascal cards</a></li></ul><p>（如果链接失效请参考源项目中Installation部分，如果在ubuntu下使用，需要下载源码构建。）</p><p>根据自己的情况，下载完成后解压即可：</p><p><img src="https://img.mahaofei.com/img/202310102108837.png" alt="image.png"></p><p><strong>（3）测试</strong></p><p>打开<code>instant-ngp.exe</code>，将<code>data\nerf\</code>下的<code>fox</code>文件直接拖到窗口中即可</p><p><img src="https://img.mahaofei.com/img/202310102110957.png" alt="image.png"></p><h1>Colmap计算相机位姿</h1><p><strong>（1）录制视频</strong></p><p>对于要三维重建的物体或场景，使用手机录制一段视频。</p><p>尽量均匀扫描，手机不要移动太快或抖动。</p><p><strong>（2）使用Colmap计算相机位姿</strong></p><p>在项目文件夹内新建一个文件夹，将录制的视频放进去。</p><p><img src="https://img.mahaofei.com/img/202310102113654.png" alt="image.png"></p><p><code>cd</code>到视频所在的目录下。在命令行内执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate nerf-ngp</span><br><span class="line">python ..\..\scripts\colmap2nerf.py --video_in desk.mp4 --run_colmap --overwrite</span><br></pre></td></tr></table></figure><p>需要等待较长的一段时间</p><p>完成后会出现分割好的image文件夹</p><p>再继续执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ..\..\scripts\colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 16 --overwrite</span><br></pre></td></tr></table></figure><p>在等待比较长的一段时间，完成。</p><h1>instant-ngp三维重建</h1><p>打开<code>instant-ngp.exe</code>，将desk文件夹整体拖进去就ok了</p><p><img src="https://img.mahaofei.com/img/202310102127121.png" alt="image.png"></p><p>视觉效果还是相当可以的，不过导出mesh模型效果比较差</p>]]></content>
    
    
    <summary type="html">Instant-NGP全称Instant Neural Graphics Primitives，它通过多分辨率哈希编码，解决了NeRF对全连接神经网络进行参数化时的效率问题，大大提升了网络的训练速度，使三维重建速度可以从几小时缩短到几秒钟。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="三维重建" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="三维重建" scheme="https://www.mahaofei.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    <category term="Nerf" scheme="https://www.mahaofei.com/tags/Nerf/"/>
    
  </entry>
  
  <entry>
    <title>Google_Mediapipe关节检测框架</title>
    <link href="https://www.mahaofei.com/post/5090460e.html"/>
    <id>https://www.mahaofei.com/post/5090460e.html</id>
    <published>2023-09-18T06:40:00.000Z</published>
    <updated>2023-09-18T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 准备工作</h1><h2 id="1-1-安装Baze">1.1 安装Baze</h2><p><strong>（1）下载Bazelisk</strong></p><p>MediaPipe使用bazel进行构建的，安装bazellisk主要是为了更新bazel</p><p>进入bazel的项目<a href="https://github.com/bazelbuild/bazelisk/releases">https://github.com/bazelbuild/bazelisk/releases</a>，下载二进制文件<a href="https://github.com/bazelbuild/bazelisk/releases/download/v1.18.0/bazelisk-linux-amd64">bazelisk-linux-amd64</a>。</p><p>然后将文件移动到<code>/usr/local/bin/bazel/</code>，并修改其可执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mv bazelisk-linux-amd64 /usr/local/bin/bazel</span><br><span class="line">sudo chmod u+x /usr/local/bin/bazel</span><br></pre></td></tr></table></figure><p>检查bazel是否安装成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel version</span><br></pre></td></tr></table></figure><p>可以查看到bazel的版本就算成功。</p><p><strong>（2）安装Bazel</strong></p><p>从 <a href="https://github.com/bazelbuild/bazel/releases">GitHub 上的 Bazel 版本页面</a>下载名为 bazel-version-installer-linux-x86_64.sh的shell脚本文件。</p><p>例如我本次安装的就是<code>bazel-6.3.2-installer-linux-x86_64.sh</code></p><p>执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">给bazel-6.3.2-installer-linux-x86_64.sh脚本可执行权限</span></span><br><span class="line">chmod +x bazel-6.3.2-installer-linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载bazel</span></span><br><span class="line">./bazel-6.3.2-installer-linux-x86_64.sh --user</span><br></pre></td></tr></table></figure><p>根据提示，将 <code>source /home/mahaofei/bin/bazel/bin/bazel-complete.bash</code> 添加到 <code>~/.bashrc</code> 中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;source /home/mahaofei/.bazel/bin/bazel-complete.bash&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><p>重启终端，使用<code>bazel --version</code>命令检查是否安装成功。</p><h2 id="1-2-下载Mediapipe">1.2 下载Mediapipe</h2><p><strong>（1）克隆项目</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:google/mediapipe.git</span><br></pre></td></tr></table></figure><p><strong>（2）安装opencv和ffmpeg</strong></p><p>进入克隆的项目中，为<code>setup_opencv.sh</code>添加可执行权限，并运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x setup_opencv.sh</span><br><span class="line">./setup_opencv.sh</span><br></pre></td></tr></table></figure><h1>2 使用</h1><p><strong>（1）python环境</strong></p><p>创建虚拟环境，并安装必要的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mediapipe python=3.8</span><br><span class="line">conda activate mediapipe</span><br><span class="line">pip install opencv-python</span><br><span class="line">pip install opencv-contrib-python</span><br><span class="line">pip install mediapipe</span><br></pre></td></tr></table></figure><p><strong>（2）使用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line">mp_drawing = mp.solutions.drawing_utils</span><br><span class="line">mp_drawing_styles = mp.solutions.drawing_styles</span><br><span class="line">mp_holistic = mp.solutions.holistic</span><br><span class="line"></span><br><span class="line">cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> mp_holistic.Holistic(</span><br><span class="line">    min_detection_confidence=<span class="number">0.5</span>,</span><br><span class="line">    min_tracking_confidence=<span class="number">0.5</span>) <span class="keyword">as</span> holistic:</span><br><span class="line">  <span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    success, image = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> success:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;Ignoring empty camera frame.&quot;</span>)</span><br><span class="line">      <span class="comment"># If loading a video, use &#x27;break&#x27; instead of &#x27;continue&#x27;.</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    image.flags.writeable = <span class="literal">False</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    results = holistic.process(image)</span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line">    image.flags.writeable = <span class="literal">True</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</span><br><span class="line">    mp_drawing.draw_landmarks(</span><br><span class="line">        image,</span><br><span class="line">        results.face_landmarks,</span><br><span class="line">        mp_holistic.FACEMESH_CONTOURS,</span><br><span class="line">        landmark_drawing_spec=<span class="literal">None</span>,</span><br><span class="line">        connection_drawing_spec=mp_drawing_styles</span><br><span class="line">        .get_default_face_mesh_contours_style())</span><br><span class="line">    mp_drawing.draw_landmarks(</span><br><span class="line">        image,</span><br><span class="line">        results.pose_landmarks,</span><br><span class="line">        mp_holistic.POSE_CONNECTIONS,</span><br><span class="line">        landmark_drawing_spec=mp_drawing_styles</span><br><span class="line">        .get_default_pose_landmarks_style())</span><br><span class="line"></span><br><span class="line">    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)</span><br><span class="line">    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#右手21个节点坐标</span></span><br><span class="line">    <span class="keyword">if</span> results.right_hand_landmarks:</span><br><span class="line">        <span class="keyword">for</span> index, landmarks  <span class="keyword">in</span> <span class="built_in">enumerate</span>(results.right_hand_landmarks.landmark):</span><br><span class="line">            <span class="built_in">print</span>(index,landmarks )</span><br><span class="line"><span class="comment">#鼻子坐标</span></span><br><span class="line">    <span class="comment">#print(results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE])</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;MediaPipe Holistic&#x27;</span>, cv2.flip(image, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">5</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">cap.release()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Google所开发的通用框架。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="实例分割" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="关节检测" scheme="https://www.mahaofei.com/tags/%E5%85%B3%E8%8A%82%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【模仿动作】从人类演示中学习机器人动作规划方法</title>
    <link href="https://www.mahaofei.com/post/d76756ed.html"/>
    <id>https://www.mahaofei.com/post/d76756ed.html</id>
    <published>2023-09-07T06:41:50.000Z</published>
    <updated>2023-09-07T06:41:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文总结</p><ol><li>人手臂与机器人的实施映射控制（视频、肌电信号、佩戴IMU）</li><li>人类控制机器人演示，再由机器人在其他环境实现相似任务</li><li>从人类视频学习人手动作意图，实现机器人末端的运动模仿</li><li>基于目标物体的位姿变化趋势实现模仿学习<br>目前从人类手臂关节直接学习机器人动作的方法，大多通过在手臂上安装传感器例如IMU等或通过相机视觉识别人手姿态实现遥操作。但是从人手视频直接提取关节用于陌生环境，没有相关的研究。</li></ol></blockquote><h1>1 MimicPlay: Long-Horizon Imitation Learning by Watching Human Play</h1><blockquote><p><strong>标题</strong>：模拟游戏：通过观看人类游戏进行的长期模拟学习<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://mimic-play.github.io/">https://mimic-play.github.io/</a>(code is coming soon)</p></blockquote><h2 id="1-1-目标问题-5">1.1 目标问题</h2><p>由于人类比遥控机器人能更快的完成长时间任务，因此启发从人类演示中学习机器人规划策略。</p><p>为了减少数据需求，采用人类与环境的交互视频作为数据。开发一个分层学习框架，从人类数据中学习潜在的规划控制方法。</p><h2 id="1-2-方法-4">1.2 方法</h2><p><img src="https://img.mahaofei.com/img/202309071622980.png" alt="image.png"></p><p><strong>（1）从人类数据中学习潜在规划</strong></p><p>给定输入：视觉观察$o_t$，未来的目标图像$g_t$，当前手部位置$l_t$<br>训练过程中，$g_t$被视为执行动作后的未来帧<br>规划期的目标是根据视频提示V生成目标图像的动作规划。</p><ol><li>人类演示数据收集</li><li>跟踪人手三维轨迹：使用双目相机获取人手的3D轨迹，利用现成的<a href="https://github.com/ddshan/hand_object_detector">手部检测器</a>确定2维图像中的手部位置，然后利用双目视图重建手的3D轨迹。</li><li>学习潜在规划：使用两个卷积网络分别将当前图像和目标图像处理为低维特征，再与手部位置连接在一起，使用MLP处理为潜在规划特征。生成3D手部运动轨迹。为了处理同一个任务的不同方式的实现，使用高斯混合模型对潜在规划的轨迹分布进行建模。</li></ol><p><strong>（2）计划引导的多任务模仿学习</strong></p><p>机器人的底层策略使用行为克隆算法进行训练，使用通过遥操作收集的机器人演示数据。</p><ol><li>视频条件下的潜在规划生成：使用遥操作机器人任务视频来提示训练时潜在规划器生成相应的规划。</li><li>基于Transformer的规划引导模仿：将机器人手上相机观察和本体姿态信息处理为低维向量，再与潜在计划连接起来，通过Transformer架构来计算最终的机器人控制命令。</li><li>多任务学习</li></ol><h2 id="1-3-思考-4">1.3 思考</h2><p>李飞飞团队的作品，从视频中学习人手的运动轨迹，code is coming soon，等待后续再细看。</p><h1>2 One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</h1><blockquote><p><strong>标题</strong>：通过领域自适应元学习观察人类的一次性模仿<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<br><a href="https://github.com/tianheyu927/mil">官方版: https://github.com/tianheyu927/mil</a><br><a href="https://github.com/daiyk/daml_pytorch">Pytorch版: https://github.com/daiyk/daml_pytorch</a></p></blockquote><h2 id="2-1-目标问题-2">2.1 目标问题</h2><p>提出一种从人类视频中进行学习的方法，通过使用各种先前任务的人类和机器人演示数据，使机器人执行人类演示的任务。</p><h2 id="2-2-方法-2">2.2 方法</h2><p><strong>（1）问题描述</strong></p><p>将先验知识和少量证据组合起来，形成一个人类演示的形式。从中推断出完成任务的机器人的策略参数。</p><p><strong>（2）领域自适应元学习</strong></p><p>能够处理从人类的视频演示中学习，学习一组参数，以便在人类演示的基础上进行梯度下降后，模型可以有效地执行新任务。</p><p>由于人类和机器人的动作无法直接对应，因此考虑学习只对策略激活起作用。</p><p><strong>（3）学习时间适应目标</strong></p><p>要从人类的视频中进行学习，需要捕获视频中的相关信息，例如人类的意图和任务的相关对象。要确定哪些行为正在被演示，哪些对象是相关的，通常需要同时检查多个帧来确定人类的运动。因此本文的学习适应目标将多个时间步长耦合，从多个时间步骤对策略进行操作。</p><p>此处使用卷积网络来表示自适应目标，使用递归神经网络LSTM进行时间卷积。</p><p><strong>（4）概率解释</strong></p><p>将学习到的自适应目标纳入到概率图模型的框架中，推断特定任务的策略参数。</p><h2 id="2-3-思考">2.3 思考</h2><p>思路看起来很可以，就是数学推理比较复杂，很难看得懂。</p><h1>3 Waypoint-Based Imitation Learning for Robotic Manipulation</h1><blockquote><p><strong>标题</strong>：基于航路点的机器人操纵模拟学习<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></p></blockquote><p>行为克隆BC目前有很多问题，路径点可以通过减少BC的范围来解决这个问题，但是传统路径点需要人工监督标注。</p><p>本文提出了线性运动近似的，模仿学习的自动轨迹点提取模块，将演示分解为一组轨迹点，进行线性插值，近似实现演示动作。</p><p>并且该方法可以与任务BC算法相结合，提高其成功率。</p><h1>4 Building Robot Intelligence by Scaling Human Supervision</h1><blockquote><p><strong>标题</strong>：通过扩展人类监督构建机器人智能<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2021</p></blockquote><h2 id="4-1-研究背景">4.1 研究背景</h2><p><strong>几十年来，我们一直在想象一个机器人可以充当个人助理的世界，能够完成我们每天做的各种任务和家务，比如做饭、打扫卫生、洗衣，甚至组装橱柜。机器人领域的研究人员一直致力于实现这一梦想。然而，不幸的是，今天的自主机器人远未达到操纵能力的水平。尽管研究在使机器人能够完全自主地完成特定任务的方面取得了令人印象深刻的进展，包括拾取物体，或将它们堆叠在一起。但机器人和人类的操作能力之间存在很大差距。人类智能地使用物体，并在日常生活中以丰富的方式与它们互动，比如当我们用刀切菜做饭时，或者用螺丝刀拧紧螺丝组装橱柜时。这种有目的的与物体的互动对机器人来说是十分困难的。</strong></p><p><strong>作为人类，我们在一生中积累了一系列不同的先前经验，这些经验我们可以在日常生活中借鉴。此外，即使我们不知道如何做某事，我们也可以通过观看其他人的视频来快速学习，例如通过观看YouTube上其他人组装橱柜的视频来学习如何组装橱柜。这就提出了一个问题——我们是否可以类似地为机器人提供丰富多样的先前经验，并使他们能够从这些数据集中学习操作技能？</strong></p><p><strong>这激发了数据驱动的机器人，这是一种有用的范式，让机器人从大型数据集中学习操作。但是这种方法通常有两种变体，第一种是机器人自行收集数据，数据一开始是随机的，但会随着时间推移慢慢变好。由于机器人必须自己学习，限制了可以学习的人物的复杂性。第二种则是人类控制机器人并引导它完成任务，然是这通常是不可扩展的，因此可以收集的数据量很小，这再次限制了任务的复杂性。</strong></p><p><strong>相比之下，计算机数据和自然语言处理等领域已经通过大规模高质量数据集开创了前所未有的成就，我们希望在机器人技术方面看到类似的突破。</strong></p><p><strong>为了复制这一成功经验，并解决数据驱动机器人中任务复杂性有限的问题，我们需要解决两个关键挑战。首先，收集大规模的人类数据具有挑战性。在计算机视觉领域，注释可以由人类直接标注，很容易实现并行标注和大规模人员标注。相比之下，在机器人技术中，人类必须与机器人实时互动，引导机器人完成任务。这使得提供直观和可扩展的方法来收集来自多个人的数据变得很困难。其次，从大规模数据集中学习可能并不简单。在其他领域，我们可以训练网络预测注释，这些注释对应的都是真实的标签。然而在机器人技术中，没有一种真正的方法来执行任务，不同的人可能会收集不同的轨迹，不同的策略，我们需要确定如何从这些数据集中学习。</strong></p><h2 id="4-2-研究目标">4.2 研究目标</h2><p>第一部分，讨论了如何通过充满丰富交互的人类监督来收集大规模数据，这些数据体现了机器人的类人操作能力。包括一个为解决机器人操作中对大规模人类数据集需求构建的平台，和现实世界的数据收集。</p><p>第二部分，讨论了如何使用丰富的数据集来学习机器人操作技能。</p><p>第三部分，讨论了该方法可能的进一步拓展和应用。</p><h2 id="4-3-收集人类操作数据">4.3 收集人类操作数据</h2><p><strong>为了使数据能够捕捉人类的操作，首先数据应该在所展示的解决问题的策略的种类上是多样化的。作为人类，我们很清楚什么时候应该尝试不同的方法类实现目标，而机器人应该从所有这些策略中学习，因为在特定的情况下可能需要其中的一种。其次，数据应该包含灵巧的操作，我们希望我们的机器人了解它们如何通过武力方式操作物体来实现预期的结果。最后，数据应该是大规模的，人类非常擅长在无数情况下解决问题，但机器人还不能做到这一点。我们向他们展示的数据越多，他们也就越有可能获得这种能力。</strong></p><p>在这一部分，我们提出了RoboTurk平台，一个数据收集平台，允许人类实时远程操作机器人。操作员在他们的网络浏览器中看到机器人的工作空间的视频流，用他们的智能手机控制机械臂，他们手机的运动与机器人的运动相耦合，可以自然地控制手臂，这使得人们可以轻松的提供任务演示，连接的过程快速而简单。实验表明，这些数据能够在多步骤操作任务上进行策略学习，并且在策略学习的过程中使用大量的演示可以在学习一致性和最终性能方面带来好处。</p><h2 id="4-4-从大规模人类数据集中学习操作">4.4 从大规模人类数据集中学习操作</h2><p><strong>在这一部分，我们讨论了机器人如何才能够大规模人类数据集中学习操作技能。此类数据集可能表现出巨大的多样性，并由次优解决方案组成，因此从中学习具有挑战性。我们提出了一种从大规模演示数据集中学习的新算法，即无规模交互的内隐强化IRIS算法。IRIS将控制问题分解为目标条件的低级控制器和高级目标选择机制，前者模仿短演示序列，后者为低级控制器设置目标，并选择性的组合部分次优解决方案，从而更成功的完成任务。</strong></p><p>尽管最近在模仿学习和强化学习方面缺乏开源的人类数据集和可重复的学习方法，使得评估该领域的状态变得困难。我们对六个离线学习机器人操作算法，在五个仿真和三个不同复杂度的真实环境中进行多阶段操作任务测试。我们得到了一系列经验，包括对不同算法设计选择的敏感性，对演示质量的依赖性，以及由于训练不同的目标而导致的不同停止标准。我们还强调了从人类数据集学习的可能性，例如在当前强化学习方法范围之外的具有挑战性的多阶段任务中学习熟练策略的能力，以及轻松扩展到只有原始感官信号可用的自然、真实世界操作场景的能力。我们已经开源了我们的数据集和所有算法实现，以促进未来的研究和从人类演示数据中学习的公平比较。</p><h2 id="4-5-使用人类数据集构建能力更强的机器人">4.5 使用人类数据集构建能力更强的机器人</h2><p>这一部分探讨了几个不同的应用程序，使我们更接近于我们希望的机器人能够在未来能够处理的任务。主要探讨了多任务领域（如厨房），高精度操作，和需要协作的多臂操作任务。</p><p><strong>模仿学习方法的一个常见的局限是由于训练集中的数据有限，在所展示的行为之外进行泛化是一个开放的挑战。例如在厨房场景中，我们可能希望机器人实现多种可能的配置，具有多个要操作和交互的对象，如食物、出轨、微波炉、水槽等。本章我们介绍了通过模仿进行任务泛化，这是一种新颖的模仿学习框架，使机器人能够从少量的人类演示中有效的学习复杂的现实世界操作任务。合成收集的演示中未包含的新行为。多任务领域通常呈现出一种潜在的结构，不同的任务轨迹在状态空间的公共区域相交。GTI是一个两阶段在线模仿学习算法，该算法利用交叉结构来训练目标导向的策略，这些测类推广到看不见的开始和目标状态组合。在GTI的第一阶段，我们训练了一个随机策略，该策略利用轨迹交叉点来有能力从不同的演示轨迹中组合行为在一起。在GTI的第二阶段，我们从第一阶段的无条件随机策略中收集了一小组推理，并训练一个目标导向的agent来推广到新的启动和目标配置。我们在模拟领域和现实世界中具有挑战性的长期机器人操作领域中验证了GTI。</strong></p><p><strong>模仿学习方法通常也很难完成高精度的操作任务，因为它们需要一系列精确的动作才能取得有意义的进展，比如机器人将pod插入咖啡机制作咖啡。经过培训的策略可能会在这些场景失败，因为行动上的微小偏差可能会导致策略进入未被演示覆盖的区域。基于干预的策略学习是解决这一问题的一种替代方案——它允许操作员监控经过训练的策略，并在遇到故障时接管控制权。</strong> 我们扩展了RoboTurk，使远程操作员能够监控和干预经过培训的政策。我们开发了一个简单的在系统收集的新数据上迭代训练策略的有效算法。我们证明，根据我们基于干预的系统和算法收集的数据训练的代理优于根据非干预演示者收集的同等数量样本训练的代理，并进一步证明，我们的方法在从具有挑战性的机器人线程任务和咖啡制作任务。</p><p><strong>最后，虽然通过远程操作收集的人类演示中的模仿学习（IL）是教授机器人操作技能的强大范式，但它大多局限于单臂操作。然而，许多现实世界中的任务需要多个手臂，例如举起重物或组装桌子。不幸的是，将IL应用于多臂操作任务一直具有挑战性</strong>——要求人类控制多个机械臂可能会带来巨大的认知负担，而且通常最多只能控制两个机械臂。为了应对这些挑战，我们介绍了多臂RoboTurk（MART），这是一个多用户数据收集平台，允许多个远程用户同时远程操作一组机械臂，并收集多臂任务的演示。使用MART，我们从几个地理位置不同的用户那里收集了五个新的双臂和三臂任务的演示。我们表明，从这些数据中学习因此给集中式代理带来了挑战，这些代理直接尝试同时对所有机器人动作进行建模，并对数据进行全面不同的策略架构，对我们的任务具有不同的集中程度。最后，<strong>我们提出并评估了一个基本残差策略框架，该框架允许经过训练的策略更好地适应多臂操作中常见的混合协调设置，并表明用去中心化残差模型增强的集中式策略在我们的基准任务集上优于所有其他模型。</strong></p><h1>5 Understanding and Learning Robotic Manipulation Skills From Humans</h1><blockquote><p><strong>标题</strong>：从人类身上理解和学习机器人操作技能<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="5-1-背景和动机">5.1 背景和动机</h2><p>制造机器人的性能是通过它们的精度、准确性和速度来衡量的。这导致了刚性和笨重的机器人的设计，这些机器人与人类一起工作是不安全的。他们的控制器在不使用力传感的情况下执行预先编程的轨迹，使其对位置误差高度敏感。通过使用夹具和夹具，例如装配线上的夹具，可以减少环境中的不确定性。</p><p>现实世界的环境需要低重量、人类安全、扭矩控制的机器人。<strong>如果机器人要在环境不断变化、感知能力有限的日常环境中真正发挥作用，就必须找到通过预编程轨迹控制机器人的替代方案。一种很有前途的方法是将复杂的任务划分为健壮且可重用的动作或基元。</strong> 在本文中，我们通过使用可推广的顺应原语，为在更高抽象级别上编程机器人奠定了理论和实践基础。</p><p><strong>方法的第一步是从人类演示中收集数据。然后，我们将数据分割成在任务期间执行的动作序列——基元。接下来，我们将数据投影到一个低维和物理意义的空间中，使我们能够理解人类的策略。最后，我们将这些行为编码到能够执行任务的机器人控制器上。</strong> 此外，我们的框架利用视觉和触觉反馈，让人类处于故障恢复和持续学习的循环中。</p><h2 id="5-2-从人类演示中学习">5.2 从人类演示中学习</h2><p><strong>本文的工作属于示范学习LfD的范畴。人类在操作方面非常有能力，因此从人类演示中收集数据使学习机器人新行为的一种流行方式。事实上，我们不仅可以学习单臂行为，还可以学习双臂行为，我们的系统已经证明了这一点，并在其它工作中进行了探索。</strong> 大多数先前的工作侧重于从视觉数据中学习。而我们的工作强调在执行富含接触的任务时里和数据的重要性。</p><p>近年来，互动学习是一个不断发展的研究领域，它使人类保持在循环学习的过程中。为了实现类似的工作方式，我们的框架通过使用触觉接口使人类处于循环中，我们系统手机故障恢复数据可以与从故障中学习的工作相结合，易产生更稳健的自主行为。</p><h2 id="5-3-机器人基本单元">5.3 机器人基本单元</h2><p>在这项研究中，基元是有一个兼容的框架和一组所需的任务参数定义的。顺应性框架是一个原点和空间中的三个方向，我们沿着它们控制运动和顺应性。柔顺框架附着到要操纵的对象上。任务参数包括所需的力、力矩、位置和方向。这种与机器人无关的任务规范提供了一种有物理意义的低维表示。</p><p><strong>基元库。生成一个由n个基元组成的库，对基本的操作技能进行编码，通过组合这些基元，可以以一种方式解决新的复杂任务，即所需基元的数量不会随着任务数量的增加而增加。广义上讲，关于运动基元的文献主要解决了三个主要的研究问题，生成运动基元，参数化基元以及将基元组合在一起以成功完成任务。</strong></p><p><strong>基元生成。基元的生成可以通过手动编码所需策略或者从数据中自动提取策略来实现。先前的研究已经转向人类寻求灵感，并试图提取策略。</strong></p><p><strong>基元参数化。参数化基元处理定义动作的方式。基元通常使用轨迹段进行参数化。用轨迹定义运动基元已经被证明是成功的，但该方法假设环境不确定性较低。</strong></p><p>兼容基元。我们使用框架的概念来参数化我们的原始控制器。先前的研究使用了以对象为中心的任务控制器的相同概念。然而，与本文中的工作相反，仅从视觉数据中提取控制器参数，我们认为，在处理复杂任务是，考虑序列数据是有利的。在存在位置不确定性的情况下，顺从性在任务中也起着重要作用，对于接触丰富的任务，比如抓获或本文中研究的任务。例如，基元的概念，其中柔顺基元就是用于实现对小物体的鲁棒抓取。</p><p><strong>使用基元进行规划。组合运动基元的概率方法利用了决策过程的固有不确定性，这些方法可以是完全自动化的，也可以是使用混合的方法，将自动决策算法与用户指定的图像相结合。</strong> 最近的其他方法使用语义模型来学习基于是觉得操纵任务计划，或者一些方法使用接触而不是视觉来指导决策过程。</p><h2 id="5-4-多层控制体系结构">5.4 多层控制体系结构</h2><p>该体系结构由三层感知-动作反馈回路组成。每个层都以不同的抽象级别运行，并以不同的频率运行。</p><p>在最底层，完全依赖于控制器，并有助于高速率感官反馈和控制的集成，以实现安全和可预测的机器人运动。该级别向机器人电机发送命令，因此，感知动作回路必须以非常高的频率闭环。下一层向全身控制器提供输入，从而以较慢的速率运行。最后，执行计算成本高昂的感知和规划的最高抽象级别以最低的速率运行。</p><p>全身控制级别使用任务优先级。基于优先级的控制使我们在设计原始动作时能够专注于对象及其几何约束。完整的机器人行为可以被视为由具有不同优先级的不同任务组成。例如，高优先级任务可以是避免奇异配置，另一个任务可以处理障碍或摩擦约束。以类似的方式，有一项任务专门负责实现操纵对象之间所需的几何关系。此任务被编码为基元。换句话说，基本动作只涉及对象，因为任务的所有其他方面，包括非几何约束和机器人运动学，都由控制器的其他组件处理。</p><p>基于优先级的全身控制使用零空间投影来确保满足所有不同的约束。此外，操作空间公式——使用Jacobian的动态解耦逆来计算递归零空间投影——确保具有不同优先级的任务动态解耦。先前的工作也在操作原语的上下文中使用了这种分层框架。</p><h1>6 Scaling Deep Robotic Learning to Broad Real-World Data</h1><blockquote><p><strong>标题</strong>：将深度机器人学习扩展到广泛的真实世界数据<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="6-1-背景">6.1 背景</h2><p>机器人的一个长期梦想是一种通用的家用机器人，它可以被放置在家庭环境中，也许是它以前从未见过的，并执行一系列有用的任务，如煮咖啡、清洁和烹饪。这样一个机器人无疑将在经济上和通过他们的帮助提高人类生活质量方面产生巨大影响。当然，这个梦想仍然是这样，在实现这个目标方面存在着无数的挑战，包括更好的机器人硬件、电池技术和传感。然而，核心挑战之一在于泛化，即机器人在新的物体、环境和任务中取得成功的能力。事实上，人类有这种能力，正是这种能力使我们能够完成像煮一杯咖啡这样的任务，即使在有新厨房和新物体的情况下也是如此。因此，相关的问题仍然存在——我们如何训练我们的机器人，使其能够广泛推广。</p><p>解决这个问题的一种方法是利用人类的直觉，以及用于机器人规划和控制的手部设计系统和表示。在这种方式中，人类定义了相关的对象类及其属性和关系（例如，颜色、形状、姿势、上方与下方等），然后可以使用状态估计技术从传感器观测中测量这些量，并且可以使用经典的搜索和运动规划方法来执行任务。然而，至关重要的是，这种方法是基于人类对相关对象、特性的规范，在某些情况下，甚至是每个环境和任务的对象的3D模型，这阻碍了这种方法在存在新对象和环境的情况下容易使用。</p><p>有一项工作研究了机器人如何在制造通用机器人时不依赖人类的直觉和手部设计，而是纯粹通过数据和自己的试错来学习行为。具体来说，深度强化学习研究了学习深度神经网络策略的问题，该策略在给定传感器观测的情况下采取行动，从而使策略通过从交互中学习来最大化一些定义的奖励。原则上，这种方法可以让机器人完全靠自己学习技能，而只有少量的成功指标。然而，在实践中，在机器人上运行深度强化学习带来了许多挑战，例如在重置和奖励方面需要人工监督。然而，最关键的是，深度强化学习通常需要在目标环境中进行数百万次在线环境交互才能进行学习，并且一旦完成，所学习的策略只对所训练的环境和数据有效。因此，标准的深度强化学习在消除了人手设计的大部分需求的同时，仍然没有立即让我们更接近能够在新环境和任务中操作的机器人。</p><p>退一步看，人们可能会从机器学习的其他领域寻找灵感，特别是过去几年里，自然语言处理和计算机视觉的研究领域取得了巨大进展。主要基于一个简单的配方：：（1）大量、多样化的离线数据集，（2）自监督或廉价监督的训练目标，以及（3）表达性的端到端训练的神经网络模型。这种基础模型的范例特别令人兴奋，因为这些模型表现出了令人印象深刻的泛化——例如，来自ImageNet的视觉模型可以适应癌症检测这样的全新任务，而像BERT这样的预训练语言模型的应用范围从医学编码到视觉问答。事实上，这种概括水平正是我们希望在一个通用机器人中看到的，它可以被放入一个新的环境中，并快速地学会解决新的任务。</p><p>所以为什么这个配方还没有在机器人中实现呢？现实世界中的机器人操作带来了许多独特的挑战，这使得直接复制这一配方变得困难——我们既没有足够大和多样化的机器人交互数据集，也不清楚什么类型的学习算法或监督来源可以使我们从这些数据集中大规模学习有用的技能。本文的目标在于解决这些挑战，并在机器人操作的背景下复制大规模数据和学习的配方。具体来说，我的研究集中在回答三个广泛的问题上。首先，我们如何可伸缩地收集在物理世界中交互的机器人的大型和多样化的数据集？其次，我们如何设计能够消耗如此广泛的离线数据的自我监督强化学习算法，这些数据可能来自非专家，缺乏奖励标签，并从中学习达到看不见的目标？第三，我们如何解锁网络上存在的广泛数据来源，如人类视频和自然语言，以便在我们的机器人中进行更有效的学习？</p><h1>7 Learning Perceptual Prediction: Learning From Humans and Reasoning About Objects</h1><blockquote><p><strong>标题</strong>：学习感知预测：向人类学习和对物体的推理<br><strong>作者团队</strong>：University of Pennsylvania<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="7-1-目标问题-2">7.1 目标问题</h2><p>人类在使用各种各样的感知模式进行预测时，主要关注从视觉学习。人类的视觉似乎经过了高度的优化，可以用于预测未来的视觉观测。</p><p>研究使用视觉传感器的预测也提供了许多实际优势。首先，高质量的相机很容易获得，并且尺寸、重量和功率要求都很低，这使得它们可以被包括在大多数机器人平台上，由于相机在非机器人应用中的普及，它们已经被商品化了。其次，视觉观察提供了关于环境的丰富信息，包括姿势、纹理和语义，这些信息是其他传感器无法轻易匹配的。获取大量丰富的世界信息对于使代理人能够与世界互动非常重要。</p><p>当前学习动作条件视觉预测模型的方法依赖于访问大量的具体数据，这是昂贵且耗时的，从而阻止了基于视觉预测的方法在许多应用中使用。对于机器人来说尤其如此，因为收集大量机器人数据既昂贵又耗时，而且可能不安全。现有工作表明，基于视觉预测的方法随着数据量的增加而扩展良好，因此找到新的数据来源对于使这些模型能够广泛使用至关重要。</p><p>在这篇论文中，我提出了三种不同的方法来利用非机器人数据来改进视觉预测和机器人控制。在前两项工作中，我使用人类数据来提高机器人的性能，而在第三项工作中我使用现有的非机器人数据集来实现以对象为中心的预测框架。</p><h2 id="7-2-从人类学习">7.2 从人类学习</h2><p>大型和多样化的真实世界数据集对于广泛的泛化和高性能至关重要。大型数据集可以通过自动化管道或人类远程操作进行收集。自动化数据收集过程可以收集非常大的数据集，但在到达环境中感兴趣的部分以及需要与环境进行大量交互方面存在问题。习得的探索策略可以提高代理达到有趣配置的能力，但这些方法仍然需要大量的探索。第二种方法是收集人类演示的远程操作轨迹。这种方法允许数据集轻松地达到有趣的和任务相关的配置。然而，它受到了影响，因为它依赖于人类来操作机器人，这需要训练有素的操作员，而且很快变得非常昂贵。通过从人类学习中汲取灵感，可以找到一种避免这两种方法困难的替代方法。</p><p>人类不仅有能力从自己与世界的互动中学习技能，也有能力通过观察他人来学习技能。考虑一个婴儿学习使用工具。为了成功地使用一个工具，它需要学习该工具如何与其他对象交互，以及如何移动工具来触发这种交互。这种直观的物理概念可以通过观察成年人如何使用工具来学习。更普遍地说，观察是关于世界以及行动如何导致结果的强大信息来源。然而，在存在身体差异的情况下（例如成人身体和婴儿身体之间），利用观察是具有挑战性的，因为演示者和观察者的行为之间没有直接对应关系。来自神经科学的证据表明，人类可以有效地推断出这种对应关系，并利用它们从观察中学习。</p><p>利用对人类的观察提供了大幅增加可用数据的规模和有用性的机会。与自主收集的数据不同，人类数据可以只关注配置空间中有趣的部分，避免危险或无聊的交互。与通过远程操作收集的数据集不同，人类数据集可以具有更大的规模。公开可用的人类数据集，如Ego4D或SomethingSomething，包含数十万个视频和数千小时的镜头，分布在数百个任务和数十个地点。这些数据集与自主收集的数据集的大小相当]，并比通过远程操作收集的数据集中的大小高出一到两个数量级。更重要的是，从人类的无行动观察中学习，开启了从互联网上公开生成的视频中学习的可能性，比如YouTube上的视频，这些视频提供了更多数量级的数据。目前的方法只能将这些数据的有限子集用于特定任务，但我们的工作为更广泛的利用提供了一步。</p><p>在这篇论文中，我们考虑了这样一个问题：主体能否学会利用自己的互动和其他主体的被动观察来解决任务？我们在两个环境中探讨了这个问题，第一个是学习动作条件视觉预测模型，第二个是端到端强化学习策略。</p><p>在第3章中，我们提出了一种使用人类的无动作数据和主体自己的探索来执行强化学习的方法。我们提出了克服野外人类数据和模拟机器人数据之间的域转换的方法，将动作添加到无动作的人类数据中，以及估计人类数据的奖励的方法。通过利用在现实世界中收集的人类视频，我们能够加快模拟机器人代理的学习速度。</p><h2 id="7-3-对象推理">7.3 对象推理</h2><p>虽然从人类观察中学习可以获得大量数据，但它并不能回答应该学习什么的问题。我们重点学习端到端模型，这些模型直接从传感器输入映射到预测的未来帧或内部结构很少的期望动作。通过为任务选择正确的归纳偏差集，并利用在非机器人数据上预训练的现有模型，我们应该能够用更少的数据训练我们的模型，并实现更高的性能。正确设计学习问题的结构也可以使模型更容易地用于下游任务。我们关注的是假设世界是由物体组成的简单归纳偏见。</p><p>大多数用于操纵的动态交互可以通过将场景分解为对象来建模。虽然有些材料，如液体或颗粒介质，不容易被表示为对象，但大多数操作任务都涉及操作离散对象。分拣箱子、重组房间，甚至喝杯咖啡，这些任务主要由与离散对象的交互控制。</p><p>以对象为中心的预测模型在预测和困难任务方面表现出了成功的性能。通过将场景分解为离散对象，这些预测模型可以在更长的时间范围内保持每个对象的内聚性。此外，通过将世界状态内部表示为对象集合，以对象为中心的预测模型可以轻松地与规划者对接，并提供一个非常可解释的界面，有助于调试和验证。</p><h1>8 Affordances from Human Videos as a Versatile Representation for Robotics</h1><blockquote><p><strong>标题</strong>：人类视频作为机器人的通用表示<br><strong>作者团队</strong>：CMU, Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://robo-affordances.github.io/">https://robo-affordances.github.io/</a></p></blockquote><h2 id="8-1-目标问题-2">8.1 目标问题</h2><p>从人类视频中学习可操作的动作表示，该模型在未来帧的监督下预测接触点和轨迹路径点。</p><p>论文主要关注三个问题：</p><ol><li>如何表示可操作性？</li><li>如何以数据驱动和可扩展的方式学习这种表示？</li><li>如何实现跨机器人的视觉启发的方法部署？</li></ol><p>对应这三个问题，本文提出了以下三种观点：</p><ol><li>接触点和接触之后的轨迹是比较好的机器人操作的表示方法；</li><li>利用了自我中心的数据集，聚焦于所有有人类的帧，来预测接触点和接触之后的轨迹，通过使用现成的工具来估计自我运动、人体姿势和手-物体交互；</li><li>实现了一种称为Vision-Robotics Bridge(VRB)的方法，实现这些功能与不同类型机器人的无缝集成。</li></ol><h2 id="8-2-方法-2">8.2 方法</h2><p><strong>（1）可操作性表示</strong></p><p>提取人类启示的最自然的方式是观察人们如何与世界互动。常规的思路是从视频中准确模拟人类的运动，但这导致了一个以人为中心的模型，很不容易推广，因为人类的形态和机器人完全不同。</p><p>因此本文采用机器人需求驱动的第一性原理，机器人的本体的信息通常是已知的，因此使用运动规划达到3D空间中的点是很容易实现的，关键难点在于与环境的互动位置在哪里，以及接触之后如何移动。</p><p>受此启发，采用接触点 $c$ 和接触之后的轨迹 $\tau$ 作为视觉启发的简单操作表示，可以很容易的传递给机器人。其中 $\tau=f(I_t,h_t)$，$I_t$ 是时间步长 $t$ 的图像，$h_t$ 是像素空间中人手位置。</p><p><strong>（2）从自我中心的视频中学习操作</strong></p><p>接下来的问题是如何处理视觉输入的人体或手，从人类视频中提取接触点 $c$ 和轨迹 $\tau$。</p><ol><li>从人类视频中提取操作</li></ol><p>对于给定的视频 $V$，例如人开门，使用现有的手部对象检测模型，对每一帧图像 $I_t$ 生成手的2D边界框和离散接触变量 $o_t$，使用这些信息，我们可以过滤每个图像中 $o_t$ 表示接触的帧，从而找到发生<strong>接触的第一个时间步长</strong> $t_{contanct}$。</p><p><strong>手的像素空间位置构成了接触之后的轨迹</strong> $\tau$，为了提取接触点，我们使用手边界框以及颜色分割来找到手与其他物体边界框相交的点，利用高斯混合模型拟合这些接触点。</p><p>同时要考虑，一个人打开门的时候，人手不仅会移动，<strong>相机也会移动</strong>，需要补偿相机的运动，使用但应矩阵来解决这一问题，通过匹配连续帧之间的特征来获得单应性矩阵，产生变换后的轨迹。</p><p>需要完成<strong>视觉的转移</strong>，即训练视频中包含人手，但机器人任务中的视角不会有，因此考虑将所有的可操作性映射回第一帧，即人类还没有进入场景时。如果人总在视频中，要么将人裁剪出去，要么丢弃。</p><ol start="2"><li>训练模型</li></ol><p>以输入图像为条件，训练模型预测接触点和接触后的轨迹。然而由于学习的任务是多模态的，比如人从桌子上拿起杯子可能是要喝水或者倒到其他地方，因此考虑建立空间概率分布，预测多个heatmap处理这一问题。</p><p>输入图像使用ResNet进行编码，给出潜在空间表示，然后使用卷积层将这个潜在表示投影到K个概率分割中，得到GMM均值的标签的估计。</p><p><img src="https://img.mahaofei.com/img/202311011426427.png" alt="image.png"></p><p>为了估计接触后的轨迹，本文使用基于Transformer的预测。给定场景中，人类可能与许多对象进行交互，这些对象可能不存在在训练数据中，因此我们通过对接触点周围进行采样来解决，实现更好的泛化。</p><p><strong>（3）机器人学习</strong></p><p>本文用于引导现有的机器人学习方法，考虑了四种不同的机器人模式。</p><p><img src="https://img.mahaofei.com/img/202311011430303.png" alt="image.png"></p><ol><li>离线数据采集中的模仿学习</li></ol><p>给定一个图像输入，模型产生接触点和轨迹，我们将这一组数据存储在数据集中。收集到足够的数据后，我们使用模仿学习来控制策略，实现特定的任务。</p><ol start="2"><li>自由奖励的探索</li></ol><p>目标是发现尽可能多的不同技能，然而现实中从头开始探索效率太低了，因为机器人可能会花费大量时间尝试探索，但仍然无法学习有意义的技能来解决人类想要的任务。</p><p>我们考虑先收集数据，然后对所有轨迹进行排序。对于后续的数据采集从高度探索性的轨迹开始进行引导，进一步探索。</p><ol start="3"><li>目标条件的学习</li></ol><p>利用已知的知识，例如打开的门的图像，监督其进行探索学习。</p><ol start="4"><li>可操作性作为动作空间</li></ol><p>将机器人在连续空间中的操作，以空间的方式进行参数化，为每个位置分配一个基元。通过学习获得大量预测，利用GMM拟合到这些轨迹点上，获得离散的接触点和轨迹，机器人只需要在这个空间上进行搜索。</p><h2 id="8-3-思考-2">8.3 思考</h2>]]></content>
    
    
    <summary type="html">从人类抓取物体的视频中学习机器人的动作生成与规划方法，相关的研究的调研。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【模仿抓取】从人类演示中学习机械臂抓取</title>
    <link href="https://www.mahaofei.com/post/f9be0f4e.html"/>
    <id>https://www.mahaofei.com/post/f9be0f4e.html</id>
    <published>2023-08-13T02:50:01.000Z</published>
    <updated>2023-08-13T02:50:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 DemoGrasp: Few-Shot Learning for Robotic Grasping with Human Demonstration</h1><blockquote><p><strong>标题</strong>：DemoGrasp: 机器人抓握的少镜头学习与人体演示<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：</p></blockquote><h2 id="1-1-目标问题-3">1.1 目标问题</h2><h3 id="1-1-1-现存问题">1.1.1 现存问题</h3><p>现有的位姿估计方法要么需要计算目标物体的6D位姿，要么需要学习一组抓取点。前者的方法不能很好的扩展到多个对象实例或类，后者需要大型注释数据集，并且由于其对新几何图形的泛化能力交叉而受到阻碍。</p><h3 id="1-1-2-解决思路">1.1.2 解决思路</h3><p>通过简单简短的人类演示教机器人如何抓取物体，不需要许多带注释的图像，也不局限于特定的几何形状。</p><h3 id="1-1-3-大致方法">1.1.3 大致方法</h3><p>首先构建一个人机交互的RGB-D图像序列。利用该序列来构建表示交互的手和对象网格。完成重建对象形状的缺失部分，并估计重建模型与场景中可见对象之间的相对变换。最后将物体和人手之间的相对姿态的先验知识以及对场景中当前物体姿态的估计转化为机器人必要的抓取指令。</p><h3 id="1-1-4-引言总结">1.1.4 引言总结</h3><p><strong>为什么要做这个研究：</strong><br>当前的机器人抓取缺乏泛化能力，因为它们要么专注于估计物体姿态，要么学习抓取点，这需要物体的详细先验信息或大量注释。就像人手一样，机器人的抓取器和手臂的运动范围也有自然的限制，自由度也有限，这限制了它们可能的抓取姿势。虽然机器人抓取器和人手的运动模型可能有很大差异，但应该可以从人类操作中提取信息，并从中推断出目标机器人的足够抓取命令。通过有限的人类演示，机器人可以模仿人类行为，从而无缝抓取物体。</p><p><strong>本文主要做了什么：</strong><br>我们专注于这种模仿，机器人反映了人类的互动，如图1。该任务可以分为视觉感知和解释部分，其中人类教员演示先验操纵（Demo），机器人从中推断出操纵当前场景所需的抓握信息（抓握）。如果从人手到机器人抓取器有足够的映射，将任务分解为这两个阶段可以使我们的方法扩展到大量不同的抓取器。最终，这为通过自然人类演示来教授机器人铺平了道路，从而实现更高水平的自动化，尤其是在结构较少的环境中。</p><p><strong>本文大致是如何实现的：</strong><br>在从各种不同的角度向机器人演示物体（Demo）的过程中，我们的方法不断跟踪手和物体，这些手和物体被融合到截断有符号距离场（TSDF）中，用于3D重建。使用手和对象的语义分割，可以分离并进一步处理重建，以检索对象和手的完整3D表示。然后，我们利用MANO手部模型提取相关的3D手部网格，并将其与重建对象紧密对齐。在推理过程中，我们使用PPF FoldNet来预测对象是否存在，以及它从对象到相机空间的相对变换。然后，应用所估计的姿势从所估计的手网格导出最终抓握指令。</p><h2 id="1-2-方法-2">1.2 方法</h2><p><strong>总体流程：</strong></p><ol><li>在一组人类演示RGB-D图像上分割手和物体，并使用记录的深度图重建它们的形状</li><li>补全物体形状</li><li>提取手部姿态</li><li>估计对象的6D姿态，转换手部模型，推理抓取指令</li></ol><p><img src="https://img.mahaofei.com/img/202308131124867.png" alt="image.png"></p><h3 id="1-2-1-人-物交互的三维重建">1.2.1 人-物交互的三维重建</h3><p>使用MaskRCNN对手和物体进行分割，并且应用了二进制交叉熵来防止类间竞争。</p><p>利用分割后的深度图像，通过KinectFusion创建相应的TSDF体素，并通过输入帧与TSDF之间的ICP配准实现无漂移跟踪。<br>（因为家用物体几何形状简单，因此同时跟踪手和物体，手的结构复杂稳定了跟踪结果）</p><p>利用分割结果，通过两个单独的TSDF重建将手和物体分离开。</p><h3 id="1-2-2-物体形状补全">1.2.2 物体形状补全</h3><p>由于自遮挡和部分可见性，重建的模型还不完整。</p><p>使用3D CNN直接矫正TSDF体积，然后通过行进立方体进行形状提取。<br>（这里使用了UNet的3D变体，输入是64x64x64的体素，输出每个体素的预测分数表示体素是否被占用。</p><h3 id="1-2-3-手部姿态估计">1.2.3 手部姿态估计</h3><p>从重建的手形状中估计手部参数模型。</p><p>使用MANO手部模型，将手部姿态和形状参数映射到网格中。由于手部也受到了部分遮挡，因此使用辅助接触和碰撞损失联合训练CNN进行手部网格和物体网格估计。</p><p>为了进一步改进抓握位置，使用ICP将手部网格与手部TSDF体素对齐。</p><h3 id="1-2-4-抓取指令生成">1.2.4 抓取指令生成</h3><p>首先检索物体姿态，然后用它来变换手部网格，并用手部模型的拇指和食指计算抓握点。</p><h2 id="1-3-思考-2">1.3 思考</h2><ol><li>物体的三维重建可以采用其他方式，或者结合CAD模型补全的方式，相比于使用3D CNN预测效果会更好。</li><li>手部姿态的提取也可以考虑采用更新的算法，例如识别手部关键点，而不是预测手部网格的方式。</li><li>抓取姿态生成是直接使用拇指食指作为二指抓取姿态，是否可以考虑其他方式，提高抓取的可靠性。</li></ol><h1>2 Learning to Grasp Familiar Objects Based on Experience and Objects’ Shape Affordance</h1><blockquote><p><strong>标题</strong>：基于经验和物体形状的相似目标抓取<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS<br><strong>时间</strong>：2019<br><strong>代码</strong>：</p></blockquote><h2 id="2-1-目标问题">2.1 目标问题</h2><h3 id="2-1-1-现存问题">2.1.1 现存问题</h3><p>对于已知物体的抓取方法，物体具有抓取数据库，机器人通过估计物体姿态，然后利用国旅行假设找到合适抓取姿态，但是这些方法的缺点是不可能将所有对象的模型都放入机器人的数据库。</p><p>需要一种从以前的经验推广到新对象的模型的能力。</p><h3 id="2-1-2-解决思路">2.1.2 解决思路</h3><p>整合人类抓握经验中的关键线索（拇指指尖和手腕的位置方向），提出了一种有效的抓握方法。</p><h2 id="2-2-方法">2.2 方法</h2><h3 id="2-2-1-从不完整点云上生成抓取点">2.2.1 从不完整点云上生成抓取点</h3><p>在抓取时，熟悉对象上的抓取点在对象上具有相似的相对位置。</p><p>基于这个原理，使用3D SHOT形状描述符描述物体，能够精确的描述兴趣点相对于整个对象和表面的位置。具体学习抓取点的过程如下：</p><ol><li>收集从部分点云中选择的兴趣点的SHOT特征、LR特征、RGB特征</li><li>通过计算简单的统计数据，如范围、均值、标准差、熵等，降低沿点维度的特征维度</li><li>将特征输入到用于对象分类的极限学习机中。</li></ol><h3 id="2-2-2-构建抓取模型">2.2.2 构建抓取模型</h3><p>没看懂。</p><p>大概是建立大拇指和物体之间的坐标变换关系，然后将其转换为三指夹爪与物体之间的坐标变换。</p><h3 id="2-2-3-腕关节约束估计">2.2.3 腕关节约束估计</h3><p>主要是解决受外在单一视角下点云被遮挡，无法精确确定手腕方向的问题。</p><h1>3 R3M: A Universal Visual Representation for Robot Manipulation</h1><blockquote><p><strong>标题</strong>：R3M:机器人操纵的通用视觉表示<br><strong>作者团队</strong>：斯坦福大学，Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://tinyurl.com/robotr3m">https://tinyurl.com/robotr3m</a></p></blockquote><h2 id="3-1-目标问题">3.1 目标问题</h2><p>训练机器人根据图像完成操作任务。<strong>给定一段文字，例如“将铲子放到锅里”，机器人根据视觉执行相应的动作</strong>。</p><p><strong>（1）传统方法的局限性</strong></p><p>传统且广泛使用的方法是使用同构数据从头开始训练端到端的模型，但是由于训练数据难以获取，限制了这种方法的泛化。而我们还有没合适的机器人数据集，最近的数据集都是由少数不同环境有限任务组成，因此泛用性受到限制。</p><p><strong>（2）本文的突破思想</strong></p><p>参考<code>ImageNet</code>等通用有效的模型，机器人领域目前还没有类似的模型出现，但是思想可以借鉴，就是使用丰富的<code>in-the-wild data</code>（野生数据？），也就是使用人类与环境交互的视频，这些数据庞大且多样化，包含全球各种场景与任务。</p><p><strong>（3）本文方法简述</strong></p><p>训练了一种机器人操纵表示方法R3M。R3M能够学习具有挑战性的任务，例如将菜放入锅中，折叠毛巾等。</p><h2 id="3-2-方法">3.2 方法</h2><p>本文认为，机器人操作的良好表现由以下三个方面组成</p><ul><li>机器人应该捕获时间动态，因为机器人在环境中要按时间顺序完成任务</li><li>机器人应该捕获于一相关的特征</li><li>机器人应该是紧凑的</li></ul><p><strong>（1）时间对比学习</strong></p><p>训练编码器生成一个表示，是的时间上较近的图像之间的距离小于时间上较远的图像或来自不同视频的图像。</p><p><strong>（2）视频语言对齐</strong></p><p>捕获语言的特征，学习视频场景中的语义部分。</p><p><strong>（3）正则化</strong></p><p>降低状态空间的维度来保证克隆训练的策略符合专家状态分布。</p><h2 id="3-3-思考">3.3 思考</h2><p>与本人方向有差别，本文更偏向于语义，视觉只是作为一个感知手段。</p><h1>4 Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video</h1><blockquote><p><strong>标题</strong>：对抗性技能网络：来自视频的无监督机器人技能学习<br><strong>作者团队</strong>：德国弗赖堡大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="http://robotskills.cs.uni-freiburg.de/">http://robotskills.cs.uni-freiburg.de/</a></p></blockquote><h2 id="4-1-目标任务">4.1 目标任务</h2><p>从未标记的多视角视频中学习机器人操作任务。</p><p><strong>（1）传统方法的局限性</strong></p><p>现有的强化学习方法尽管有一些进展，但是这些方法都是学习每项任务的解决方案，并且依赖于手动的、面向任务设置的奖励函数，所获得的策略也是针对于特定任务的，无法转移到新任务上。</p><p><strong>（2）本文的创新点</strong></p><p>提出一种无监督的技能学习方法，称为对抗性技能网络ASN，通过观看视频来发现和学习可转移的技能。学习到的技能被用于RL，以便通过组合以前的技能来解决更广泛的任务。</p><p>该方法不需要帧和任务ID的对应关系，不需要任何额外的监督。</p><h2 id="4-2-方法">4.2 方法</h2><p><strong>Adversarial Skill Networks对抗性技能网络</strong></p><p>我们在对抗性框架中学习技能度量空间。网络的编码部分试图最大化熵以增强通用性。鉴别器在测试时不使用，它试图最小化其预测的熵，以提高对技能的识别。最后，最大化所有技能的边际类熵会导致所有任务类的统一使用。请注意，不需要关于框架和它们所源自的任务之间关系的信息。<br>（没看懂）</p><h2 id="4-3-思考">4.3 思考</h2><p>似乎可以从无标签的视频中学习任务。但是过于理论化。</p><h1>5 BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning</h1><blockquote><p><strong>标题</strong>：BC-Z:利用机器人模仿学习实现零样本任务泛化<br><strong>作者团队</strong>：谷歌、加州大学伯克利分校、斯坦福<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://sites.google.com/view/bc-z/home">https://sites.google.com/view/bc-z/home</a></p></blockquote><h2 id="5-1-目标问题">5.1 目标问题</h2><p>使基于视觉的机器人操作系统能推广到新任务。</p><p>为此，开发了一个交互式模仿学习系统，可以传达人物的不同信息作为条件，包括自然语言或者人类演示视频，该系统可以从演示中进行学习。并且发现学习到100个任务之后，可以执行24个未训练的任务且不需要演示。</p><p><strong>（1）现存问题</strong></p><p>机器人技术的一大挑战就是创造一种能够在非结构化环境中基于任意的用户命令执行大量任务。这一工作的关键挑战是泛化。机器人必须要能处理新的环境，识别和操纵以前从未见过的物体，并且理解从未被要求执行过的命令的意图。</p><p>传统的方法是在像素级进行端到端的学习，然后由足够的真实世界的数据，这些方法原则上能够使机器人在新的任务、对象、场景中进行泛化。但实际上这一目标还是遥不可及。</p><p>本文要解决的问题就是通过零样本或者少样本推广基于视觉的机器人操纵任务的问题。</p><p><img src="https://img.mahaofei.com/img/202308151409631.png" alt="image.png"></p><h2 id="5-2-数据收集">5.2 数据收集</h2><p>为100个预先指定的任务手机了人类演示的视频，这些视频包含了推物体、拿取放置物体等9项基本任务。</p><p>搭建一套远程操作系统，远程操作设备通过USB连接到机器人上，通过两个手持控制器遥控操作站在机器人后面，使用控制器以第三人称视角操作机器人，机器人实时响应跟随操作员演示各种任务。</p><h2 id="5-3-方法">5.3 方法</h2><h3 id="5-3-1-语言和视频编码">5.3.1 语言和视频编码</h3><p>编码器以语言命令或人类视频作为输入，并生成任务。</p><ul><li>如果是语言命令，使用预训练的多语言语句编码器为每个任务生成512维语言向量</li><li>如果是视频，使用基于ResNet18的卷积网络</li></ul><h3 id="5-3-2-训练策略">5.3.2 训练策略</h3><p>给定固定的任务，我们通过XYZ和轴角预测的Huber损失和抓取器角度的对数损失来训练。</p><p>开环辅助检测，如果以开环的方式运行，将采取是个行动的开环轨迹。开环预测提供了一个辅助训练目标，并可以离线检查闭环规划质量。</p><p>将状态差异作为操作，标准的模仿学习会将演示动作直接作为目标标签，而本文的专家克隆行为会导致一些小动作或抖动，因此考虑将动作定义为未来目标和下一步的差异，使用自适应算法确定手臂和夹爪的移动量。</p><h3 id="5-3-3-网络架构">5.3.3 网络架构</h3><p>使用ResNet18作为主干，从主干最后一个平均池化层分出多个head，每个head是一个多层感知机，对末端执行器动作的一部分进行建模，具体见原文。</p><h2 id="5-3-思考">5.3 思考</h2><p>首先提供演示视频和文字，然后手动控制机器人执行任务收集数据。似乎仍然较为繁琐。</p><h1>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training</h1><blockquote><p><strong>标题</strong>：VIP：通过价值内隐预训练实现普遍的视觉奖励和表现<br><strong>作者团队</strong>：Meta AI，宾夕法尼亚大学<br><strong>期刊会议</strong>：ICLR<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://sites.google.com/view/vip-rl">https://sites.google.com/view/vip-rl</a></p></blockquote><h2 id="6-1-目标问题">6.1 目标问题</h2><p>特定任务的机器人数据的成本较高且稀缺。从大型、多样化的离线人类视频中学习已经成为获得普遍有效的途径。然而如何将这些人类视频用于通用的奖励学习仍然是一个未解决的问题。</p><h2 id="6-2-方法">6.2 方法</h2><p><strong>（1）从人类视频中自我监督的价值学习</strong></p><p>人类视频包含目标导向的行为，利用人类视频进行表示学习的一个合理方法是在人类策略的空间上计算离线目标条件问题，然后提取到视觉表示。</p><p><strong>（2）隐含的时间对比学习</strong></p><p>当有意义的指示任务的开始和结束的两个帧在嵌入空间中接近时，初始帧和目标帧之间能够捕获长程语义时间依赖性。</p><p><strong>（3）基于隐含价值的预训练</strong></p><p>具体算法见原文。</p><h2 id="6-3-思考">6.3 思考</h2><p>类似R3M</p><h1>7 Graph-Structured Visual Imitation</h1><blockquote><p><strong>标题</strong>：图形结构的视觉模仿<br><strong>作者团队</strong>：索尼<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2019<br><strong>代码</strong>：无</p></blockquote><h2 id="7-1-目标问题">7.1 目标问题</h2><p>当机器人动作使工作空间中检测到的相应视觉实体的相对空间配置与演示更好的匹配时，会得到奖励。</p><p>本文使用人类手指关键点检测器、使用合成增强进行离线训练的对象检测器、由视点变化监督的点检测器。在没有人类注释数据或机器人交互的情况下为每次演示学习多个视觉实体检测器。</p><h2 id="7-2-方法">7.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308151609226.png" alt="image.png"></p><p><strong>（1）检测视觉实体</strong></p><p>人手关键点检测：使用现有的手部检测器，并使用D435i获取3D位置。将机器人平行钳口夹持器映射到演示者的拇指和食指指尖。通过在两个指尖设置距离阈值来检测抓取和释放动作。</p><p>点特征检测器：训练后，在模仿者和演示者的环境中匹配点特征，建立对应关系。</p><p>合成数据扩充：使用背景移除来提取出2D掩模，并使用合成数据增强来训练视觉检测器。</p><p><strong>（2）动态图构造的运动显著性</strong></p><p><strong>（3）基于可视化实体图的策略学习</strong></p><p>目标是当机器人从单个人类演示中模仿物体操纵任务。具体的成本代价函数参考原文。</p><h2 id="7-3-思考">7.3 思考</h2><p>提取手部关键点映射到机器人夹爪，同时使用物体关键点检测来实现运动策略的生成。思路上不如DemoGrasp更直观。</p><p>可行的方法</p><ol><li>提取演示视频中物体位姿</li><li>模仿执行动作<ol><li>将当前视角下物体位姿与演示视频中每一帧位姿计算误差损失</li><li>根据误差实时计算末端位姿调整姿态</li><li>将当前帧手臂关键点加入，获取机械臂各关节应到的位姿</li><li>执行机械臂动作（期间加入机械臂避障与轨迹平滑）</li></ol></li></ol><h1>8 Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</h1><blockquote><p><strong>标题</strong>：通过观看学习：人体视频中操纵技能的物理模拟<br><strong>作者团队</strong>：多伦多大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="http://www.pair.toronto.edu/lbw-kp/">http://www.pair.toronto.edu/lbw-kp/</a></p></blockquote><h2 id="8-1-目标问题">8.1 目标问题</h2><p>通过观看学习，通过模仿指定任务的单个视频来进行策略学习的算法框架。</p><ul><li>由于人类手臂与机器人手臂形态不同，我们的框架学习无监督的人-机器人的翻译来克服形态不匹配问题。</li><li>为了捕捉对学习状态至关重要的显著区域的细节，我们的模型采取了无监督关键点检测。检测到的关键点形成包含语义上有意义的信息的结构化表示，并可以直接用于计算奖励和策略学习。</li></ul><h2 id="8-2-方法">8.2 方法</h2><p>本文所提出的LbW框架由三个部分组成</p><ul><li>图像到图像的翻译网络：逐帧翻译输入的人类演示视频，生成机器人演示视频</li><li>关键点检测器：将生成的机器人演示视频作为输入，提取每帧的关键点，形成关键点轨迹</li><li>策略网络：将当前的基于关键点的观察表示传递给策略网络，用于预测与环境交互的动作</li></ul><p><img src="https://img.mahaofei.com/img/202308151841683.png" alt="image.png"></p><h2 id="8-3-思考">8.3 思考</h2><p>与其说是模仿学习网络，不如说是一个图像翻译网络，基于CycleGAN的图像翻译，将人手演示翻译成机器人动作视频，然后提取视频中机器人的关键点轨迹，通过策略函数实现实物机器人的动作。</p><h1>9 Learning Periodic Tasks from Human Demonstrations</h1><blockquote><p><strong>标题</strong>：从人类演示中学习周期性任务<br><strong>作者团队</strong>：卡内基梅隆大学<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="9-1-目标问题">9.1 目标问题</h2><p>使用主动学习来优化参数，提出了一个目标最大限度的提高机器人操纵物体的运动与演示视频中物体运动之间的相似性。重点在于可变形物体和颗粒物体。（用布擦拭表面，缠绕电缆，用勺子搅拌颗粒物质等）</p><h2 id="9-2-方法">9.2 方法</h2><p>本文提出的框架由两部分组成</p><ul><li>表示学习模块：关键点检测模型从独立收集的非特定任务的人类和机器人数据中提取一致的关键点</li><li>姿态优化模块：将产生在检测的关键点方面与人类演示相匹配的机器人视频</li></ul><h2 id="9-3-思考">9.3 思考</h2><p>给定人类演示动作和手动操控机器人演示动作，机器人学习两者的相似性，然后重复演示动作使其更接近人类演示效果。</p><h1>10 One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks</h1><blockquote><p><strong>标题</strong>：复合视觉运动任务的一次性层次模拟学习<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<a href="https://sites.google.com/view/one-shot-hil">https://sites.google.com/view/one-shot-hil</a></p></blockquote><h2 id="10-1-目标问题">10.1 目标问题</h2><p>真实机器人上从人类执行任务的视频中学习多阶段任务。</p><h2 id="10-2-方法">10.2 方法</h2><p>对于每个子任务，我们提供多个人类演示和多个机器人演示（需要对象和执行的任务对应，但是不用相同的对象位置、执行速度）</p><p><strong>（1）基元的合成</strong>：训练了一个人类相位预测器和机器人相位预测器，从人类执行视频中学习特定的机器人策略</p><p><strong>（2）原始相位预测</strong>：学习如何分割复合任务的人类演示；何时学习策略过度到下一个。</p><h2 id="10-3-思考">10.3 思考</h2><p>提供人的演示视频，机器人的演示视频，然后训练策略。最后利用训练的策略，提供一段人类演示视频，机器人执行对应的操作。</p><h1>11 Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller</h1><blockquote><p><strong>标题</strong>：基于解耦层次控制器的第三人称视觉模仿学习<br><strong>作者团队</strong>：MIT<br><strong>期刊会议</strong>：NeurIPS<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="https://pathak22.github.io/hierarchical-imitation/">https://pathak22.github.io/hierarchical-imitation/</a></p></blockquote><h2 id="11-1-目标问题">11.1 目标问题</h2><p>通过从第三人称视角观看人类演示视频，可以在未知场景中操纵新物体。</p><h2 id="11-2-方法">11.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308152040902.png" alt="image.png"></p><p><strong>（1）目标生成器</strong></p><p>从人类演示视频中推断像素空间中的目标，并以像素级的表示形式将其转化为机器人环境中的目标。</p><p>也是使用图像翻译的方法，将人类演示图像翻译为机器人演示图象。</p><p><strong>（2）反向控制器</strong></p><p>跟踪视觉目标推理模型中生成的线索，并生成机器人要执行的动作。</p><p>使用ResNet18模型。</p><p><strong>（3）第三人称模仿</strong></p><p>以交替方式运行目标生成器和反向控制器。目标生成器生成子目标，低级控制器生成机器人关节角度，直到人类演示结束。</p><h2 id="11-3-思考">11.3 思考</h2><p>还是使用图像翻译的思路，把人手操作图像翻译成机械臂操作图像，再由控制器生成机器人关节角度。</p><h1>12 You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration</h1><blockquote><p><strong>标题</strong>：Yodo：单一视觉演示的类别级操作<br><strong>作者团队</strong>：罗格斯大学<br><strong>期刊会议</strong>：RSS<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="12-1-目标问题">12.1 目标问题</h2><p>由于最近的跨对象类别级操作虽然有很好的结果，但通常需要昂贵的真实数据收集和为每个对象类别和任务手动指定语义关键点。并且粗略的关键点预测和忽略中间动作序列阻止了在抓取和防止之外的复杂任务的应用。</p><p>本工作提出了一种新的操作框架。该框架利用了无模型6D跟踪技术，解析单个演示视频中的类别级任务轨迹，整个执行过程被分解为远程、无碰撞运动和最后一英寸操作三个步骤。</p><h2 id="12-2-方法">12.2 方法</h2><p>对于每个演示视频帧，通过无模型6D位姿估计跟踪目标位姿，对象位姿在容器的坐标系中表示，这样允许泛化到新的场景。</p><p><strong>（1）类别级表示的离线学习</strong></p><p>建立了一个9D物体表示方法，6D位姿+3D缩放</p><p><strong>（2）无模型的物体6D跟踪</strong></p><p>物体运动跟踪要实现两个目的</p><ul><li>演示阶段，解析录制的视频，提取容器坐标系中被操纵的对象的6D运动轨迹</li><li>在线执行期间，为闭环控制器提供视觉反馈</li></ul><p><strong>（3）类别级行为克隆作为最后一步策略</strong></p><p>产生密集的离散轨迹，以便机器人能沿轨迹到达下一个目标</p><p><strong>（4）基于局部注意的动态类别级框架</strong></p><p>自动动态地规范坐标系原点。</p><p><strong>（5）抓取物体并使其沿关键点移动</strong></p><p>常规的抓取方法</p><h2 id="12-3-思考">12.3 思考</h2><p>将目标位姿表示为相对于另一个物体的相对位姿，这样有助于场景的泛化。</p><p>整体思想就是使用6D位姿估计获得目标的运动轨迹，然后重复这条轨迹。</p>]]></content>
    
    
    <summary type="html">从人类抓取物体的视频中学习机器人抓取的相关工作调研。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
  </entry>
  
  <entry>
    <title>ROS系统Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/4add66b0.html"/>
    <id>https://www.mahaofei.com/post/4add66b0.html</id>
    <published>2023-05-15T09:05:05.000Z</published>
    <updated>2023-05-15T09:05:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、安装问题</h1><h2 id="ROS安装时rosdep-init与rosdep-update问题解决方法">ROS安装时rosdep_init与rosdep_update问题解决方法</h2><p><strong>解决方法</strong></p><p>使用下面的命令替代上面两行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3-pip</span><br><span class="line">sudo pip3 install rosdepc</span><br><span class="line">sudo rosdepc init</span><br><span class="line">rosdepc update</span><br></pre></td></tr></table></figure><h1>二、环境问题</h1><h2 id="Unable-to-find-either-executable-‘empy’-or-Python-module-‘em’…-try-installing-the-package-‘python3-empy’">Unable to find either executable ‘empy’ or Python module ‘em’…  try  installing the package ‘python3-empy’</h2><p><strong>（1）问题原因</strong></p><p>Anaconda使用的是Python3版本，但是ROS使用的Python2</p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="Could-not-find-a-package-configuration-file-provided-by-“某某包”-with-any-of-the-following-names">Could not find a package configuration file provided by “某某包” with any of  the following names</h2><p><strong>（1）问题原因</strong></p><p>缺少<code>某某包</code></p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ros-noetic-某某包</span><br></pre></td></tr></table></figure><h1>三、配置问题</h1><h2 id="ERROR-cannot-launch-node-of-type-robot-state-publisher-state-publisher-Cannot-locate-node-of-type-state-publisher-in-package-robot-state-publisher-Make-sure-file-exists-in-package-path-and-permission-is-set-to-executable-chmod-x）">ERROR: cannot launch node of type [robot_state_publisher/state_publisher]: Cannot locate node of type [state_publisher] in package [robot_state_publisher]. Make sure file exists in package path and permission is set to executable (chmod +x）</h2><p><strong>（1）问题原因</strong></p><p>使用launch文件启动某个节点时出现这个问题，是因为launch文件中name、pkg、type不统一导致的。</p><p><strong>（2）解决方法</strong></p><p>检查launch文件，确保name、pkg、type一样，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot; /&gt;</span><br></pre></td></tr></table></figure><h2 id="joint-state-publisher-gui没有显示">joint state publisher gui没有显示</h2><p><strong>（1）问题描述</strong></p><p>使用ROS进行仿真，想用joint state publisher进行机械臂控制，但是启动launch文件后没有报错信息，但也没有joint state publisher gui。</p><p><strong>（2）解决方法</strong></p><p>2020年开始，gui已经移出了 joint state publisher, 并且成为了一个新的package：joint state publisher gui. 之前那种使用gui参数的方式调用joint state publisher 是仍然可行的，但是不会调用gui。</p><p>在launch文件中，将joint state publisher 替换成joint__state__publisher_gui。</p>]]></content>
    
    
    <summary type="html">在使用ROS系统进行机器人实验中，遇到的各种错误信息汇总，不定时更新。</summary>
    
    
    
    <category term="机器人" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="ros" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/ros/"/>
    
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
  </entry>
  
  <entry>
    <title>ROS Gazebo 6D机械臂抓取仿真实验</title>
    <link href="https://www.mahaofei.com/post/7fec171b.html"/>
    <id>https://www.mahaofei.com/post/7fec171b.html</id>
    <published>2023-05-15T07:23:56.000Z</published>
    <updated>2023-05-15T07:23:56.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、基础知识</h1><h2 id="1-1-URDF">1.1 URDF</h2><p>URDF是ROS中机器人模型的描述格式，包括机器人的外观、物理属性、关节类型等方面。</p><ul><li><code>&lt;robot&gt;</code>：最顶层标签</li><li><code>&lt;link&gt;</code>：描述刚提的外观形状、碰撞几何、颜色、惯性矩阵等</li><li><code>&lt;joint&gt;</code>：描述两个link之间的关系，有6种类型，最常用的是<code>revolute</code>类型，有关节位置限制的旋转关节</li></ul><p>xacro模型可以将部分URDF打包成一个&quot;类&quot;，在其他模型中调用。</p><p>功能包中一般包括以下四个部分</p><ol><li><code>cfg</code>：配置文件</li><li><code>launch</code>：加载urdf模型，并在rviz中展示</li><li><code>meshes</code>：urdf用到的外观模型</li><li><code>urdf</code>：urdf模型定义</li></ol><h2 id="1-2-Gazebo">1.2 Gazebo</h2><h2 id="1-3-Moveit控制">1.3 Moveit控制</h2><p><strong>（1）Moveit!大致功能</strong></p><ul><li>运动学计算</li><li>运动规划</li><li>碰撞检测</li></ul><p>最重要的节点是<code>move_group</code>，输入可以是RVIZ中的数据或点云和深度图。路径规划一般使用的OMPL库，碰撞检测使用FCL库。最后发送个机械臂让机械臂执行轨迹。</p><h2 id="1-4-机械臂运动规划">1.4 机械臂运动规划</h2><h2 id="1-5-基于深度学习的视觉避障">1.5 基于深度学习的视觉避障</h2><h1>二、实验环境搭建</h1><h2 id="1-1-安装ROS">1.1 安装ROS</h2><p>参考<a href="https://www.mahaofei.com/post/b278544f.html">Ubuntu20.04安装ROS Noetic</a>文章</p><p>![[01-Ubuntu20.04安装ROS Noetic#二、安装ROS]]</p><h2 id="1-2-安装Moveit">1.2 安装Moveit!</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ros-noetic-moveit</span><br></pre></td></tr></table></figure><h2 id="1-3-安装UR机器人及驱动">1.3 安装UR机器人及驱动</h2><p>复制代码后，修改下面的内容，使其能在noetic版本的ros上运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/srv/SetPayload.srv</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">float32 payload</span><br><span class="line">geometry_msgs/Vector3 center_of_gravity</span><br><span class="line">-----------------------</span><br><span class="line">bool success</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/CMakeLists.txt</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8.3)</span><br><span class="line">project(ur_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Find catkin macros and libraries</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># is used, also find other catkin packages</span></span></span><br><span class="line">find_package(catkin REQUIRED COMPONENTS message_generation std_msgs geometry_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate messages in the &#x27;msg&#x27; folder</span></span></span><br><span class="line">add_message_files(</span><br><span class="line">   FILES</span><br><span class="line">   Analog.msg</span><br><span class="line">   Digital.msg</span><br><span class="line">   IOStates.msg</span><br><span class="line">   RobotStateRTMsg.msg</span><br><span class="line">   MasterboardDataMsg.msg</span><br><span class="line">   RobotModeDataMsg.msg</span><br><span class="line">   ToolDataMsg.msg</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate services in the &#x27;srv&#x27; folder</span></span></span><br><span class="line">add_service_files(</span><br><span class="line">   FILES</span><br><span class="line">   SetPayload.srv</span><br><span class="line">   SetSpeedSliderFraction.srv</span><br><span class="line">   SetIO.srv</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate added messages and services with any dependencies listed here</span></span></span><br><span class="line">generate_messages(</span><br><span class="line">   DEPENDENCIES</span><br><span class="line">   std_msgs</span><br><span class="line">   geometry_msgs</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># catkin specific configuration ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line">catkin_package(</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> INCLUDE_DIRS include</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> LIBRARIES ur_msgs</span></span><br><span class="line">   CATKIN_DEPENDS message_runtime std_msgs geometry_msgs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> DEPENDS system_lib</span></span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Build ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Install ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Testing ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/package.xml</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;package format=&quot;2&quot;&gt;</span><br><span class="line">  &lt;name&gt;ur_msgs&lt;/name&gt;</span><br><span class="line">  &lt;version&gt;1.2.5&lt;/version&gt;</span><br><span class="line">  &lt;description&gt;The ur_msgs package&lt;/description&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;author&gt;Andrew Glusiec&lt;/author&gt;</span><br><span class="line">  &lt;author&gt;Felix Messmer&lt;/author&gt;</span><br><span class="line">  &lt;maintainer email=&quot;g.a.vanderhoorn@tudelft.nl&quot;&gt;G.A. vd. Hoorn&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;miguel.prada@tecnalia.com&quot;&gt;Miguel Prada Sarasola&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;nhg@ipa.fhg.de&quot;&gt;Nadia Hammoudeh Garcia&lt;/maintainer&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;license&gt;BSD&lt;/license&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt;</span><br><span class="line">  &lt;build_depend&gt;message_generation&lt;/build_depend&gt;</span><br><span class="line">  &lt;depend&gt;std_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;depend&gt;geometry_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;</span><br><span class="line"></span><br><span class="line">  &lt;export&gt;</span><br><span class="line">  &lt;/export&gt;</span><br><span class="line">&lt;/package&gt;</span><br></pre></td></tr></table></figure><p>完成之后，就可以编译了。<code>source</code>之后使用<code>roslaunch ur5_moveit_config demo.launch</code></p><h2 id="1-4-简单测试">1.4 简单测试</h2><p><strong>（1）Rviz打开UR5模型</strong></p><p>机械臂夹爪模型路径为<code>universal_robot/urdf/ur5_gripper_joint_limited_robot.urdf.xacro</code>。</p><p>可以通过<code>universal_robot/ur_description/launch/view_ur5_with_gripper.launch</code>启动，从Rviz中查看模型情况，并使用<code>joint_state_publisher_gui</code>对机械臂模型拖动控制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5_with_gripper.launch</span><br></pre></td></tr></table></figure><p><strong>（2）Rviz中Moveit测试</strong></p><p>使用下面的程序可以在 Rviz 中进行 Moveit 轨迹规划测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur5_moveit_config demo.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur5_gripper_moveit_config demo.launch</span><br></pre></td></tr></table></figure><p><strong>（3）Gazebo中Moveit测试</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5.launch</span><br><span class="line">roslaunch ur5_moveit_config ur5_moveit_planning_execution.launch sim:=true</span><br><span class="line">roslaunch ur5_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><code>ur5.launch</code>：用于启动 gazebo 仿真环境。具体包括以下几个部分，启动空环境、定义 robot_description 参数服务器、发送到gazebo中生成机器人、启动并加载控制器。</p><p><code>ur5_moveit_planning_execution.launch</code>：用于启动 MoveIt 相关组件。具体包括以下几个部分：设置 sim参数， 根据 sim 参数重映射 follow_joint_trajectory 话题，启动MoveIt。</p><p><code>moveit_rviz.launch</code>：用于启动 Rviz 相关组件。具体包括以下几个部分：加载配置参数，启动Rviz。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5_with_gripper.launch</span><br><span class="line">roslaunch ur5_single_arm_moveit_config ur5_moveit_planning_execution.launch</span><br><span class="line">roslaunch ur5_gripper_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202308041008966.png" alt=""></p><h2 id="1-5-导入自定义物体">1.5 导入自定义物体</h2><p><strong>（1）网络方法</strong></p><p>使用 MeshLab 加载自己的物体模型。</p><p>点击【Filters -&gt; Normals … -&gt; Compute normals for points sets】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Remeshing -&gt; Surface Reconstruction: Screened Poisson】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Trivial Per-Triangle】，按照如下设置，重要的是Method。</p><p><img src="https://img.mahaofei.com/img/202305242228283.png" alt="image.png"></p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】，按照如下设置，重要的是Source Mesh和Target Mesh。</p><p><img src="https://img.mahaofei.com/img/202305242231021.png" alt="image.png"></p><p><strong>（2）摸索方法</strong></p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Flat Plane】</p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】</p>]]></content>
    
    
    <summary type="html">使用Gazebo搭建6D机械臂仿真环境，并添加相机，然后实现抓取仿真实验。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E5%AE%9E%E9%AA%8C/"/>
    
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>【目标检测算法】YOLOV8代码复现</title>
    <link href="https://www.mahaofei.com/post/16b5f6b3.html"/>
    <id>https://www.mahaofei.com/post/16b5f6b3.html</id>
    <published>2023-05-06T06:13:12.000Z</published>
    <updated>2023-05-06T06:13:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、算法笔记</h1><h1>二、代码复现</h1><h2 id="2-1-搭建环境">2.1 搭建环境</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolov8 python=3.7</span><br><span class="line">conda activate yolov8</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>下载作者开源的程序，并安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/ultralytics</span><br><span class="line">cd ultralytics</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-命令行使用教程">2.2 命令行使用教程</h2><p><strong>（1）语法规则</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yolo TASK MODE ARGS</span><br><span class="line"></span><br><span class="line">Where   TASK (optional) is one of [detect, segment, classify]</span><br><span class="line">        MODE (required) is one of [train, val, predict, export, track]</span><br><span class="line">        ARGS (optional) are any number of custom &#x27;arg=value&#x27; pairs like &#x27;imgsz=320&#x27; that override defaults.</span><br></pre></td></tr></table></figure><p><strong>（2）训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01</span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo predict model=yolov8n-seg.pt source=&#x27;https://youtu.be/Zgi9g1ksQHc&#x27; imgsz=320</span><br></pre></td></tr></table></figure><p><strong>（4）评价</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640</span><br></pre></td></tr></table></figure><h2 id="2-3-Python使用教程">2.3 Python使用教程</h2><p><strong>（1）训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n.pt&#x27;</span>) <span class="comment"># 从预训练模型开始</span></span><br><span class="line">model.train(epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><strong>（2）评价</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line">model.val()  <span class="comment"># 使用model.pt的data yaml进行评价</span></span><br><span class="line">model.val(data=<span class="string">&#x27;coco128.yaml&#x27;</span>)  <span class="comment"># 或指定数据进行评价</span></span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><p>获取预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line"><span class="comment"># 接受所有类型 - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam</span></span><br><span class="line"><span class="comment"># 从摄像头</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;0&quot;</span>)</span><br><span class="line"><span class="comment"># 从文件夹</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>, show=<span class="literal">True</span>) <span class="comment"># Display preds. Accepts all YOLO predict arguments</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL图像</span></span><br><span class="line">im1 = Image.<span class="built_in">open</span>(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im1, save=<span class="literal">True</span>)  <span class="comment"># save plotted images</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从ndarray</span></span><br><span class="line">im2 = cv2.imread(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im2, save=<span class="literal">True</span>, save_txt=<span class="literal">True</span>)  <span class="comment"># save predictions as labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL/ndarray的列表</span></span><br><span class="line">results = model.predict(source=[im1, im2])</span><br></pre></td></tr></table></figure><p>预测结果分析（results会包含预测所有结果的列表，当有很多图像的时候要注意避免内存溢出，特别是在实例分割时）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. return as a list</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  return as a generator (stream=True)</span></span><br><span class="line">results = model.predict(source=<span class="number">0</span>, stream=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># Detection</span></span><br><span class="line">    result.boxes.xyxy   <span class="comment"># box with xyxy format, (N, 4)</span></span><br><span class="line">    result.boxes.xywh   <span class="comment"># box with xywh format, (N, 4)</span></span><br><span class="line">    result.boxes.xyxyn  <span class="comment"># box with xyxy format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.xywhn  <span class="comment"># box with xywh format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.conf   <span class="comment"># confidence score, (N, 1)</span></span><br><span class="line">    result.boxes.cls    <span class="comment"># cls, (N, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Segmentation</span></span><br><span class="line">    result.masks.data      <span class="comment"># masks, (N, H, W)</span></span><br><span class="line">    result.masks.xy        <span class="comment"># x,y segments (pixels), List[segment] * N</span></span><br><span class="line">    result.masks.xyn       <span class="comment"># x,y segments (normalized), List[segment] * N</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Classification</span></span><br><span class="line">    result.probs     <span class="comment"># cls prob, (num_class, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Each result is composed of torch.Tensor by default, </span></span><br><span class="line"><span class="comment"># in which you can easily use following functionality:</span></span><br><span class="line">result = result.cuda()</span><br><span class="line">result = result.cpu()</span><br><span class="line">result = result.to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">result = result.numpy()</span><br></pre></td></tr></table></figure><h2 id="2-3-数据集制作（实例分割）">2.3 数据集制作（实例分割）</h2><p><strong>（1）使用Labelme创建实例分割数据集</strong></p><p>安装labelme</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme</span><br></pre></td></tr></table></figure><p>安装完成后直接在命令行输入<code>labelme</code>即可打开。</p><p>使用label进行标注，将生成的json文件和原始图像jpg，放入同一个文件夹中。</p><p><strong>（2）Labelme格式转COCO格式</strong></p><p>参考<a href="https://pypi.org/project/labelme2coco/">pypi的labelme2coco包</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme2coco</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者使用清华源</span></span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple labelme2coco</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labelme2coco path/to/labelme/dir --train_split_rate 0.85</span><br></pre></td></tr></table></figure><p><strong>（3）COCO格式转YOLO格式</strong></p><p>使用<code>labelme</code>制作实例分割的coco格式数据集，然后使用<a href="https://github.com/ultralytics/JSON2YOLO">ultralytics/JSON2YOLO</a>项目将json文件转换成yolo的训练格式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/JSON2YOLO.git</span><br><span class="line">cd JSON2YOLO</span><br></pre></td></tr></table></figure><p>创建一个虚拟环境，然后使用下面的命令安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第387行，设置为刚才得到的COCO注释的位置，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> source == <span class="string">&#x27;COCO&#x27;</span>:</span><br><span class="line">convert_coco_json(<span class="string">&#x27;datasets/20230223_Phone_4Obj_Coco/annotations&#x27;</span>,  <span class="comment"># directory with *.json</span></span><br><span class="line">  use_segments=<span class="literal">True</span>,</span><br><span class="line">  cls91to80=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第289行，因为不是coco80中的物体类型，是自己设置的，因此需要修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cls = coco80[ann[&#x27;category_id&#x27;] - 1] if cls91to80 else ann[&#x27;category_id&#x27;] - 1  # class</span></span><br><span class="line">cls = ann[<span class="string">&#x27;category_id&#x27;</span>]  <span class="comment"># class</span></span><br></pre></td></tr></table></figure><p>运行程序将COCO格式json文件转换为YOLO格式txt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python general_json2yolo.py</span><br></pre></td></tr></table></figure><p>结果保存在new_dir中，需要手动把images复制过去。最后得到的数据集如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_root</span><br><span class="line">├── images</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">└── labels</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br></pre></td></tr></table></figure><p><strong>（4）创建数据集的YAML文件</strong></p><p>打开目录<code>ultralytics/datasets</code>，复制一份其中的<code>coco128-seg.yaml</code>，重命名为<code>custom-seg.yaml</code>，然后根据自己的数据集进行修改。</p><p>例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span></span><br><span class="line"><span class="attr">path:</span> <span class="string">/media/mahaofei/OneTouch/Dataset/Program_data/image_processing/ultralytics/20230223_Phone_4Obj_YOLO</span>  <span class="comment"># dataset root dir</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">images/train2017</span>  <span class="comment"># train images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">images/train2017</span>  <span class="comment"># val images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">test:</span>  <span class="comment"># test images (optional)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classes</span></span><br><span class="line"><span class="attr">names:</span></span><br><span class="line">  <span class="attr">0:</span> <span class="string">ammeter</span></span><br><span class="line">  <span class="attr">1:</span> <span class="string">coffeebox</span></span><br><span class="line">  <span class="attr">2:</span> <span class="string">realsensebox</span></span><br><span class="line">  <span class="attr">3:</span> <span class="string">sucker</span></span><br></pre></td></tr></table></figure><h2 id="2-4-开始训练">2.4 开始训练</h2><p>新建一个python文件如<code>train.py</code>，添加内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a model</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;)  # build a new model from YAML</span></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n-seg.pt&#x27;</span>)  <span class="comment"># load a pretrained model (recommended for training)</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;).load(&#x27;yolov8n.pt&#x27;)  # build from YAML and transfer weights</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.train(data=<span class="string">&#x27;custom-seg.yaml&#x27;</span>, epochs=<span class="number">100</span>, imgsz=<span class="number">3904</span>, batch=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="2-5-结果预测">2.5 结果预测</h2>]]></content>
    
    
    <summary type="html">使用经典的YOLO算法进行实例分割</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【抓取姿态估计算法】RGB Matters论文笔记与复现</title>
    <link href="https://www.mahaofei.com/post/a1b0a01b.html"/>
    <id>https://www.mahaofei.com/post/a1b0a01b.html</id>
    <published>2023-05-04T00:30:31.000Z</published>
    <updated>2023-05-04T00:30:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><p><strong>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</strong></p><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="1-1-目标问题-4">1.1 目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="1-2-方法-3">1.2 方法</h2><p>大致流程为：</p><ol><li>使用Angle View Net网络生成图像不同位置的抓取器方向 $P_{img}=(u,v,r_x,r_y,r_z,c)$，即图像中坐标的位置，其对应的夹爪旋转姿态，以及置信度。</li><li>对置信度高的预测，结合深度图计算距离和夹爪宽度$P_{cam}=x,y,z,rx,ry,rz,w$</li></ol><p><strong>（0）定义</strong></p><p>抓握姿势定义为 (x, y, z, rx, ry, rz, w)，其中(x, y, z)代表夹持器的位置，(rx, ry, rz)代表夹持器的旋转，w代表夹持器的宽度。</p><p>夹持器本文仅考虑平行夹爪，使用 (h, l, wmax) 定义，三个参数分别代表夹具的 高度、长度和最大宽度。</p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒（因为同一个位置进行抓取有不止一个可行的旋转）。</p><p>可以使用下面的模型，将方向解耦为接近方向和绕平面的旋转，将夹持器旋转预测作为一个分类问题进行预测，共有VxA类方向。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>网络通过将RGB图像栅格化，对于每一个网格，AVN预测一个1维VxA个元素的向量，包含每个方向的置信度。最终得到(VxA)xGHxGW的tensor。AVN最终的输出表示为每个角度的heatmap。</p><p>作者在代码中给出的是V=60,A=6的测试。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="1-3-思考-3">1.3 思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1>二、复现过程</h1><h2 id="2-1-环境搭建-2">2.1 环境搭建</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n rgb_matters python=3.7</span><br><span class="line">conda activate rgb_matters</span><br></pre></td></tr></table></figure><p>下载程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/GouMinghao/rgb_matters</span><br><span class="line">cd rgb_matters</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-测试Demo">2.2 测试Demo</h2><p>下载作者训练好的模型：<a href="https://drive.google.com/drive/folders/1upW4gvQk5ftXfpLHtvCogudpP4kNyoGq?usp=sharing">Google Drive</a></p><p>在代码目录创建一个<code>weights</code>的目录，然后将下载的模型放入其中，完成后文件夹结构如下</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rgbd_graspnet/</span><br><span class="line">├── check_label_integrity.py</span><br><span class="line">├── train.py</span><br><span class="line">├── train.sh</span><br><span class="line">├── vis_label.py</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">└── weights</span><br><span class="line">    ├── kn_jitter_79200.pth</span><br><span class="line">    ├── kn_no_norm_76800.pth</span><br><span class="line">    ├── kn_norm_63200.pth</span><br><span class="line">    ├── kn_norm_only_73600.pth</span><br><span class="line">    └── rs_norm_56400.pth</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230424162408.png" alt=""></p><h1>三、代码分析</h1><h2 id="3-1-输出结果分析">3.1 输出结果分析</h2><p><strong>（1）热力图获取</strong></p><p>使用下面的代码进行预测热力图</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = RGBNormalNet(num_layers=args.num_layers, use_normal=args.use_normal, normal_only=args.normal_only)</span><br><span class="line">state_dict = torch.load(weights_path)</span><br><span class="line">net.load_state_dict(state_dict[&quot;net&quot;], strict=False)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net.eval()</span><br><span class="line"></span><br><span class="line">rgb, _ = load_data(rgb_path, depth_path)</span><br><span class="line"></span><br><span class="line">rgb = rgb.unsqueeze(0).to(device)</span><br><span class="line"></span><br><span class="line">prob_map = net(rgb)</span><br></pre></td></tr></table></figure><p>其中的<code>prob_mat</code>是预测的热力图<code>shape=(batch_size, 360, h, w)</code>一共360张热力图（360张包括接近方向v=60和平面内旋转A=6）</p><p><strong>（2）夹爪姿态获取</strong></p><p>使用<code>convert_grasp()</code>函数从360张热力图中提取夹爪姿态。</p><p>夹爪姿态为<code>Grasp</code>实例，包括以下几个参数：</p><ul><li><code>score</code>：float类型，抓取得分</li><li><code>width</code>：float类型，夹爪宽度</li><li><code>height</code>：float类型，夹爪高度</li><li><code>depth</code>：float类型，夹爪深度</li><li><code>rotation_matrix</code>：shape(3, 3)数组，旋转矩阵</li><li><code>translation</code>：shape(3)数组，平移向量</li><li><code>object_id</code>：int类型，抓取物体类别</li></ul><p>具体参考下面两张图：</p><p><img src="https://img.mahaofei.com/img/202305142107952.png" alt="image.png"></p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p>]]></content>
    
    
    <summary type="html">复现上交提出的RGB Matters算法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>抓取姿态估计算法调研</title>
    <link href="https://www.mahaofei.com/post/791dd0f5.html"/>
    <id>https://www.mahaofei.com/post/791dd0f5.html</id>
    <published>2023-04-23T07:16:08.000Z</published>
    <updated>2023-04-23T07:16:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Hybrid Physical Metric For 6-DoF Grasp Pose Detection</h1><blockquote><p><strong>标题</strong>：用于6D抓取检测的混合物理度量<br><strong>作者团队</strong>：清华大学（王生进）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/luyh20/FGC-GraspNet">https://github.com/luyh20/FGC-GraspNet</a></p></blockquote><h2 id="一、目标问题">一、目标问题</h2><p>单个物理指标会导致离散的抓取置信度分数，在百万抓取数据训练时会导致预测结果不准确。</p><p>本文定义了一种新的度量方式，基于力封闭度量、物体平面度、重力和碰撞测量。</p><p>本文设计了平面重力碰撞FGC-GraspNet，适用于多任务多分辨率学习体系。</p><h2 id="二、混合物理度量">二、混合物理度量</h2><p><img src="https://img.mahaofei.com/img/202304231518654.png" alt="image.png"></p><p><strong>（1）平面度</strong></p><p>平面度越高的抓的越稳。利用点的局部法向量的相似性计算平坦度得分。</p><p><strong>（2）重心度量</strong></p><p>夹持力更接近物体重心的更稳定。计算物体重心到两个接触点的连线的距离作为重力得分。</p><p><strong>（3）碰撞扰动度量</strong></p><p>当夹爪接近物体时容易发生碰撞，因此取夹爪两个最大行程端点与物体接触点的欧氏距离最小值作为碰撞扰动得分。</p><p><strong>（4）混合物理度量</strong></p><p>混合物理度量是上面度量的加权组合。</p><h2 id="三、-FGC-GraspNet">三、 FGC-GraspNet</h2><p>通过最远点采样FPS得到20000x3对点云输入，网络由PointNet++，FA分支、RD分支组成。</p><p><img src="https://img.mahaofei.com/img/202304231530817.png" alt=""></p><ul><li>PointNEt++用于提取点特征</li><li>低分辨率的特征进入FA分支进行前景分割和逐点逼近方向得分回归</li><li>高分辨率的特征用于RD旋转分支。</li></ul><h2 id="四、思考">四、思考</h2><p>将混合物理度量纳入LOSS的计算过程确实有意义。而且具有一定的复用性，其它算法也可借鉴此设计。</p><h1>Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter</h1><blockquote><p><strong>标题</strong>：基于体素的抓取网络：杂乱场景中实时6D抓取检测<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：CoRL2020<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/vgn">https://github.com/ethz-asl/vgn</a></p></blockquote><h2 id="一、目标问题-2">一、目标问题</h2><p>本文提出了一种网络从深度相机中获得场景信息，预测6D抓取的网络。</p><h2 id="二、论文方法">二、论文方法</h2><p><strong>（1）网络架构</strong></p><p>由滤波器、卷积层组成的感知模块将输入体素映射为特征图，然后进行卷积、上采样操作，最后是三个独立的分支用于预测抓取质量、旋转和夹爪宽度。</p><p><strong>（2）抓取检测</strong></p><p>使用一些方法去除不可能的抓取姿势，然后应用非极大抑制来获得候选的抓取列表。</p><h2 id="三、思考">三、思考</h2><p>非常基础的方法，已经有人在此基础上进行了扩展并发表了顶会。</p><h1>Efficient Learning of Goal-Oriented Push-Grasping Synergy in Clutter</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：浙江大学（熊蓉）<br><strong>期刊会议</strong>：RAL<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy">https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy</a></p></blockquote><h2 id="一、目标问题-3">一、目标问题</h2><p>在混乱场景中抓取物体时，有时需要一些预抓取动作，例如推动。使机械臂能够分离目标对象并稳定的实现抓取。</p><h2 id="二、方法">二、方法</h2><p><img src="https://img.mahaofei.com/img/202304231536100.png" alt=""></p><p>环境准备：固定的RGBD相机拍摄工作空间，将RGBD投影到重力方向，使用颜色高度图和深度高度图表示每个状态。</p><p><strong>（1）有目标的抓取训练</strong></p><p>训练一个有目标条件下的抓取网络，当有足够的训练之后，成功抓取的Q值稳定。</p><p><strong>（2）有目标的推动训练</strong></p><p>训练一个有目标条件下的推动网络，推动的奖励函数是基于抓取网络反向训练设计的。</p><p><strong>（3）交替训练</strong></p><p>利用交替训练来解决物体分布不匹配的问题，进一步提高混乱环境中抓取策略性能。</p><h2 id="三、思考-2">三、思考</h2><p>推物体再抓取物体相当于一个两阶段方法，可以不用在底层进行训练，而是在高层的规划决策层来进行判断，发布任务是推物体还是抓物体。</p><h1>TransGrasp: Grasp Pose Estimation ofaCategory ofObjects byTransferring Grasps fromOnlyOne Labeled Instance</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：大连理工大学（孙怡）<br><strong>期刊会议</strong>：ECCV<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/yanjh97/TransGrasp">https://github.com/yanjh97/TransGrasp</a></p></blockquote><h2 id="一、目标问题-4">一、目标问题</h2><p>现有大多数方法需要大量的抓取数据来训练，为了解决这个问题，本文实现只标记一个对象预测一类对象的抓取姿态。</p><h2 id="二、方法-2">二、方法</h2><p><strong>（1）学习类别的对应关系</strong></p><ol><li>Shape Encoder和DIFDecoder组成神经网络，训练得到对象变形到模板的密集对应关系</li></ol><p><strong>（2）抓取姿态估计</strong></p><ol><li>点云首先从相机坐标系转换到对象坐标系</li><li>生成对象实例的变形到模板</li><li>将带有抓取注释的模型输入到DeformNet中获得模型的变形</li><li>由两者的共同模板见你对应关系，通过对准物体表面上的抓握点来引导抓握姿势的变换</li><li>通过refine模块进行优化</li><li>将优化后的抓握知识转换为相机坐标进行抓取</li></ol><h2 id="三、思考-3">三、思考</h2><p>这种算法只能实现与模板形状相似的物体进行抓取，而且每个类别要先手工标记1000个抓握姿势。</p><h1>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</h1><blockquote><p><strong>标题</strong>：ContactGraspNet：在杂乱场景中高效生成6-DoF抓取<br><strong>作者团队</strong>：NVIDIA<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a></p></blockquote><h2 id="1-目标问题-22">1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="2-方法-22">2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/202304231545643.png" alt=""></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/202304231546650.png" alt=""></p><h2 id="3-思考-22">3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</h1><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="一、目标问题-5">一、目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="二、方法-3">二、方法</h2><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒。可以使用下面的模型，将夹持器旋转预测作为一个分类问题进行预测。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>AVN最终的输出表示为角视图热图。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="三、思考-4">三、思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1>CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation</h1><blockquote><p><strong>标题</strong>：CaTGrasp：从仿真中学习杂乱场景的类别级抓取<br><strong>作者团队</strong>：Rutgers University（美国罗格斯大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/wenbowen123/catgrasp">https://github.com/wenbowen123/catgrasp</a></p></blockquote><h2 id="一、目标问题-6">一、目标问题</h2><p>提出了一个框架学习工业对象的抓取，不需要真实的数据或手动注释</p><h2 id="二、方法-4">二、方法</h2><p>给定同一类别的3D模型的数据库，该方法学习</p><ul><li>以对象为中心的NUNOCS表示</li><li>hotmap：抓握过程中手-对象接触区域的任务实现成功的可能性</li><li>抓取姿势的编码本</li></ul><p><strong>（1）类别级标准NUNOCS表示</strong></p><p>将同一个类别的不同实例对象转换到标准空间，并缩放为标准大小。</p><p><strong>（2）稳定抓取学习</strong></p><p>首先给定从当前实例到规范模型的9D变换，将相同的变换应用于抓取来得到抓取建议。</p><p>将生成的抓取用于训练基于PointNet构建的网络，预测抓取质量。</p><p><strong>（3）实例分割</strong></p><p>使用了3D U-Net，将整个场景的点云作为输入，预测每点偏移到物体中心，将偏移点聚类为实例段。</p><p><strong>（4）仿真中生成训练数据</strong></p><p>利用PyBullet模拟生成合成数据。</p><h2 id="三、思考-5">三、思考</h2><p>该方法提出了一种使用仿真数据进行训练，减少人工标注的方法。抓取的方法没有太多创新，仍然需要每个类别提供多个预先的实例以及抓取姿态用于训练。</p><h1>Closed-Loop Next-Best-View Planning for Target-Driven Grasping</h1><blockquote><p><strong>标题</strong>：闭环次优视图规划用于目标驱动的抓取<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/active_grasp">https://github.com/ethz-asl/active_grasp</a></p></blockquote><h2 id="一、目标问题-7">一、目标问题</h2><p>从密集遮挡环境中抓取物体</p><h2 id="二、方法-5">二、方法</h2><p>该方法具有以下前提条件</p><ul><li>机械臂末端连接深度相机</li><li>相机光学中心和手抓中心已经校准</li><li>已知物体的部分视图和3D边界框</li></ul><p><img src="https://img.mahaofei.com/img/202304231603042.png" alt=""></p><p>首先将点云观测yt和相机姿态xt继承，重建为体素图。计算体素的可抓取性，以及可能的抓取姿态。如果可抓取性不满足要求，就调整机械臂位置计算下一张图</p><p><strong>（1）抓取检测</strong></p><p>使用体积抓取网络VGN进行抓取。该网络将体素网格M映射到抓握质量分数Q、平行抓握方向R、开口宽度W。</p><p>过滤掉指尖不在目标边界框的抓取姿态、无法找到反向运动学解的抓取姿态。</p><p><strong>（2）次优视图规划器</strong></p><p><strong>世界表示</strong>：使用TSDF（截断有符号距离函数）表示大小为 l 的立方体体素。</p><p><strong>视图生成</strong>：将候选视图生成在目标边界上半球内。</p><p><strong>信息增益</strong>：TSDF 重建的完整性对于抓取的检测和预测准确性有很大影响。因此使用了后侧体素 IG 公式的变体，对被遮挡具有负距离的体素使用光线投影来计算隐藏对象体素的数量。</p><ol><li>在策略更新的最大数量中加入时间预算</li><li>如果抓取分数低于给定的阈值，就会停止算法，因为获取不到有用信息</li><li>如果VGN在几帧内保持稳定的抓取配置，就停止</li></ol><h2 id="三、思考-6">三、思考</h2><p>该方法是在位姿估计的基础上进行的抓取预测，可以将该方法与Gen6D结合起来，获得物体的6D抓取位姿。</p><h1>Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection</h1><blockquote><p><strong>标题</strong>：边缘抓取网络：一种基于图的SE(3)不变的抓取检测方法<br><strong>作者团队</strong>：Northeastern University（美国东北大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/HaojHuang/Edge-Grasp-Network">https://github.com/HaojHuang/Edge-Grasp-Network</a></p></blockquote><h2 id="一、目标问题-8">一、目标问题</h2><p>以单个视角观察到的点云为输入，得到一组抓取姿态</p><h2 id="二、方法-6">二、方法</h2><p><strong>（1）裁剪点云</strong></p><p>给定一个点云p和接近点pa，只有接近点pa的相邻点会影响抓取，因此以pa为中心裁剪一个球。</p><p><strong>（2）PointNet卷积</strong></p><p>使用PointNet计算接近点与最近邻点，每个点的特征。</p><p><strong>（3）计算全局特征</strong></p><p>将逐点特征传递给MLP，用最大池化层生成一级全局特征。这些全局特征再与点特征相连传递到第二个MLP计算全局特征。</p><p>对于每个抓取，通过将全局特征与点特征连接来计算边缘特征，用分类器表示边缘抓取。</p><p><strong>（4）抓取评估</strong></p><p>使用sigmoid函数的四层MLP来预测抓取成功率，以边缘特征为输入计算抓取是否成功。</p><h2 id="三、思考-7">三、思考</h2><p>该方法类似于DenseFusion的思想，即提取逐点特征和全局特征，进行特征融合，本文得到的融合特征即边缘特征，利用该特征再使用分类器得到抓取位姿。</p>]]></content>
    
    
    <summary type="html">调研近三年顶会顶刊上的抓取姿态估计的论文</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="抓取姿态估计" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>CVPR2022高被引论文笔记</title>
    <link href="https://www.mahaofei.com/post/6ec34466.html"/>
    <id>https://www.mahaofei.com/post/6ec34466.html</id>
    <published>2023-04-23T02:45:17.000Z</published>
    <updated>2023-04-23T02:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、目标检测</h1><h2 id="1-1-视频目标检测">1.1 视频目标检测</h2><h3 id="Video-Swin-Transformer">Video Swin Transformer</h3><blockquote><p><strong>标题</strong>：视频 Swin Transformer<br><strong>作者团队</strong>：Microsoft Research Asia<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>现今大多数的视觉识别模型都是基于Transformer建立的，本文在此基础上进行调整，得到更好的速度和精度。</p><p><strong>（2）方法</strong></p><ol><li>总体架构</li></ol><p>视频定义为TxHxWx3，patch为2x4x4x3的块，每个patch有96个特征维度。该架构的主要组件是Video Swin Transformer模块，通过将标准的Transformer的Multihead self-attention(MSA)模块替换为基于3D Shift Window的MSA模块，来实现。</p><p><img src="https://img.mahaofei.com/img/202305071021361.png" alt="image.png"></p><ol start="2"><li>3D MSA模块</li></ol><p>由于视频有时间维度，全局自注意模块会导致巨大的计算和内存成本。MSA模块就比传统的全局自注意模块要高效。</p><p>更进一步，基于Swin Transformer的2D移位窗口扩展到3D，实现了跨窗口链接，保证了体系结构的表达能力。</p><p><img src="https://img.mahaofei.com/img/202305071033856.png" alt="image.png"></p><h1>二、图像分割</h1><h1>三、图像处理</h1><h2 id="3-1-图像合成">3.1 图像合成</h2><h3 id="High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models">High-Resolution Image Synthesis with Latent Diffusion Models</h3><blockquote><p><strong>标题</strong>：具有潜在扩散模型的高分辨率图像合成<br><strong>作者团队</strong>：海德堡大学；Runway ML<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>扩散模型已经在包括图像数据在内的很多数据上，实现了很好的数据合成效果。但这些模型由于直接操作像素，需要昂贵的GPU资源。</p><p>本文提出的潜在扩散模型，达到了降低复杂性和保留细节的平衡点。</p><p><strong>（2）方法</strong></p><p>主要方法是：使用自动编码模型，学习一个在感知上与图像空间等效的空间，压缩学习阶段和生成学习阶段来减少资源需求。</p><ol><li>感知压缩模型<br>利用了结合perceptual loss, patch-based, adversarial objective的自动编码器。</li><li>潜在扩散模型<br>扩散模型是概率模型，通过逐渐对正态分布变量去噪来学习数据分布。<br>通过由自动编码器得到的高效、低维的空间，与高维像素空间相比更适合生成模型。</li><li>调节机制<br>通过使用交叉注意力机制增强基础网络UNet，能够处理各种模态的输入。</li></ol><p><strong>（3）思考</strong></p><p>将需要高运算量的像素操作，通过自动编码转换为了低维空间的操作，节省了计算量。</p><h1>四、三维视觉</h1><h1>五、位姿估计</h1><h1>六、机器人</h1><h1>七、神经网络</h1><h2 id="7-1-神经网络结构设计">7.1 神经网络结构设计</h2><h3 id="A-ConvNet-for-the-2020s">A ConvNet for the 2020s</h3><blockquote><p><strong>标题</strong>：2020s的ConvNet<br><strong>作者团队</strong>：Facebook AI<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/facebookresearch/ConvNeXt">https://github.com/facebookresearch/ConvNeXt</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>20年以来，由于Vision Transformers的引入，它开始快速取代卷积神经网络。但只使用Transformers也有些问题，因此后来又出现了hierarchical Transformers，其中加入了几个卷积神经网络作为先验。但这些方法都可以归结为Transformers的优势。</p><p>本文想要探讨纯卷积神经网络所能实现的极限。</p><p><strong>（2）最佳方法</strong></p><ol><li><strong>训练技术</strong>：使用AdamW优化器、数据增强、随机擦除、正则化等方法可以显著提高训练模型的性能</li><li><strong>宏观设计</strong>：<ul><li>阶段比例：ResNet中各阶段的比例很大程度是经验获得的，SwinTransformer的比例是1:1:3:1，传统的ResNet比例是(3,4,6,3)，此处调整为(3,3,9,3)与SwinT相同，发现也提高了模型准确率</li><li>模块设计：标准的ResNet模块包括一个7x7步长2的卷积层，然后是一个最大池化层。此处模仿SwinT，设计为4x4步长为4的卷积层作为基础模块。</li></ul></li><li><strong>使用分组卷积技术</strong>，可以有效提高网络性能</li><li><strong>反向瓶颈</strong>：使MLP的隐藏维度比输入维度宽4倍，这在几个ConvNet中以及Transformer中设计思路相同。</li><li><strong>更大的卷积核</strong>：尽管堆叠小卷积核可以有效利用硬件，但测试证明，总体上大卷积核能够提高模型性能</li><li><strong>微观设计</strong>：<ul><li>更少的归一化层</li><li>使用层归一化LN代替批归一化BatchNorm</li><li>分离下采样层：ResNet中，下采样是通过每个阶段开始的残差块实现的，在层和层之间加入单独的下采样层发现可以提高准确率</li></ul></li></ol><p><strong>（3）总结</strong></p><ol><li>尽可能丰富数据，增大随机化程度：使用AdamW优化器、数据增强、随机擦除、正则化等方法</li><li>使用更优化的网络结构：调整各阶段卷积比例、使用反向瓶颈设计、更少的归一化层、更大的卷积核、在每个阶段之间加入下采样层。</li></ol>]]></content>
    
    
    <summary type="html">阅读CVPR的高被印论文，开拓视野。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="顶会顶刊" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E9%A1%B6%E4%BC%9A%E9%A1%B6%E5%88%8A/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>如何使用Git管理项目代码</title>
    <link href="https://www.mahaofei.com/post/dd16f220.html"/>
    <id>https://www.mahaofei.com/post/dd16f220.html</id>
    <published>2023-04-17T07:00:50.000Z</published>
    <updated>2023-04-17T07:00:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、注册Github并创建仓库</h1><p>这一步不细说了，需要科学上网，参考<a href="https://www.mahaofei.com/post/96c83ac9.html">这篇文章</a>，[[03_科学上网方法（如何访问Google, ChatGPT）|Google学术访问方法]]。</p><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><h1>二、下载Git并配置</h1><h2 id="2-1-Git安装">2.1 Git安装</h2><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><p>在资源管理器内右键，选择<code>Git bash here</code>打开Git界面。</p><h2 id="2-2-Git配置">2.2 Git配置</h2><p>输入下面的代码，按下回车，生成ssh密钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;邮箱地址&quot;</span><br></pre></td></tr></table></figure><p>出现要求设置密码，可以不用设置，连续回车两次就可以。</p><p>打开<code>C:\Users\用户名\.ssh</code>，可以看到有一个<code>id_rsa.pub</code>文件，这就是刚才生成的密钥。</p><p>使用记事本打开此文件，复制里面的密钥内容。</p><h2 id="2-3-Github添加ssh-key">2.3 Github添加ssh key</h2><p>进入<a href="https://github.com/">Github官网</a>，点击右上角【setting --&gt; SSH and GPG keys --&gt; New SSH key】，在这里添加密钥，其中</p><ul><li>Title：自己写一个ssh key的名字，用于区分多个ssh key</li><li>Key：刚刚复制的密钥<br>填写完成后点击Add SSH key添加。</li></ul><p>然后在git bash中输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>如果连接成功，会让你输入<code>yes/no</code>，输入yes即可。</p><h2 id="2-4-配置用户名和邮箱">2.4 配置用户名和邮箱</h2><p>输入下面的代码配置自己的用户名和邮箱，两个信息都要和Github账号的信息一致</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;用户名&quot;</span><br><span class="line">git config --global user.email &quot;邮箱&quot;</span><br></pre></td></tr></table></figure><h1>三、代码管理</h1><h2 id="3-1-代码上传">3.1 代码上传</h2><p><strong>（1）初始化</strong></p><p>创建一个文件夹，在这个文件夹内，右键<code>git bash here</code>，然后输入<code>git init</code>完成初始化。</p><p>可以看到目录中出现了一个<code>.git</code>隐藏文件夹，这说明已经完成了初始化。</p><p><strong>（2）链接远程仓库</strong></p><p>在刚刚的<code>git bash</code>窗口，输入下面的命令同步到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p>如果出现fatal: remote origin already exists.可按以下步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br><span class="line">git pull git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p><strong>（3）上传本地文件</strong></p><p>添加本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add.</span><br></pre></td></tr></table></figure><p>提交本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;说明信息，一般说明本次提交更新了什么&quot;</span><br></pre></td></tr></table></figure><p>推送到远端仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push git@github.com:用户名/仓库名.git</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><h2 id="3-2-拉取代码">3.2 拉取代码</h2><p>从项目中拉取代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><p>如果出现<code>fatal: refusing to merge unrelated histories</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br></pre></td></tr></table></figure><h2 id="3-3-分支管理">3.3 分支管理</h2><p><strong>（1）查看分支</strong></p><p>在命令行窗口的光标处，输入git branch命令，查看 Git 仓库的分支情况。分支前有*表示是当前所在的分支。</p><p><strong>（2）创建分支</strong></p><p>使用下面的命令创建一个名为a的分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch a</span><br></pre></td></tr></table></figure><p><strong>（3）分支切换</strong></p><p>在命令行窗口的光标处，输入git checkout a命令，切换到a分支。</p><p><strong>（4）合并分支</strong></p><p>切换到master分支，然后输入git merge a命令，将a分支合并到master分支。</p><p><strong>（5）删除分支</strong></p><p>在命令行窗口的光标处，输入git branch -d a命令，删除a分支。</p><p><strong>（6）为分支添加标签</strong></p><p>在命令行窗口的光标处，输入git tag test_tag命令，为当前分支添加标签test_tag</p><h2 id="3-4-修改分支名称">3.4 修改分支名称</h2><p>假设分支名称为oldName，想要修改为 newName</p><ol><li>本地分支重命名(还没有推送到远程)</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><ol start="2"><li>远程分支重命名 (已经推送远程-假设本地分支和远程对应分支名称相同)</li></ol><p>重命名远程分支对应的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><p>到github修改默认分支的分支名。</p><p>上传新命名的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin newName</span><br></pre></td></tr></table></figure><p>把修改后的本地分支与远程分支关联</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --set-upstream-to origin/newName</span><br></pre></td></tr></table></figure><p>注意：如果本地分支已经关联了远程分支，需要先解除原先的关联关系：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --unset-upstream </span><br></pre></td></tr></table></figure><h2 id="3-5-报错信息">3.5 报错信息</h2><p><strong>（1）error: src refspec master does not match any. error: failed to push some refs to</strong></p><p>仔细检查push的是<code>master</code>分支还是<code>main</code>分支。</p>]]></content>
    
    
    <summary type="html">当有多台设备，或者同一个项目有多个版本的代码时，利用git管理项目代码就十分必要了。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/tags/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    <category term="Git" scheme="https://www.mahaofei.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>【抓取算法】Contact GraspNet</title>
    <link href="https://www.mahaofei.com/post/c18d351e.html"/>
    <id>https://www.mahaofei.com/post/c18d351e.html</id>
    <published>2023-04-07T07:52:27.000Z</published>
    <updated>2023-04-07T07:52:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><blockquote><p><strong>标题</strong>：Contact-GraspNet: 在杂乱场景中高效生成6-DoF抓取<br><strong>期刊会议</strong>：ICRA2021<br><strong>作者团队</strong>：Martin Sundermeyer（NVIDIA）<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a><br><strong>数据集</strong>：</p></blockquote><h2 id="1-1-目标问题-2">1.1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="1-2-方法">1.2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/20230404152359.png" alt="image.png"></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/20230404153946.png" alt="image.png"></p><h2 id="1-3-思考">1.3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1>二、算法复现</h1><h2 id="2-1-准备工作-2">2.1 准备工作</h2><p><strong>（1）环境搭建</strong></p><p>下载代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/NVlabs/contact_graspnet.git</span><br></pre></td></tr></table></figure><p>创建虚拟环境<br>（这个环境是没问题的，如果出现依赖不满足要求的情况，可以先删掉那项，创建完环境后再手动安装）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f contact_graspnet_env.yml</span><br></pre></td></tr></table></figure><p>重新编译<code>pointnet_tfops</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh compile_pointnet_tfops.sh</span><br></pre></td></tr></table></figure><p><strong>（2）模型和数据准备</strong></p><p>从作者给出的连接下载<a href="https://drive.google.com/drive/folders/1tBHKf60K8DLM5arm-Chyf7jxkzOr5zGl?usp=sharing">trained models</a>，并将它们放到<code>./checkpoints/</code>，下载<a href="https://drive.google.com/drive/folders/1v0_QMTUIEOcu09Int5V6N2Nuq7UCtuAA?usp=sharing">test data</a>，并将它们放到<code>./test_data</code></p><h2 id="2-2-预测抓取">2.2 预测抓取</h2><p>给定一个深度图(.npy文件/单位m)，相机内参，2D实例分割图，运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/inference.py \</span><br><span class="line">       --np_path=test_data/0.npy \</span><br><span class="line">       --local_regions --filter_grasps</span><br></pre></td></tr></table></figure><p><code>--np_path</code>：输入的.npz/.npy文件，带有深度、内参、实力分割图、RGB信息<br><code>--ckpt_dir</code>：checkpoint目录，默认为<code>checkpoint/scene_test_2048_bs3_hor_sigma_001</code>，非常干净的深度数据使用<code>scene_2048_bs3_rad2_32</code>，非常混乱的深度数据使用<code>scene_test_2048_bs3_hor_sigma_0025</code><br><code>--local_regions</code>：裁剪的3D实例分割<br><code>--filter_grasps</code>：筛选抓取点，使他们只为于对象的表面<br><code>--skip_border_objects</code>：忽略碰到深度图边缘的实例分割<br><code>--forward_passes</code>：前向计算的次数，增加可以提高抓取的采样点<br><code>--z_range</code>：[min, max]的z值来裁剪输入点云<br><code>--arg_configs TEST.second_thres:0.19 TEST.first_thres:0.23</code>：覆盖抓取的配置置信度来获得更多或更少的抓取候选</p><h2 id="2-3-训练网络">2.3 训练网络</h2><p><strong>（1）下载数据集</strong></p><ul><li>下载<a href="https://drive.google.com/file/d/1zcPARTCQx2oeiKk7a-wdN_CN-RUVX56c/view?usp=sharing">Acronym</a>数据集</li><li>从<a href="https://www.shapenet.org/">https://www.shapenet.org/</a>下载ShapeNet meshe</li><li>创建watertiget<ul><li>下载并构建<a href="https://github.com/hjwdzh/Manifold">https://github.com/hjwdzh/Manifold</a></li><li>创建watertight mesh，假设物体路径为model.obj：<code>manifold model.obj temp.watertight.obj -s</code></li><li>简化：<code>simplify -i temp.watertight.obj -o model.obj -m -r 0.02</code></li></ul></li></ul><p>下载10000个带有Contact抓取信息的桌面训练场景<a href="https://drive.google.com/drive/folders/1eeEXAISPaStZyjMX8BHR08cdQY4HF4s0">Google Drive</a>，解压为下面的格式</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">acronym</span><br><span class="line">├── grasps</span><br><span class="line">├── meshes</span><br><span class="line">├── scene_contacts</span><br><span class="line">└── splits</span><br></pre></td></tr></table></figure><p><strong>（2）训练Contact-GraspNet</strong></p><p>如果在没有外设的服务器上训练，设置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYOPENGL_PLATFORM=&#x27;egl&#x27;</span><br></pre></td></tr></table></figure><p>使用配置文件<code>contact_graspnet/config.yaml</code>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/train.py --ckpt_dir checkpoints/your_model_name \</span><br><span class="line">                                 --data_path /path/to/acronym/data</span><br></pre></td></tr></table></figure><p><strong>（3）生成自己的Contact Grasps和场景</strong></p><p>所下载的<code>scene_contacts</code>是从Acronym数据集生成的，要生成自己的数据集，下载安装<a href="https://github.com/NVlabs/acronym">acronym_tools</a>。</p><p>第一步，对象的6D抓取被映射到保存在<code>mesh_contacts</code>的接触点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_contact_infos.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>根据生成的<code>mesh_contacts</code>，可以创建桌面场景保存到<code>scene_contacts</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>一个线程大约花费3天，可以多次运行命令在多个核上并行处理。</p><p>可视化显示创建的桌面场景和抓取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym \</span><br><span class="line">       --load_existing scene_contacts/000000.npz -vis</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">复现NVIDIA提出的抓取估计算法Contact GraspNet</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>使用内网穿透SakuraFrp远程连接服务器</title>
    <link href="https://www.mahaofei.com/post/9ed2c32f.html"/>
    <id>https://www.mahaofei.com/post/9ed2c32f.html</id>
    <published>2023-04-05T05:59:35.000Z</published>
    <updated>2023-04-05T05:59:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Linux端配置</h1><p><strong>（1）ssh配置</strong></p><p>安装ssh服务器与客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt -y install openssh-server</span><br><span class="line">sudo apt -y install openssh-client</span><br></pre></td></tr></table></figure><p>配置ssh客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><ul><li>​将<code>PermitRootLogin prohibt-password</code> 修改为 <code>PermitRootLogin yes</code></li><li>将<code>PasswordAuthentication yes</code> 前的#删除，取消注释</li></ul><p>重启ssh服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh restart</span><br></pre></td></tr></table></figure><p>查看ssh服务运行状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh status</span><br></pre></td></tr></table></figure><p><strong>（2）Sakura配置</strong></p><p><a href="https://www.natfrp.com/user/">SakuraFrp</a></p><p>进入隧道列表新建隧道</p><ul><li>尽量选择国内节点</li><li>隧道类型为TCP隧道</li><li>本机端口为SSH</li><li>主机ip默认127.0.0.1即可(代指内网穿透本机)</li></ul><p><img src="https://img.mahaofei.com/img/20230405140317.png" alt=""></p><p>在官网下载对应版本的frpc，复制下载链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget -O frpc &lt;下载地址&gt;</span><br><span class="line">chmod 755 frpc</span><br><span class="line">ls -ls frpc</span><br><span class="line">md5sum frpc</span><br><span class="line">frpc -v</span><br></pre></td></tr></table></figure><p>隧道配置文件中复制隧道密钥</p><p>Ubuntu中使用下面的命令开启隧道</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frpc -f &lt;复制的密钥&gt;</span><br></pre></td></tr></table></figure><h1>Windows端配置</h1><p>打开【设置-应用-添加功能】，添加OpenSSH 服务器和OpenSSH 客户端。</p><p>打开服务，找到 OpenSSH SSH Server 和 OpenSSH Authentication Agent -&gt; 启动服务并设为自动。</p><p>打开 power shell，使用以下命令检查安装和运行情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-Service sshd</span><br></pre></td></tr></table></figure><p>打开Sakura官网，打开隧道列表，点击要连接的隧道，点击一键认证，下载exe认证程序并运行。</p><p>然后使用<code>ssh -p &lt;端口号&gt; &lt;用户名&gt;@&lt;地址&gt;</code>进行远程连接</p><h1>VSCode远程ssh开发环境</h1><p>安装插件 <code>Remote - SSH</code></p>]]></content>
    
    
    <summary type="html">不想使用向日葵和todesk等工具远程连接桌面，而且个人电脑和服务器也不在一个局域网下，想要远程连接服务器，因此考虑使用内网穿透。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】Gen6D算法</title>
    <link href="https://www.mahaofei.com/post/76335f84.html"/>
    <id>https://www.mahaofei.com/post/76335f84.html</id>
    <published>2023-03-29T13:22:39.000Z</published>
    <updated>2023-03-29T13:22:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1>论文笔记</h1><h2 id="1-介绍">1. 介绍</h2><h3 id="1-1-目标问题">1.1 目标问题</h3><p>现有的位姿估计算法要么需要高质量的物体模型，要么需要提供额外的深度图或物体掩码图，这对于位姿估计的实际应用有很大的限制。本文提出的方法只需要一些物体的姿态图像，就能够在任意环境中预测物体位姿。</p><p>作者认为一个位姿估计器应该具有以下特点：</p><ul><li>通用性：可以应用于任意物体，而无需对对象或类别进行训练</li><li>无模型：用于一个未见过的物体时，只需要一些已知姿态的参考图像来定义物体参考坐标系即可</li><li>输入简单：仅输入RGB图像来估计位姿，而不需要深度图或物体掩码图</li></ul><p><strong>（1）如何设计视角选择器，从参考图像中找到与查询图像视角最接近的</strong></p><p>本文使用神经网络对查询图像和参考图像进行逐像素比较，产生相似性得分，并选择具有最高相似性得分的参考图像。并添加了全局归一化层和自注意层来共享不同参考图像之间的相似性信息，为选择最相似的参考图像提供了上下文信息。</p><p><strong>（2）实现没有模型的姿态优化</strong></p><p>本文提出了一种新的基于三维空间的姿态优化方法，给定一个查询图像和一个输入姿态，找到几个接近输入姿态的参考图像，将这些参考图像投影回3D空间中，构建特征空间，通过3D的CNN将构建的特征空间与查询图像的特征相匹配，来优化姿态。</p><h3 id="1-2-现有工作">1.2 现有工作</h3><p>现有位姿估计方法大都是基于特定实例的，不能推广到未见过的物体，通常都需要根据物体3D模型来渲染大量图像进行训练。有一些方法可以推广到类别级，也不需要对象的模型，但仍然无法预测没见过的类别的物体。</p><h2 id="2-实现方法">2. 实现方法</h2><p><strong>数据规范化</strong>：对于每个物体，通过对参考图像中的点进行三角测量等方法估计物体的大致大小，然后对物体坐标系进行归一化，使物体中心位于原点，大小为1，此时物体位于原点的单位球体内。</p><p>Gen6D包括一个物体检测器，一个视角选择器，一个姿态优化器。</p><p><img src="https://img.mahaofei.com/img/20230403203020.png" alt="image.png"></p><p>物体检测其首先利用查询图像和参考图像来检测物体所在区域。然后视角选择器将查询图像于参考图像相匹配，产生粗略的初始姿态。最后由姿态优化器进一步细化以得到精确的对象姿态。</p><h3 id="2-1-物体检测">2.1 物体检测</h3><p>将检测问题分解成两部分</p><ol><li>找到对象中心的2D投影点q</li><li>估计包围单位球体的正方形边界框。</li></ol><p><img src="https://img.mahaofei.com/img/20230403204751.png" alt="image.png"></p><p>物体中心的深度可以使用$d=2f/S_q$求得，其中2是单位球体的直径，f是虚拟焦距（将主点设为投影点q），$S_q$是边界框边长。这就是物体的初始平移。</p><blockquote><p>问题：这里将物体归一化之后求出的深度d还是真实深度吗？虚拟焦距又是如何确定的？</p></blockquote><p>检测器使用了VGG网络提取参考图像和查询图像的特征图，然后将所有参考图像的特征图作为卷积核与查询图像的特征图卷积，得到分数图。考虑尺度差异，设置再多个预定义尺度上进行卷积，最后得到热力图和比例图。选择热力图上的最大值位置作为对象中心2D投影，使用比例图上相同比例的比例作为边界框的大小$S_q=S_r*s$。</p><blockquote><p>问题：这里将所有参考图像的特征图都进行卷积，那么参考图像上物体特征和背景特征是如何区分的？</p></blockquote><h3 id="2-2-视角选择">2.2 视角选择</h3><p>将查询图像与每个参考图像比较，计算相似性得分。计算每个参考图像和查询图像的元素乘积，获得得分图，并计算相似性参数。</p><p><img src="https://img.mahaofei.com/img/20230404192903.png" alt="image.png"></p><p><strong>（1）平面内旋转</strong><br>为了考虑平面内旋转，本文将参考图像旋转Na个预定义角度，查询时使用所有旋转版本进行逐元素乘积。</p><p><strong>（2）全局归一化</strong><br>使用参考图像的所有特征图计算的均值和方差，对相似度网络生成的特征图进行归一化。这样做可以用特征图的分布来编码上下文相似性，并放大不同图像之间的相似性差异。</p><p><strong>（3）参考视角变换</strong><br>在所有参考图像的相似性特征向量上应用变换，包括它们的视角、注意力层。这样的变换器使得特征向量相互通信以编码上下文信息，有助于确定最相似的参考图像。</p><h3 id="2-3-姿态优化">2.3 姿态优化</h3><p>经过上面两个步骤，我们已经有了粗略的物体位姿。本步骤对位姿进行优化。</p><p>选择接近输入姿态的6个参考图像，通过CNN提取特征图，然后将特征图投影到3D空间中，并计算特征的均值和方差作为空间顶点的特征。<br>对于查询图像，使用同样的CNN提取特征图，将特征图投影到3D空间中，并将查询特征与参考图像特征的均值和方差连接起来。</p><p>最后在空间特征上使用3DCNN预测残差来更新输入姿态。</p><p><img src="https://img.mahaofei.com/img/20230404195340.png" alt="image.png"></p><h1>3. 实验分析</h1><h1>二、算法复现</h1><h2 id="2-1-环境搭建">2.1 环境搭建</h2><h3 id="2-1-1-Python环境">2.1.1 Python环境</h3><p>创建[[02_Anaconda的基本使用与在Pycharm中调用|Anaconda虚拟环境]]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gen6d python=3.7</span><br><span class="line">conda activate gen6d</span><br></pre></td></tr></table></figure><p>安装pytorch环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 -c pytorch</span><br></pre></td></tr></table></figure><p>安装依赖，打开<code>requirements.txt</code>，删除其中的pytorch, torchvision, cudatoolkit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="2-1-2-自制数据集工具">2.1.2 自制数据集工具</h3><p><strong>（1）COLMAP</strong></p><p>参考官网教程：<a href="https://colmap.github.io/install.html">https://colmap.github.io/install.html</a></p><p>安装依赖库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-regex-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev \</span><br><span class="line">    libcgal-qt5-dev</span><br></pre></td></tr></table></figure><p>下载COLMAP源代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/colmap/colmap</span><br><span class="line">cd colmap</span><br></pre></td></tr></table></figure><p>修改<code>CMakeLists.txt</code>文件，添加下面的内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_CUDA_ARCHITECTURES &quot;70&quot;)</span><br></pre></td></tr></table></figure><p>开始编译、安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -GNinja</span><br><span class="line">ninja</span><br><span class="line">sudo ninja install</span><br></pre></td></tr></table></figure><p><strong>（2）CloudCompare</strong></p><p>安装Flatpak</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install flatpak</span><br></pre></td></tr></table></figure><p>安装Software Flatpak plugin</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-software-plugin-flatpak</span><br></pre></td></tr></table></figure><p>添加Flathub repository</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo</span><br></pre></td></tr></table></figure><p>安装CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak install flathub org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p>运行CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><strong>（3）安装ffmpeg</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ffmpeg</span><br></pre></td></tr></table></figure><h2 id="2-2-数据集准备">2.2 数据集准备</h2><h3 id="2-2-1-官方数据集">2.2.1 官方数据集</h3><p><strong>（1）下载数据集</strong></p><p>从<a href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EkWESLayIVdEov4YlVrRShQBkOVTJwgK0bjF7chFg2GrBg?e=Y8UpXu">原作者给出的链接</a>中下载预训练模型，GenMOP数据集和processed LINEMOD数据集。</p><p><strong>（2）组织数据集</strong></p><p>将下载的文件按照下面的格式进行整理。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- model</span><br><span class="line">        |-- detector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- selector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- refiner_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br></pre></td></tr></table></figure><h3 id="2-2-2-自制数据集">2.2.2 自制数据集</h3><p><strong>（1）视频录制</strong></p><p>使用手机录制目标物体的参考视频和测试视频。注意：参考视频需要满足以下条件</p><ul><li>参考视频中对象是静态的</li><li>参考视频中背景尽可能纹理丰富且平整，摄像角度要尽可能覆盖每个角度，以便COLMAP恢复相机姿态</li></ul><p><strong>（2）组织文件</strong></p><p>将视频按照下面的路径进行组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- video</span><br><span class="line">           |-- mouse-ref.mp4</span><br><span class="line">           |-- mouse-test.mp4</span><br></pre></td></tr></table></figure><p><strong>（3）将参考视频拆分为图像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 每10帧保存一张图像，最大图像边长为960</span></span></span><br><span class="line">python prepare.py --action video2image \</span><br><span class="line">                  --input data/custom/video/ref/coffeebox-ref.mp4 \</span><br><span class="line">                  --output data/custom/coffeebox/images \</span><br><span class="line">                  --frame_inter 10 \</span><br><span class="line">                  --image_size 960 \</span><br><span class="line">                  --transpose</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 或者</span></span></span><br><span class="line">python prepare.py --action video2image --input data/custom/video/ammeter-ref.mp4 --output data/custom/ammeter/images --frame_inter 10 --image_size 960 --transpose</span><br></pre></td></tr></table></figure><p>拆分后的视频保存在<code>data/custom/mouse/images</code>中。</p><p><strong>（4）运行COLMAP SfM恢复相机姿态</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action sfm --database_name custom/ammeter --colmap &lt;path-to-your-colmap-exe&gt;</span><br></pre></td></tr></table></figure><p>注：<code>&lt;path-to-your-colmap-exe&gt;</code>可以通过命令<code>which colmap</code>来查找，一般ubuntu路径为<code>/usr/local/bin/colmap</code>，windows路径为<code>E:/Programming/COLMAP-3.8-windows-cuda/COLMAP.bat</code></p><p><strong>（5）手动处理点云</strong></p><p>通过裁减对象点云来手动确定对象所在区域。例如使用<a href="https://www.cloudcompare.org/">CloudCompare</a>来可视化处理COLMAP重建的点云，重建的点云位于<code>data/custom/mouse/colmap/pointcloud.ply</code>中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230327215520.png" alt=""></p><p>导出裁剪后的点云为<code>data/custom/mouse/object_point_cloud.ply</code>。</p><p><img src="https://img.mahaofei.com/img/20230327220042.png" alt=""></p><p><strong>（6）手动确定对象的X轴正方向和Y轴正方向</strong></p><p><img src="https://img.mahaofei.com/img/20230327220221.png" alt=""></p><p><img src="https://img.mahaofei.com/img/20230327220225.png" alt=""></p><p>编辑一个<code>data/custom/mouse/meta_info.txt</code>文件来保存你的X+和Z+信息，例如</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.297052 0.350839 -0.000593</span><br><span class="line">0.973488 0.054352 -0.222188</span><br></pre></td></tr></table></figure><p><strong>（7）确保您具有以下文件，这些文件由上述步骤生成</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- mouse</span><br><span class="line">           |-- object_point_cloud.ply  ## object point cloud</span><br><span class="line">           |-- meta_info.txt           ## meta information about z+/x+ directions</span><br><span class="line">           |-- images                  ## images</span><br><span class="line">           |-- colmap                  ## colmap project</span><br></pre></td></tr></table></figure><p><strong>（8）从处理后的参考图像中预测姿势</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python predict.py --cfg configs/gen6d_pretrain.yaml \</span><br><span class="line">                  --database custom/coffeebox_lied \</span><br><span class="line">                  --video data/custom/video/coffeebox-test.mp4 \</span><br><span class="line">                  --resolution 960 \</span><br><span class="line">                  --transpose \</span><br><span class="line">                  --output data/custom/ammeter_processed/test \</span><br><span class="line">                  --ffmpeg &lt;path-to-ffmpeg-exe&gt;</span><br></pre></td></tr></table></figure><h2 id="2-3-训练与评估">2.3 训练与评估</h2><p>将文件按照下面的形式组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br><span class="line">    |-- shapenet</span><br><span class="line">        |-- shapenet_cache</span><br><span class="line">        |-- shapenet_render</span><br><span class="line">        |-- shapenet_render_v1.pkl</span><br><span class="line">    |-- co3d_256_512</span><br><span class="line">        |-- apple</span><br><span class="line">            ...</span><br><span class="line">    |-- google_scanned_objects</span><br><span class="line">        |-- 06K3jXvzqIM</span><br><span class="line">            ...</span><br><span class="line">    |-- coco</span><br><span class="line">        |-- train2017</span><br></pre></td></tr></table></figure><h3 id="2-3-1-训练detector">2.3.1 训练detector</h3><p>修改<code>train_meta_info.py</code>的第86行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;genmop_train&#x27;</span>: [<span class="string">f&#x27;genmop/<span class="subst">&#123;name&#125;</span>-test&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">&#x27;ammeter&#x27;</span>, <span class="string">&#x27;coffeebox&#x27;</span>, <span class="string">&#x27;realsensebox&#x27;</span>]],</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第109行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">genmop_meta_info=&#123;</span><br><span class="line">    <span class="string">&#x27;ammeter&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0222805</span>, -<span class="number">0.409031</span>, <span class="number">0.912248</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.401556</span>, <span class="number">0.773825</span>, <span class="number">0.340199</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;coffeebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0718405</span>, -<span class="number">0.471545</span>, <span class="number">0.878911</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.582604</span>, -<span class="number">0.490501</span>, -<span class="number">0.219265</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;realsensebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.103463</span>, -<span class="number">0.521284</span>, <span class="number">0.847088</span>],np.float32), <span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">1.690831</span>, <span class="number">0.688506</span>, <span class="number">0.590004</span>],np.float32)&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第212行，修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras, images, points3d = read_model(<span class="string">f&#x27;<span class="subst">&#123;GenMOP_ROOT&#125;</span>/<span class="subst">&#123;seq_name&#125;</span>/colmap/sparse/0&#x27;</span>)</span><br></pre></td></tr></table></figure><p>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/detector/detector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-2-训练selector">2.3.2 训练selector</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/selector/selector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-3-训练refiner">2.3.3 训练refiner</h3><p>为refiner训练进行数据准备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database linemod/cat \</span><br><span class="line">                  --que_split linemod_val \</span><br><span class="line">                  --ref_database linemod/cat \</span><br><span class="line">                  --ref_split linemod_val</span><br><span class="line"></span><br><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database genmop/tformer-test \</span><br><span class="line">                  --que_split all \</span><br><span class="line">                  --ref_database genmop/tformer-ref \</span><br><span class="line">                  --ref_split all </span><br></pre></td></tr></table></figure><p>该命令会在<code>data/val</code>生成信息，该信息会被用于生成refiner的有效数据</p><p>训练refiner</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/refiner/refiner_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-4-评估所有组件">2.3.4 评估所有组件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object TFormer from the GenMOP dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name genmop/tformer</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object <span class="built_in">cat</span> from the LINEMOD dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name linemod/cat</span><br></pre></td></tr></table></figure><h1>三、现存问题</h1><p><strong>优点</strong></p><ol><li>只需要对给定物体录制1-2分钟的视频，使用程序1-2小时<strong>添加数据集</strong>，即可实现新物体的位姿估计，不需要再训练网络</li><li><strong>精度</strong>还可以</li></ol><p><strong>缺点</strong></p><ol><li>对于<strong>方形凸形物体识别较好，对于物体内部存在中空区域</strong>，例如圆环等物体识别效果较差</li><li>由于<strong>参考视频要求物体静止，因此无法录到物体底面的特征</strong>，对于物体底面识别效果较差（可考虑物体正反放置录制两次，对于同一个物体使用两个参考视频进行预测，选择置信度高的位姿）</li><li>当进行识别时，如果<strong>图像中不存在物体也会生成一个估计位姿</strong>（可以考虑根据置信度判断输出，或者在位姿估计前使用yolo等算法预判断物体位置）</li><li>当存在<strong>遮挡时位姿估计效果较差</strong>，可能会出现只框处未被遮挡的部分，或者在遮挡物体上强行进行位姿估计。</li><li>当要同时识别的物体很多时，对于显卡显存要求比较大，而且计算会很慢，服务器1.5s/it。如果每次只对某个特定物体进行识别，速度还可以。</li></ol>]]></content>
    
    
    <summary type="html">算法复现</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="Gen6D" scheme="https://www.mahaofei.com/tags/Gen6D/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】GDRNPP算法</title>
    <link href="https://www.mahaofei.com/post/250dc866.html"/>
    <id>https://www.mahaofei.com/post/250dc866.html</id>
    <published>2023-03-27T01:23:53.000Z</published>
    <updated>2023-04-01T01:23:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</h1><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Gu Wang,  Tsinghua University, BNRist<br><strong>关键词</strong>：位姿估计；端到端<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/THU-DA-6D-Pose-Group/GDR-Net">https://github.com/THU-DA-6D-Pose-Group/GDR-Net</a></p></blockquote><h2 id="1-目标问题-21">1 目标问题</h2><p>提出一种端到端的位姿估计算法。</p><h2 id="2-方法-21">2 方法</h2><p><img src="https://img.mahaofei.com/img/20230316104020.png" alt=""></p><p><strong>（1）网络架构</strong></p><p>首先向GDR-Net提供256x256的ROI图，预测出三个64x64的中间特征图</p><ul><li>稠密对应图$M_{2D-3D}$：将稠密坐标映射$M_{XYZ}$对跌倒2D像素坐标上得到，反映了对象的几何形状信息。</li><li>表面区域注意图$M_{SRA}$：采用最远点采样从$M_{XYZ}$中到处，代表了对象的对称性。</li><li>可见对象掩码$M_{vis}$</li></ul><p>使用一个简单的2D卷积Patch Pnp模块直接从特征图中回归6D对象姿态。Patch PnP模块由三个卷积层组成，然后用两个全连接层用于扁平化特征，最后连个全连接层输出R6D旋转和tSITE平移。</p><h2 id="3-思考-21">3 思考</h2><p>本文专注于图像的特征提取和处理工作，实现从单一图片预测位姿的功能。</p><h1>二、算法复现</h1><h2 id="2-1-数据集准备">2.1 数据集准备</h2><p>下载<a href="https://bop.felk.cvut.cz/datasets/">BOP数据集</a>和<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">VOC2012数据集</a>，从<a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EgOQzGZn9A5DlaQhgpTtHBwB2Bwyx8qmvLauiHFcJbnGSw?e=EZ60La">Onedrive (password: groupji)</a>或者<a href="https://pan.baidu.com/s/1FzTO4Emfu-DxYkNG40EDKw">百度网盘(密码: vp58)</a>下载test_boxes，完成后datasets文件夹的结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">datasets/</span><br><span class="line">├── BOP_DATASETS   # https://bop.felk.cvut.cz/datasets/</span><br><span class="line">    ├──tudl</span><br><span class="line">    ├──lmo</span><br><span class="line">    ├──ycbv</span><br><span class="line">    ├──icbin</span><br><span class="line">    ├──hb</span><br><span class="line">    ├──itodd</span><br><span class="line">    └──tless</span><br><span class="line">└──VOCdevkit</span><br></pre></td></tr></table></figure><p>从<a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EgOQzGZn9A5DlaQhgpTtHBwB2Bwyx8qmvLauiHFcJbnGSw?e=EZ60La">Onedrive (password: groupji)</a>或者<a href="https://pan.baidu.com/s/1LhXblEic6pYf1i6hOm6Otw">百度网盘(密码10t3)</a>下载预训练模型，并将其放到<code>./output</code>文件夹内。</p><h2 id="2-2-环境准备">2.2 环境准备</h2><p>要求Ubuntu 18.04/20.04, CUDA 10.1/10.2/11.6, python &gt;= 3.7, PyTorch &gt;= 1.9, torchvision</p><p><strong>（1）创建虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gdrnpp python=3.7</span><br><span class="line">conda activate grdnpp</span><br></pre></td></tr></table></figure><p><strong>（2）安装依赖</strong></p><p>打开<code>requirements/requirement.txt</code>，修改第48行为<code>pytorch-lightning==1.6.0</code></p><p>运行<code>sh scripts/install_deps.sh</code></p><p><strong>（3）从<a href="https://github.com/facebookresearch/detectron2">源码</a>安装detectron2</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install &#x27;git+https://github.com/facebookresearch/detectron2.git&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">(add --user <span class="keyword">if</span> you don<span class="string">&#x27;t have permission)</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Or, to install it from a local clone:</span></span></span><br><span class="line">git clone https://github.com/facebookresearch/detectron2.git</span><br><span class="line">python -m pip install -e detectron2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">On macOS, you may need to prepend the above commands with a few environment variables:</span></span></span><br><span class="line">CC=clang CXX=clang++ ARCHFLAGS=&quot;-arch x86_64&quot; python -m pip install ...</span><br></pre></td></tr></table></figure><p><strong>（4）编译 fps 的cpp扩展</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh core/csrc/compile.sh</span><br></pre></td></tr></table></figure><p><strong>（5）编译egl_renderer的cpp扩展</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh lib/egl_renderer/compile_cpp_egl_renderer.sh</span><br></pre></td></tr></table></figure><h2 id="2-3-目标检测算法">2.3 目标检测算法</h2><p>从 <a href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/liuxy21_mails_tsinghua_edu_cn/EkCTrRfHUZVEtD7eHwLkYSkBCTXlh9ekDteSzK6jM4oo-A?e=m0aNCy">Onedrive</a> (password: groupji) or <a href="https://pan.baidu.com/s/1AU7DGCmZWsH9VgQnbTRjow">BaiDuYunPan</a>(password: aw68)中下载预训练模型。</p><p><strong>（1）训练</strong></p><p><code>./det/yolox/tools/train_yolox.sh &lt;config_path&gt; &lt;gpu_ids&gt; (other args)</code></p><p><strong>（2）测试</strong></p><p><code>./det/yolox/tools/test_yolox.sh &lt;config_path&gt; &lt;gpu_ids&gt; &lt;ckpt_path&gt; (other args)</code></p><h2 id="2-4-位姿估计算法">2.4 位姿估计算法</h2><p><strong>（1）训练</strong></p><p>打开<code>core/gdrn_modeling/datasets/lm_pbr.py</code>，注释第190行<code>assert osp.exists(xyz_path), xyz_path</code></p><p><code>./core/gdrn_modeling/train_gdrn.sh &lt;config_path&gt; &lt;gpu_ids&gt; (other args)</code></p><p>例如：</p><p><code>./core/gdrn_modeling/train_gdrn.sh configs/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv.py 0</code></p><p><strong>（2）测试</strong></p><p><code>./core/gdrn_modeling/test_gdrn.sh &lt;config_path&gt; &lt;gpu_ids&gt; &lt;ckpt_path&gt; (other args)</code></p><p>例如：</p><p><code>./core/gdrn_modeling/test_gdrn.sh configs/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv.py 0 output/gdrn/ycbv/convnext_a6_AugCosyAAEGray_BG05_mlL1_DMask_amodalClipBox_classAware_ycbv/model_final_wo_optim.pth</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;一、GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;期刊 / 会议&lt;/stron</summary>
      
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="GDRNPP" scheme="https://www.mahaofei.com/tags/GDRNPP/"/>
    
  </entry>
  
  <entry>
    <title>TensorBoard的使用丨深度学习曲线生成</title>
    <link href="https://www.mahaofei.com/post/6db9da8f.html"/>
    <id>https://www.mahaofei.com/post/6db9da8f.html</id>
    <published>2023-03-23T14:14:20.000Z</published>
    <updated>2023-03-23T14:14:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1>TensorBoard的安装</h1><p>要求Pytorch版本必须在1.2.0以上。</p><p>使用下面的命令安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br></pre></td></tr></table></figure><h1>TensorBoard的代码调用</h1><p><strong>（1）导入包，并创建TensorBoard回调对象</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> TensorBoard</span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs/learning_rate_scheduler&quot;</span>) <span class="comment">#指定TensorBoard日志目录</span></span><br></pre></td></tr></table></figure><p><strong>（2）在模型的训练过程中导入回调</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">global_step = <span class="number">0</span> <span class="comment"># 初始化 global_step 为 0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># 训练过程</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将学习率和训练损失添加到 TensorBoard</span></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Loss&#x27;</span>, loss, global_step=global_step)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Learning_Rate&#x27;</span>, lr, global_step=global_step)</span><br><span class="line">global_step += <span class="number">1</span>  <span class="comment"># 为每个batch更新 global_step 计数器</span></span><br></pre></td></tr></table></figure><h1>查看曲线</h1><p>训练开始后，打开一个终端，输入下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir logs/learning_rate_scheduler</span><br></pre></td></tr></table></figure><p>然后打开浏览器的<a href="http://localhost:6006/">http://localhost:6006/</a>页面，就可以看到曲线。</p><p><img src="https://img.mahaofei.com/img/20230323221410.png" alt=""></p>]]></content>
    
    
    <summary type="html">在深度学习训练过程中，我们必定会需要观察系统的Loss、Learning_rate等参数的变化，因此实时绘制曲线图是十分有必要的。本文就介绍了如何利用Pytorch的TensorBoard绘制曲线图。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="深度学习基础" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】ECCV2020-2022 6D位姿估计相关论文</title>
    <link href="https://www.mahaofei.com/post/f0f72f50.html"/>
    <id>https://www.mahaofei.com/post/f0f72f50.html</id>
    <published>2023-03-21T11:20:13.000Z</published>
    <updated>2023-03-21T11:20:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1>ECCV2020</h1><h2 id="01-CosyPose-Consistent-multi-view-multi-object-6D-pose-estimation">01. CosyPose: Consistent multi-view multi-object 6D pose estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2020<br><strong>作者 / 机构</strong>：Yann Labbe,  Ecole normale superieure, CNRS, PSL Research University, Paris, France<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ylabbe/cosypose">https://github.com/ylabbe/cosypose</a></p></blockquote><h4 id="1-目标问题-16">1 目标问题</h4><p>在相机位置未知的情况下，利用多视角信息来提高物体姿态估计的准确性和鲁棒性</p><h4 id="2-方法-16">2 方法</h4><p><strong>（1）建立6D姿态初始候选对象</strong></p><p>给定一组具有已知3D模型的对象和场景的单个图像，我们为每个对象输出一组候选检测，并为每个检测输出对象相对于与图像相关联的相机的6D姿态。</p><p><strong>（2）对象候选匹配</strong></p><p>匹配多个视图中可见的对象，以获得单个一致的场景。</p><p><strong>（3）全局场景细化</strong></p><p>所有物体和相机的6D姿态都经过了优化，以最大限度地减少全局重投影误差。</p><h4 id="3-思考-16">3 思考</h4><p><strong>（1）创新点</strong></p><ul><li>提出了一种基于渲染和比较的单视角单物体6D姿态估计方法，用于生成每个图像中的物体姿态假设。</li><li>开发了一种基于RANSAC的鲁棒方法，用于匹配不同图像中的单个物体姿态假设，并利用这些对象级别的对应关系来恢复相机之间的相对位置。</li><li>开发了一种基于对象级别捆绑调整（object-level bundle adjustment）的全局优化方法，用于在所有视角下最小化重投影误差，并改善噪声单视角物体姿态。</li></ul><p><strong>（2）实用性</strong></p><p>从多个视图中推测物体的6D位姿，对于抓取场景实用性较差。</p><h1>ECCV2022</h1><h2 id="01-DProST-Dynamic-Projective-Spatial-Transformer-Network-for-6D-Pose-Estimation">01. DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：<br><strong>关键词</strong>：<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/parkjaewoo0611/DProST">https://github.com/parkjaewoo0611/DProST</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-17">1 目标问题</h4><p>提出了一种新的基于投影网格的姿态估计系统。</p><p><img src="https://img.mahaofei.com/img/20230402093533.png" alt=""></p><h4 id="2-方法-17">2 方法</h4><ul><li>使用深度神经网络从RGB图像中提取特征，并预测物体位置和大小。</li><li>在相机空间上根据预测位置和大小生成一个锥形光束网格，并将其反向变换到物体空间。</li><li>使用参考图像和掩码从物体模型或重建特征中提取纹理特征，并将其映射到变换后的网格上。</li><li>使用双线性插值从映射后的纹理特征中采样得到重建图像，并与输入图像进行比较。</li><li>使用基于网格距离和网格匹配损失函数来优化网络参数和姿态参数。</li></ul><h4 id="3-思考-17">3 思考</h4><p>深度神经网络提取特征，投影网格重建图像，使用损失函数优化参数。</p><h2 id="02-DCL-Net-Deep-Correspondence-Learning-Network-for6D-Pose-Estimation">02. DCL-Net: Deep Correspondence Learning Network for6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Hongyang Li, South China University of Technology, Guangzhou, China<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/Gorilla-Lab-SCUT/DCL-Net">https://github.com/Gorilla-Lab-SCUT/DCL-Net</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-18">1 目标问题</h4><p>从点对应关系中直接估计6D物体姿态，而不是使用间接的对应学习目标</p><h4 id="2-方法-18">2 方法</h4><p>这篇论文的主要方法是提出了一种新的深度对应学习网络（DCL-Net），它利用双重特征解耦和对齐（FDA）模块，在特征空间中建立相机坐标系和物体坐标系之间的部分到部分对应关系和完整到完整对应关系。具体步骤如下：</p><ul><li>首先，对于部分物体观测和其CAD模型，分别提取它们的点特征图；</li><li>然后，设计两个FDA模块，分别建立部分到部分对应关系和完整到完整对应关系。具体来说，每个FDA模块将两个点特征图作为输入，并将每个特征图解耦为独立的姿态特征图和匹配特征图；然后利用匹配特征图学习一个注意力图来建立深度对应关系；最后，根据注意力图将两个系统的姿态特征图和匹配特征图进行对齐和配对，得到姿态特征对和匹配特征对；</li><li>接着，将两个FDA模块得到的两组对应关系进行融合，因为它们具有互补优势；然后利用融合后的匹配特征对学习置信度得分来衡量深度对应关系的质量；同时利用置信度得分加权融合后的姿态特征对来直接回归物体姿态；</li><li>最后，提出了一个基于置信度的姿态优化网络来进一步迭代地提高姿态精度。</li></ul><h4 id="3-思考-18">3 思考</h4><p>点特征方法。</p><h2 id="03-Perspective-Flow-Aggregation-for-Data-Limited-6D-Object-Pose-Estimation">03. Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Yinlin Hu, EPFL CVLab, Lausanne, Switzerland<br><strong>关键词</strong>：位姿估计；少数据情况<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/cvlab-epfl/perspective-flow-aggregation">https://github.com/cvlab-epfl/perspective-flow-aggregation</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-19">1 目标问题</h4><p>在数据有限的情况下，使用合成图像或少量真实图像来训练一个6D物体姿态估计的模型</p><h4 id="2-方法-19">2 方法</h4><ul><li>首先，利用合成图像和真实图像（如果有的话）来训练一个基于深度学习的特征提取器，用于从输入图像中提取出与物体姿态相关的特征。</li><li>然后，利用合成图像和真实图像（如果有的话）来训练一个基于透视流（perspective flow）的模块，用于将输入图像中的特征点映射到目标物体模型上。透视流是指由于相机和物体之间相对运动而导致的特征点在不同视角下的位移。</li><li>最后，利用一种称为透视流聚合（perspective flow aggregation）的技术，将多个透视流进行融合，并通过最小二乘法求解出最优的6D物体姿态。</li></ul><h4 id="3-思考-19">3 思考</h4><p>投影透视方法。</p><h2 id="04-Learning-Based-Point-Cloud-Registration-for-6D-Object-Pose-Estimation-in-the-Real-World">04. Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World</h2><blockquote><p><strong>期刊 / 会议</strong>：ECCV2022<br><strong>作者 / 机构</strong>：Zheng Dang, CVLab, EPFL, Lausanne, Switzerland<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/AnsonYanxin/MatchNorm">https://github.com/AnsonYanxin/MatchNorm</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h4 id="1-目标问题-20">1 目标问题</h4><h4 id="2-方法-20">2 方法</h4><ul><li>首先，它提出了一种基于深度学习的点云匹配模块，用于从源点云和目标点云中提取特征，并计算两个点云之间的相似度矩阵。</li><li>然后，它提出了一种基于归一化的点云对齐模块，用于根据相似度矩阵找到最佳的刚性变换矩阵，使得源点云和目标点云之间的距离最小化</li></ul><h4 id="3-思考-20">3 思考</h4><p>代码不完全。</p>]]></content>
    
    
    <summary type="html">检索阅读近三年ECCV6D位姿估计相关论文并进行记录</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="ECCV" scheme="https://www.mahaofei.com/tags/ECCV/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】CVPR2020-2022 6D位姿估计相关论文</title>
    <link href="https://www.mahaofei.com/post/afed67af.html"/>
    <id>https://www.mahaofei.com/post/afed67af.html</id>
    <published>2023-03-15T01:29:33.000Z</published>
    <updated>2023-03-15T01:29:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1>CVPR 2020</h1><h2 id="01-HybridPose-6D-Object-Pose-Estimation-under-Hybrid-Representations">01. HybridPose: 6D Object Pose Estimation under Hybrid Representations</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Chen Song, The University of Texas at Austin<br><strong>关键词</strong>：位姿估计；混合特征<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/chensong1995/HybridPose">https://github.com/chensong1995/HybridPose</a></p></blockquote><h3 id="1-目标问题-3">1 目标问题</h3><p>6D位姿估计</p><h3 id="2-方法-3">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230315170643.png" alt=""></p><p>算法由中间特征预测网络和姿态回归网络组成：</p><p><strong>（1）预测模块</strong></p><p>将图像作为输入，用三个预测网络输出预测的关键点、边缘向量和对称对应关系</p><ul><li>关键点：利用PVNet的关键点预测方法</li><li>边缘向量：每两个关键点之间的向量</li><li>对称对应关系：扩展了FlowNet网络，结合了像素流和语义掩码</li></ul><p><strong>（2）姿态回归模块</strong></p><p>姿态回归网络：包括初始化子模块和优化子模块</p><ul><li>初始化子模块：使用中间特征回归初始姿态</li><li>优化子模块：使用GM鲁棒范数并优化，获得最终姿态</li></ul><h3 id="3-思考-3">3 思考</h3><p>方法比较直观，使用关键点、关键点向量和对称关系进行姿态预测。</p><p>但是实际应用比较困难，训练前需要使用FSP生成关键点标签、使用SymSeg生成对称性标签，并且还要提供分割模板。而且还需要PVNet的融合数据。</p><hr><h2 id="02-Single-Stage-6D-Object-Pose-Estimation">02. Single-Stage 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Yinlin Hu, CVLab, EPFL, Switzerland<br><strong>关键词</strong>：位姿估计；单阶段<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/cvlab-epfl/single-stage-pose">https://github.com/cvlab-epfl/single-stage-pose</a></p></blockquote><h3 id="1-目标问题-4">1 目标问题</h3><p>提出一种单阶段框架，解决两阶段框架（先建立3D对象关键点和2D图像的对应关系然后回归）的缺点，加快训练速度。</p><h3 id="2-方法-4">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316095154.png" alt=""></p><p>通过一些实例分割网络建立了3D物体和2D图像的关系后，使用三个主要模块来直接从这些对应聚类预测位姿：</p><ul><li><p>局部特征提取模块</p></li><li><p>特征聚合模块：在不同聚类中聚合特征</p></li><li><p>全局推理模块：有全连接层组成，用于将最终姿态估计为四元数和平移</p></li><li><p>提出了一种新颖的<strong>投影分布</strong>（Projection Distribution）表示法，将三维物体关键点在二维图像上的投影建模为一个概率分布，而不是一个确定的位置。</p></li><li><p>设计了一个<strong>单阶段6D姿态估计网络</strong>（Single-Stage 6D Pose Estimation Network），利用卷积神经网络和全连接层来预测每个物体关键点在图像上的投影分布参数。</p></li><li><p>采用了一种<strong>最大似然估计</strong>（Maximum Likelihood Estimation）方法，根据预测的投影分布参数和已知的三维物体模型来直接计算物体在相机坐标系下的旋转矩阵和平移向量。</p></li></ul><h3 id="3-思考-4">3 思考</h3><p>似乎需要与其它网络结合，从其他网络的中间层进行特征提取。</p><p>Github资料较少。</p><hr><h2 id="03-G2L-Net-Global-to-Local-Network-for-Real-time-6D-Pose-Estimation-with-Embedding-Vector-Features">03. G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2020<br><strong>作者 / 机构</strong>：Wei Chen, School of Computer Science, University of Birmingham<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/DC1991/G2L_Net">https://github.com/DC1991/G2L_Net</a></p></blockquote><h3 id="1-目标问题-5">1 目标问题</h3><p>提高位姿估计算法的准确度和速度。</p><h3 id="2-方法-5">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316101524.png" alt=""></p><p><strong>（1）全局定位</strong></p><p>使用2D检测器（例如yolo）来预测目标的边界框和标签，并将得到的概率图中的最大概率位置作为球体中心（结合深度图的3D坐标），来获得一个球体空间，减少后续3D搜索空间。</p><p><strong>（2）平移定位</strong></p><p>进行3D分割和平移残差预测，并将对象点的坐标系转换为局部规范坐标系。</p><p><strong>（3）旋转定位</strong></p><p>使用逐点嵌入向量特征提取器来提取嵌入向量特征，然后输入解码器回归出输入点云的旋转。</p><h3 id="3-思考-5">3 思考</h3><p>相当于将DenseFusion的实例分割先验步骤进行了替换，使用了yolo+点云分割来代替。最后的特征还是逐点特征。</p><h1>CVPR2021</h1><h2 id="01-GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation">01. GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Gu Wang,  Tsinghua University, BNRist<br><strong>关键词</strong>：位姿估计；端到端<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/THU-DA-6D-Pose-Group/GDR-Net">https://github.com/THU-DA-6D-Pose-Group/GDR-Net</a></p></blockquote><h3 id="1-目标问题-6">1 目标问题</h3><p>提出一种端到端的位姿估计算法。</p><h3 id="2-方法-6">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230316104020.png" alt=""></p><p><strong>（1）网络架构</strong></p><p>首先向GDR-Net提供256x256的ROI图，预测出三个64x64的中间特征图</p><ul><li>稠密对应图$M_{2D-3D}$：将稠密坐标映射$M_{XYZ}$对跌倒2D像素坐标上得到，反映了对象的几何形状信息。</li><li>表面区域注意图$M_{SRA}$：采用最远点采样从$M_{XYZ}$中到处，代表了对象的对称性。</li><li>可见对象掩码$M_{vis}$</li></ul><p>使用一个简单的2D卷积Patch Pnp模块直接从特征图中回归6D对象姿态。Patch PnP模块由三个卷积层组成，然后用两个全连接层用于扁平化特征，最后连个全连接层输出R6D旋转和tSITE平移。</p><h3 id="3-思考-6">3 思考</h3><p>本文专注于图像的特征提取和处理工作，实现从单一图片预测位姿的功能。方法不够直观。</p><h2 id="02-FS-Net-Fast-Shape-based-Network-for-Category-Level-6D-Object-Pose-Estimation-with-Decoupled-Rotation-Mechanism">02. FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2021<br><strong>作者 / 机构</strong>：Wei Chen, School of Computer Science, University of Birmingham<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/DC1991/FS_Net">https://github.com/DC1991/FS_Net</a></p></blockquote><h3 id="1-目标问题-7">1 目标问题</h3><p>解决以往类别级姿态特征提取效率低，精度和推理速度低的问题。</p><h3 id="2-方法-7">2 方法</h3><p>设计了一种具有3D图卷积的方向感知自动编码器，用于潜在特征提取。</p><p>提出解耦旋转机制，利用两个解码器互补的访问旋转信息。</p><p>使用两个残差来估计平移。</p><p>提出一种在线box-cage的三维变形机制来增强训练数据。</p><p><img src="https://img.mahaofei.com/img/20230316151555.png" alt=""></p><ol><li>输入RGB图像。</li><li>使用yolov3检测对象的2D位置、类别标签、类概率图，并将最大概率的位置作为3D球体的中心。从而得到目标3D球体点云。</li><li>使用三维变形机制进行数据扩充。</li><li>使用基于形状的3DGC自动编码器来进行点云分割，用于旋转的潜在特征学习。<br>3DGC由m个单位向量组成，卷积值是核向量和n个最近向量之间的余弦相似度之和。</li><li>从潜在特征中将旋转信息解码为两个垂直向量。</li><li>利用残差估计网络预测平移。</li></ol><h3 id="3-思考-7">3 思考</h3><p>提出的使用三维变形机制进行数据扩充很有意思，或许后续很多方法都可以加上这个步骤，使得算法更具有鲁棒性。</p><p>需要训练yolo模型和FS_Net模型。</p><p><strong>NOCS数据集</strong></p><h1>CVPR2022</h1><h2 id="01-OVE6D-Object-Viewpoint-Encoding-for-Depth-based-6D-Object-Pose-Estimation">01. OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Dingding Cai, Tampere University<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/dingdingcai/OVE6D-pose">https://github.com/dingdingcai/OVE6D-pose</a></p></blockquote><h3 id="1-目标问题-8">1 目标问题</h3><p>已知物体的分割掩码，物体的三维mesh模型，预测从物体坐标系到相机坐标系的变换R+T。</p><h3 id="2-方法-8">2 方法</h3><p><strong>（1）训练阶段</strong></p><p>使用ShapeNet中的3D物体模型来训练OVE6D模型，这个阶段只进行一次，得到的模型参数在后续保持固定。</p><p><strong>（2）编码阶段</strong></p><p>将目标物体的3D网络模型转换为view points编码本，这个阶段对每个物体只进行一次。（view points编码本是一个特征向量的集合）</p><p><strong>（3）推理阶段</strong></p><p>从输入的物体深度图像和物体分割掩码中推理物体的6D姿态</p><ol><li>视角估计：将输入图像和物体ID作为输入，通过与view points编码本中的特征向量进行余弦相似度匹配找到最接近的预定义视角，并输出索引和置信度。</li><li>平面旋转估计：输入图像、ID、预定视角索引、置信度，通过卷积神经网络回归出相对于相机坐标系的旋转。</li><li>平移估计：输入图像、ID、预定义视角索引、置信度、平面旋转角度，通过另一个卷积神经网络输出物体的3D位置。</li></ol><h3 id="3-思考-8">3 思考</h3><p>算法需要预先训练好ShapeNet，然后确定一个view points编码本，过程较复杂不够简洁直观。</p><h2 id="02-OnePose-One-Shot-Object-Pose-Estimation-without-CAD-Models">02. OnePose: One-Shot Object Pose Estimation without CAD Models</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Jiaming Sun, Zhejiang University<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/zju3dv/OnePose">https://github.com/zju3dv/OnePose</a></p></blockquote><h3 id="1-目标问题-9">1 目标问题</h3><p>实现不依赖于CAD模型的位姿估计</p><h3 id="2-方法-9">2 方法</h3><p>借鉴了视觉定位的思想，只需要一个简单的RGB视频扫描物体，就可以构建一个稀疏的SfM模型。然后，利用一个通用的特征匹配网络将这个模型与新的查询图像对齐，从而得到物体姿态。</p><p>提出了一种新的图注意力网络（GATs），可以将同一个SfM点对应的2D特征聚合成3D特征，并与查询图像中的2D特征进行自注意力和交叉注意力匹配。</p><p><img src="https://img.mahaofei.com/img/20230316201824.png" alt=""></p><ol><li>对于每一个物体，使用视频扫描得到一组相机姿态以及物体的3D边界框。</li><li>利用SFM重建一个稀疏的点云模型</li><li>SfM的2D-3D对应关系被建立起来</li><li>使用注意力聚合层，将2D描述符聚合到3D描述符</li><li>通过PnP回归计算出物体位姿</li></ol><p><strong>总体实现流程如下</strong></p><ul><li>使用一些AR工具捕获物体数据，包括物体中心位置，尺寸，绕Z州的旋转角，相机姿态等。</li><li>从捕获的视频中提取图像，使用SfM重建稀疏点云，所有的对应图中提取2D关键点和描述符。</li><li>定位时，实时捕获一系列图像，提取2D关键点和描述符进行匹配，从数据库中查询候选图像，从而找到相机姿态。</li></ul><h3 id="3-思考-9">3 思考</h3><p>大概就是创建一个数据库，包括2D图像和重建出的点云，以及相应的2D-3D关键点和描述符，对每一个输入图像提取特征后进行匹配查询。</p><h2 id="03-Focal-Length-and-Object-Pose-Estimation-via-Render-and-Compare">03. Focal Length and Object Pose Estimation via Render and Compare</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Georgy Ponimatkin, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://ponimatkin.github.io/focalpose">https://ponimatkin.github.io/focalpose</a></p></blockquote><h3 id="1-目标问题-10">1 目标问题</h3><p>估计相机参数未知的照片中物体的6D姿态</p><h3 id="2-方法-10">2 方法</h3><ol><li>从一个3D模型库中选择一个与输入图像中物体最匹配的3D模型。</li><li>用一个CNN编码器将输入图像编码成一个特征向量。</li><li>用一个CNN解码器将特征向量解码成一个初始的6D姿态和焦距。</li><li>用渲染引擎根据初始的6D姿态和焦距渲染出一个虚拟视图，并与输入图像进行比较。</li><li>用一个损失函数计算虚拟视图和输入图像之间的差异，并反向传播更新6D姿态和焦距。</li><li>重复步骤4和5直到收敛或达到最大迭代次数。</li></ol><h3 id="3-思考-10">3 思考</h3><p>对于网络图像中物体的位姿估计，不知道实际应用场景是什么。</p><h2 id="04-ES6D-A-Computation-Efficient-and-Symmetry-Aware-6D-Pose-Regression-Framework">04. ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Ningkai Mo, ShenZhen Key Lab of Computer Vision and Pattern Recognition<br><strong>关键词</strong>：位姿估计；对称<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/GANWANSHUI/ES6D">https://github.com/GANWANSHUI/ES6D</a></p></blockquote><h3 id="1-目标问题-11">1 目标问题</h3><p>主要解决如何利用RGB-D数据来估计刚体物体的6D姿态，特别是对于具有对称性的物体</p><h3 id="2-方法-11">2 方法</h3><ul><li>设计了一个全卷积的特征提取网络，叫做XYZNet，它可以高效地从RGB和深度图中提取点云特征，并将不同模态的特征融合起来。</li><li>提出了一种新的形状表示方法，叫做分组基元（GP），它只与物体的对称性有关，而忽略了形状的细节。</li><li>基于GP，设计了一种新的姿态距离度量，叫做平均（最大）分组基元距离，或者A(M)GPD。这种度量可以作为损失函数来训练回归网络，并保证网络收敛到正确的姿态。</li></ul><p><img src="https://img.mahaofei.com/img/20230316211632.png" alt=""></p><ol><li>从RGB-D图像生成RGB-XYZ数据。RGB-XYZ数据被馈送到CNN模块以提取局部特征，该局部特征对颜色和几何信息进行编码</li><li>点云特征是通过类似PointNet的CNN模块获得的，并填充到与局部特征相同的大小</li><li>将局部特征和点云特征连接为用于姿态估计的逐点特征</li><li>选择具有最大置信度的姿势作为最终结果</li></ol><h3 id="3-思考-11">3 思考</h3><p>同样是逐点特征，这篇论文提出了XYZNet，可以更高效的提取提取点云和RGB特征，不需要提供掩码图。</p><p>代码只提供T-LESS数据集方法。</p><h2 id="05-GPV-Pose-Category-level-Object-Pose-Estimation-via-Geometry-guided-Point-wise-Voting">05.  GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：YanDi, Technical University of Munich<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/lolrudy/GPV_Pose">https://github.com/lolrudy/GPV_Pose</a></p></blockquote><h3 id="1-目标问题-12">1 目标问题</h3><p>主要解决了现有方法在处理未见过的物体实例时存在的不确定性和不稳定性的问题</p><h3 id="2-方法-12">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230317093148.png" alt=""></p><ol><li>给定一副RGB-D图像，先使用如Maks-RCNN等方法将物体从深度图中分割出来。</li><li>然后从深度三维点云中抽取1028个点，并将它们输入到GPV-Pose位姿估计网络中。</li><li>由于3DGC对于点云的移动和缩放不敏感，所以以3DGC为主干提取全局和每个点的特征，凭借附加了三个并行分支，用于姿态预测，对称性，和逐点包围盒。</li></ol><p>注：</p><ul><li>3DGC方法首先将输入点云转换为一个k近邻图（kNN graph），其中每个点与其最近的k个邻居相连。使用多层图卷积（Graph Convolution）来提取每个点的局部特征，并使用最大池化（Max Pooling）来提取全局特征。3DGC方法将全局和逐点特征拼接起来，形成一个混合特征向量，用于后续的姿态估计、对称感知重建和点投票模块。</li></ul><h3 id="3-思考-12">3 思考</h3><p><strong>（1）创新点</strong></p><ul><li>引入了一种解耦的置信度驱动旋转表示，允许几何感知恢复相关旋转矩阵</li><li>提出了一种基于点投票的位移估计模块，利用几何约束来生成可靠和精确的位移预测</li><li>在一个端到端可训练的网络中整合这两个模块，并通过一个多任务损失函数进行优化</li></ul><p><strong>（2）与DenseFusion相比</strong></p><p>它们都使用了3D图卷积网络（3DGC）来从输入点云中提取每个点的局部特征，并将其与全局特征拼接起来，形成一个混合特征向量。</p><p>GPV-Pose使用了一种解耦的置信度驱动的旋转表示，可以通过几何关系恢复旋转矩阵，而DenseFusion使用了一种直接预测四元数的方法。</p><p><strong>使用NOCS数据集。</strong></p><h2 id="06-DGECN-A-Depth-Guided-Edge-Convolutional-Network-for-End-to-End-6D-Pose-Estimation">06. DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Tuo Cao, School of Computer Science, Wuhan University, Wuhan, Hubei, China<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/maplect/DGECN_CVPR2022">https://github.com/maplect/DGECN_CVPR2022</a></p></blockquote><h3 id="1-目标问题-13">1 目标问题</h3><p>从单目RGB图像中进行位姿估计。</p><h3 id="2-方法-13">2 方法</h3><p><strong>（1）深度细化网络DRN</strong></p><p>两个不同的深度估计网络分别输出深度图DA和DB，计算两个深度图之间的差异，并将差异超过阈值的区域定义为不确定区域。</p><p><strong>（2）特征提取</strong></p><ul><li>深度估计：将彩色图像作为输入，并执行深度图预测</li><li>对象分割：利用分割的掩码，将深度图转换为3D点云，并利用3D特征提取器来提取几何特征</li></ul><p><strong>（3）2D关键点定位</strong></p><p>采用最远点采样（FPS）算法来选择物体表面上的关键点。</p><p><strong>（4）从2D-3D对应关系学习6D位姿</strong></p><p>使用动态图PnP（DG-PnP）算法，通过边缘卷积构建一个图结构，利用2D-3D对应关系中的拓扑信息来直接学习6D姿态</p><h3 id="3-思考-13">3 思考</h3><p><strong>（1）创新点</strong></p><ul><li>用一个深度引导网络同时预测分割和深度图，并用一个深度优化网络（DRN）提高深度图的质量</li><li>根据分割和深度图建立2D-3D对应关系，即将图像上的关键点与3D模型上的点匹配</li><li>提出一个动态图PnP（DG-PnP）算法，通过边缘卷积构建一个图结构，利用2D-3D对应关系中的拓扑信息来直接学习6D姿态</li></ul><p><strong>（2）实用性</strong></p><p>从单目RGB图像进行位姿估计，通过网络回归深度图。</p><h2 id="07-Templates-for-3D-Object-Pose-Estimation-Revisited-Generalization-to-New-Objects-and-Robustness-to-Occlusions">07. Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions)</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Van Nguyen Nguyen, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France<br><strong>关键词</strong>：位姿估计<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/nv-nguyen/template-pose">https://github.com/nv-nguyen/template-pose</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h3 id="1-目标问题-14">1 目标问题</h3><p>提出了一种方法，只需要物体的CAD模型，就可以将输入对象匹配到一组模板，即使在部分遮挡的情况下也可以估计3D姿态。</p><h3 id="2-方法-14">2 方法</h3><p><img src="https://img.mahaofei.com/img/20230322150858.png" alt=""></p><p>训练时，使用由真实图像和合成模板组成的对，来计算局部特征，预测两幅图像的相似性。</p><p>然后对于未看到的图像，计算其局部特征，将图像与模板数据库匹配来检索对象姿态。</p><h3 id="3-思考-14">3 思考</h3><p>编码本思想，实用性较强，可尝试。</p><h2 id="08-Coupled-Iterative-Refinement-for-6D-Multi-Object-Pose-Estimation">08. Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</h2><blockquote><p><strong>期刊 / 会议</strong>：CVPR2022<br><strong>作者 / 机构</strong>：Lahav Lipson, Princeton University<br><strong>关键词</strong>：位姿估计；迭代优化<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">https://github.com/princeton-vl/Coupled-Iterative-Refinement</a><br><strong>数据集</strong>：LINEMOD</p></blockquote><h3 id="1-目标问题-15">1 目标问题</h3><p>给定一组已知的RGBD输入，检测每个对象的6D姿态。</p><h3 id="2-方法-15">2 方法</h3><p>算法复杂，迭代优化方法。</p><h3 id="3-思考-15">3 思考</h3><p>代码效果最好，但是代码较为复杂。</p>]]></content>
    
    
    <summary type="html">检索阅读近三年CVPR6D位姿估计相关论文并进行记录</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
</feed>
