<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>马浩飞丨博客</title>
  
  <subtitle>进步无止境</subtitle>
  <link href="https://www.mahaofei.com/atom.xml" rel="self"/>
  
  <link href="https://www.mahaofei.com/"/>
  <updated>2023-11-21T06:00:57.000Z</updated>
  <id>https://www.mahaofei.com/</id>
  
  <author>
    <name>马浩飞</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【论文笔记】Dex 演示引导强化学习与手术机器人任务自动化的高效探索</title>
    <link href="https://www.mahaofei.com/post/90275938.html"/>
    <id>https://www.mahaofei.com/post/90275938.html</id>
    <published>2023-11-21T06:00:57.000Z</published>
    <updated>2023-11-21T06:00:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>CODE<br>BASE_URL<br><a href="https://api.openai-hk.com/v1/chat/completions">https://api.openai-hk.com/v1/chat/completions</a><br>OPENAI_API_KEY<br>hk-db35zr10000056242d7928372b81ceca4acf8680ac747a3f</p><h1>论文笔记</h1><blockquote><p><strong>标题</strong>：Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot<br><strong>标题</strong>：演示引导强化学习与手术机器人任务自动化的高效探索<br><strong>作者团队</strong>：香港中文大学（刘云辉团队）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/med-air/DEX">https://github.com/med-air/DEX</a></p></blockquote><h2 id="4-1-目标问题">4.1 目标问题</h2><p>虽然基于强化学习的方法为手术自动化提供了可能的方案，但是通常需要大量收集数据才能进行学习。因此本文目的是提高从演示中探索学习的效率，有效地利用专家演示数据。</p><p>具体而言，目前的问题如下：</p><ul><li>使用强化学习，如果不给出演示数据而仅通过探索学习，需要收集大量的数据来解决任务；</li><li>使用演示数据的方法，例如赋予演示数据相对于机器人探索数据更高的优先级，效率仍然低下，设置额外奖励函数的方法不仅只能针对特定环境，且容易引起局部最优；</li><li>使用 actor-critic 框架，通过正则化 actor 损失来衡量机器人与专家之间的行为差异，但是这种方式效率较低（尤其在初期机器人与演示差距较大情况下），且没有考虑 critic 的正则化，容易导致高估问题。</li></ul><p>本文贡献：</p><ul><li>提出一种 actor-critic 框架，降低 critic 的高估问题，提高强化学习过程中类似专家的行动进行探索。</li><li>使用非参数引导传播，实现未观测状态的探索</li><li>在 SurRoL 手术机器人上实验验证，效果优秀，同时部署在 dVRK 上，同样表示出强大的潜力。</li></ul><blockquote><p>dVRK(da Vinci Research Kit，达芬奇手术机器人系统)</p></blockquote><h2 id="4-2-方法-2">4.2 方法</h2><p>DEX(Demonstration-guided EXploration)，演示引导探索。</p><p><strong>（0）问题定义</strong></p><p>将手术机器人动作学习考虑为一个 off-policy 的智能体，在由马尔可夫决策过程构建的环境中进行交互。</p><blockquote><p>off-policy，指智能体不使用当前的策略来决定行动，而是使用不同的策略来生成行为数据，从过去的经历中学到最优的行为决策方法。</p></blockquote><p>在 $t$ 时刻，机器人根据当前状态 $s_t$ 以及确定性策略 $\pi$ 执行行动，环境用 $r_t=r(s_t,a_t)$ 奖励智能体，然后状态转移 $s_{t+1}$。</p><p>循环此过程，每次智能体将经验 $(s+t,a_t,r_t,s_{t+1})$ 存入重放缓冲区 $D_A$。</p><p>同时设置一个演示缓冲区 $D_E$，用于存放专家策略 $\pi$ 经验。</p><p><img src="https://img.mahaofei.com/img/202311151037433.png" alt="image.png"></p><p>如图，该方法由两部分组成：</p><ul><li>基于 actor-critic 的策略学习模块（右下角），用于从演示数据中指导探索；</li><li>基于最近邻匹配和局部加权回归的非参数模块（左上角），用于将与当前状态相差过大的演示传播到为当前状态。</li></ul><p><strong>（1）专家引导的 actor-critic 框架</strong></p><p>现有的 actor-critic 方法通过最大化预期回报来学习最优策略，但是如果 Q 值估计不准确，会阻碍探索。本文通过利用智能体和专家策略之间的动作差距来增强环境奖励。</p><p>$$<br>\max_{\pi}\mathbb{E}<em>{\pi}\left[\sum</em>{t=0}^{\infty}\gamma^{t}(r_{t}-\alpha d(a_{t},a_{t}^{e}))\right],a_{t}^{e}:=\pi^{e}(s_{t}),<br>$$</p><p>其中 $\alpha$ 是探索系数，$d()$ 衡量智能体动作和专家动作之间的相似性距离度量。</p><p>基于此奖励，本文设计了正则化 Q 函数（critic），并最小化动作价值和状态价值的差距。</p><p><strong>（2）有限演示情况下的引导的传播</strong></p><p>智能体在初始学习阶段很容易探索演示未覆盖的区域，无法实现监督 actor 探索。</p><p>常规的解决思路有行为克隆，但是当状态相差较大时，策略与专家行动仍会有较大的不同。因此本文使用非参数回归模型，从有限的演示中将经验传播实现更稳定的引导。</p><p>首先从演示缓冲区采样一小批状态和动作，然后给定一个当前状态，在一小批状态中搜索，利用 k 近邻方法找到最接近的状态，然后使用指数和函数的局部加权回归方法近似专家策略。</p><p>$$<br>\hat{\pi}^e(s)=\frac{\sum_{i=1}^k\exp\left(-|s-s^{(i)}|<em>2\right)\cdot a^{(i)}}{\sum</em>{i=1}^k\exp\left(-|s-s^{(i)}|_2)\right)}.<br>$$</p><h1>算法复现</h1><h2 id="环境配置">环境配置</h2><p>clone代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/med-air/DEX.git</span><br><span class="line">cd DEX</span><br></pre></td></tr></table></figure><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n dex python=3.8</span><br><span class="line">conda activate dex</span><br></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -e SurRoL/# install surrol environments</span><br><span class="line">pip3 install -r requirements.txt</span><br><span class="line">pip3 install -e .</span><br></pre></td></tr></table></figure><p>在虚拟环境的<code>gym/envs/__init__.py</code>的第一行注册SurRoL任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">directory: anaconda3/envs/dex/lib/python3.8/site-packages/gym/envs/__init__.py</span></span><br><span class="line">import surrol.gym</span><br></pre></td></tr></table></figure><h2 id="数据采集">数据采集</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir SurRoL/surrol/data/demo</span><br><span class="line">python SurRoL/surrol/data/data_generation.py --env NeedlePick-v0 </span><br></pre></td></tr></table></figure><h2 id="训练">训练</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 train.py task=NeedlePick-v0 agent=dex use_wb=True</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">香港中文大学刘云辉团队基于DDPG+BC改进的手术机器人强化学习方法</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>强化学习Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/b7eed305.html"/>
    <id>https://www.mahaofei.com/post/b7eed305.html</id>
    <published>2023-11-14T02:15:37.000Z</published>
    <updated>2023-11-14T02:15:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 环境搭建问题</h1><h2 id="1-1-mujoco-相关">1.1 mujoco 相关</h2><p><strong>（1）mujoco-py 安装后编译错误</strong></p><p>问题详情：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">performance hint: /home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/mujoco_py/cymj.pyx:67:5: Exception check on &#x27;c_warning_callback&#x27; will always require the GIL to be acquired.</span><br><span class="line">Possible solutions:</span><br><span class="line">1. Declare the function as &#x27;noexcept&#x27; if you control the definition and you&#x27;re sure you don&#x27;t want the function to raise exceptions.</span><br><span class="line">2. Use an &#x27;int&#x27; return type on the function to allow an error code to be returned.</span><br><span class="line">performance hint: /home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/mujoco_py/cymj.pyx:104:5: Exception check on &#x27;c_error_callback&#x27; will always require the GIL to be acquired.</span><br><span class="line">Possible solutions:</span><br><span class="line">1. Declare the function as &#x27;noexcept&#x27; if you control the definition and you&#x27;re sure you don&#x27;t want the function to raise exceptions.</span><br><span class="line">2. Use an &#x27;int&#x27; return type on the function to allow an error code to be returned.</span><br><span class="line"></span><br><span class="line">Error compiling Cython file:</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">...</span><br><span class="line">    See c_warning_callback, which is the C wrapper to the user defined function</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    global py_warning_callback</span><br><span class="line">    global mju_user_warning</span><br><span class="line">    py_warning_callback = warn</span><br><span class="line">    mju_user_warning = c_warning_callback</span><br><span class="line">                       ^</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">/home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/mujoco_py/cymj.pyx:92:23: Cannot assign type &#x27;void (const char *) except * nogil&#x27; to &#x27;void (*)(const char *) noexcept nogil&#x27;. Exception values are incompatible. Suggest adding &#x27;noexcept&#x27; to type &#x27;void (const char *) except * nogil&#x27;.</span><br><span class="line"></span><br><span class="line">Error compiling Cython file:</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">...</span><br><span class="line">    See c_warning_callback, which is the C wrapper to the user defined function</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    global py_error_callback</span><br><span class="line">    global mju_user_error</span><br><span class="line">    py_error_callback = err_callback</span><br><span class="line">    mju_user_error = c_error_callback</span><br><span class="line">                     ^</span><br><span class="line">------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>解决方法：</p><blockquote><p>参考：<a href="https://stackoverflow.com/questions/76985054/import-mujoco-py-is-giving-me-compiling-errors">stackoverflow</a> 与 <a href="https://github.com/openai/mujoco-py/issues/773#issuecomment-1639684035">github issue</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install &quot;cython&lt;3&quot;</span><br></pre></td></tr></table></figure><p><strong>（2）ERROR: GLEW initalization error: Missing GL version</strong></p><p>问题详情：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/home/mahaofei/anaconda3/envs/reskill_new/lib/python3.7/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=100) is deprecated. Use register(max_episode_steps=100) instead.</span><br><span class="line">  warnings.warn(&quot;register(timestep_limit=&#123;&#125;) is deprecated. Use register(max_episode_steps=&#123;&#125;) instead.&quot;.format(timestep_limit, timestep_limit))</span><br><span class="line">Creating window glfw</span><br><span class="line">ERROR: GLEW initalization error: Missing GL version</span><br><span class="line"></span><br><span class="line">Press Enter to exit ...Killed</span><br></pre></td></tr></table></figure><p>解决方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">强化学习环境搭建，项目运行过程中可能遇到的bug与解决方法记录</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】基于强化学习的机器人动作模仿</title>
    <link href="https://www.mahaofei.com/post/6fa4482c.html"/>
    <id>https://www.mahaofei.com/post/6fa4482c.html</id>
    <published>2023-11-09T07:36:15.000Z</published>
    <updated>2023-11-09T07:36:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 Reinforcement Learning with Videos: Combining Offline Observations with Interaction</h1><blockquote><p><strong>标题</strong>：视频强化学习：将离线观察与互动相结合<br><strong>作者团队</strong>：宾夕法尼亚大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/kschmeckpeper/rl_with_videos">https://github.com/kschmeckpeper/rl_with_videos</a></p></blockquote><h2 id="1-1-目标问题-6">1.1 目标问题</h2><p>应用强化学习使机器人学习技能，通常需要大量的机器人在线数据，但是机器人的数据收集非常麻烦困难，难以获得足够多的数据。</p><p>人类视频广泛且多样，因此考虑从人类经验中进行强化学习。但因为人类视频没有动作的标注，并且人类视频和机器人相机图像，具有巨大的图像差异和视角差异。具体问题如下：</p><ul><li>机器人必须能通过观察来更新策略，不需要任何的行动或者奖励；</li><li>人手与末端执行器视觉差异较大，自由度也不同，因此需要考虑动作空间、形态、视角、环境差异带来的变化；</li></ul><p>为了解决这些问题，本文提出了视频强化学习框架（Reinforcement Learning with Videos，RLV），使用人类数据经验和机器人数据学习策略和价值函数。</p><h2 id="1-2-方法-5">1.2 方法</h2><p><strong>（1）问题定义</strong></p><p>该论文将问题公式化为马尔可夫决策过程 MDP，定义成元组 $(S_{int},A_{int},P,R)$，其中 $S_{int}$ 是状态空间，$A_{int}$ 是动作空间，$P$ 是环境的动力学，$R$ 是奖励函数。</p><p>机器人首先被提供了人类的观测 ${(s_{obs},s_{int}')_{1:t}}$，这些观察被建模成另一组马尔可夫决策链，其具有不同的状态和动作空间，但是两者的动力学和奖励函数是相同的。</p><p><strong>（2）方法概述</strong></p><p>该论文所提出的方法如下图所示，包含两个重放池，一个是无动作的观测数据 $(s_{obs},s_{obs}‘)\in D_{obj}$，另一个是包含动作条件的交互数据 $(s_{int},a_{int},s_{int}’,r_{int})\in D_{int}$，交互数据在训练期间会更新，而观测数据仅仅是初始的观测数据集。</p><blockquote><p><strong>重放池 (reply pool)</strong>：存储了智能体过去经历过的（状态，动作，奖励，新状态）的数据结构，通过采样这个池中的数据进行训练，可以从过去的经验中学习更多的规律，提高决策能力。</p></blockquote><p><img src="https://img.mahaofei.com/img/202311091616113.png" alt="image.png"></p><ul><li>左图：从动作条件重放池中采样数据 $(s_{int},a_{int},s_{int}‘,r_{int})$，将观测状态分别编码成特征 $h_{int},h_{int}’$，训练一个可逆的模型，来从特征中预测动作 $a_{int}$。</li><li>中图：将这个可逆模型用于根据观测状态特征 $h_{obs},h_{obs}'$，预测离线视频中的缺失的机器人动作 $\hat a_{int}$，将轨迹中最后一步设置为很大的奖励，前面其它步骤都设置为很小的奖励。</li><li>右图：使用 adversarial domain confusion(ADS)来对齐特征，最后使用离线策略强化学习算法，对于数据 $((h_{int},h_{obs}),(a_{int},\hat a_{int}),(h_{int}‘,h_{obs}’),(r_{int},\hat r_{obs}))$ 进行训练。</li></ul><blockquote><p><strong>Adversarial Domain Confusion (ADC)</strong>：通过最小化源域和目标域之间的特征分布距离来实现跨域的迁移学习。</p></blockquote><p><strong>（3）动作预测</strong></p><p>本文通过监督学习训练了一个参数为 $\theta$ 的逆模型，根据一对不变的特征编码 $(h,h’)$ 计算机器人动作。由于机器人与人类视频环境相同，我们应该能够预测任一马尔可夫决策过程的数据的操作。</p><p>损失使用预测动作 $\hat a_{int}=f_{inv}(h_{int},h_{int}';\theta)$ 和真实动作的均方误差 $a_{int}$：</p><p>$$<br>L_a(a_{int},h_{int},h_{int}‘,\theta)=||a_{int}-f_{inv}(h_{int},h_{int}’;\theta)||^2<br>$$</p><p>本文使用逆模型预测人手视频中的动作数据，并用它们来训练强化学习算法。</p><p><strong>（4）奖励生成</strong></p><p>由于强化学习使用观测数据的一个障碍就是缺乏奖励，虽然可以通过上面训练的逆模型预测奖励和动作，但实际上效果可能不会很好。</p><p>本文使用了替代方案，将观测数据轨迹的最后一个时间步长分配一个大的恒定奖励，之前的每一个时间步长分配一个小的恒定奖励。</p><p>这种方式目的是保证观测数据在轨迹结束时达到目标状态。至于其中的不准确之处，可以通过机器人收集的交互数据训练来消除。</p><p><strong>（5）域自适应</strong></p><p>要使用观测数据 $(s_{obs},s_{obs})'$，需要将其映射到一个不变的量 $h$。</p><p>为了实现这个目的，本文训练了一种特征编码器 $f_{enc}$，来从观测状态 $s$ 中学习编码表示 $h=f_{enc}(s;\psi)$，这种编码器应该包含所有相关的信息，并对与观测的域来说是不变的。</p><p>本文还训练了一个鉴别器，用于区分观测数据中提取的特征 $h_{obs}$ 和机器人交互数据中提取的特征 $h_{int}$。</p><p>将特征编码器和鉴别器使用对抗性学习方法进行训练，过程中编码器试图最小化鉴别器对编码特征的域的正确分类能力，鉴别器试图最大化分类能力。</p><p>最终获得的编码器就是我们需要的，将观测数据和机器人交互数据映射到不变量 $h$ 的编码器。</p><p><strong>（6）联合优化</strong></p><p>将领域自适应损失和逆模型的损失进行联合优化。</p><h1>2 Learning Generalizable Robotic Reward Functions from “In-The-Wild” Human Videos</h1><blockquote><p><strong>标题</strong>：从“野外”人类视频中学习可推广的机器人奖励函数<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：Robotics: Science and Systems (RSS)<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://sites.google.com/view/dvd-human-videos">https://sites.google.com/view/dvd-human-videos</a></p></blockquote><h2 id="2-1-目标问题-3">2.1 目标问题</h2><p>要实现通用型机器人完成各类任务，关键是机器人能够知道任务成功和奖励的能力，该奖励函数还必须能够在不同环境、任务、对象中推广。</p><p>由于收集大规模机器人交互数据是一件十分复杂困难的问题，而人类视频中则包含了大量的不同环境中的任务信息。</p><p>本文提出了一种不可知域视频鉴别器(Domain-agnostic Video Discriminator, DVD)，通过训练鉴别器来分类两个视频是否执行相同的任务学习多任务奖励函数。并通过少量的机器人训练数据学习人类视频的广泛数据集进行推广。</p><p>要解决的问题：</p><ul><li>人类的 wild data 和机器人的观测空间有着巨大的域变换，不管是 agent 的形态、还是场景的外观。</li><li>人类的动作空间和机器人的动作空间不同，可能不能很好的实现动作的映射</li><li>人类视频很多情况下是低质量的、有噪声的，还有着复杂的背景或视角</li></ul><p>解决思路：</p><ul><li>训练一个分类器预测两个视频是否完成的是同一个任务，也就是不可知域视频鉴别器（DVD）</li><li>训练完成后，DVD 能够将人类视频作为演示，机器人的行为作为另一个视频，输出一个分数，衡量任务成功的奖励。</li></ul><h2 id="2-2-方法-3">2.2 方法</h2><p><strong>（1）Domain-Agnostic Video Discriminators</strong></p><ul><li>一个预训练视频编码器将视频 $d_i$ 编码为特征 $h_i$</li><li>一个全连接神经网络，预测两个视频是否完成同样的任务<ul><li>损失函数设置见原文</li><li>奖励函数通过训练分类器来获得</li></ul></li></ul><p>本文的关键是训练一个分类器来学习 $R_\theta$，该分类器两个视频作为输入，判断两个视频是否属于同一个任务。视频可以来自于人类数据集或机器人数据集。</p><p>首先对视频进行采样，设两个视频为 $d_i$ 和 $d_j$，采样一批视频 $(d_i,d_i’,d_j)$ 其中 $d_i$ 和 $d_i’$ 是完成相同的任务，$d_j$ 是完成不同的个任务，最小化平均交叉熵损失训练 $R_\theta$，损失函数见原文，最终得到奖励函数如下：</p><p>$$<br>R_\theta(d_i,d_j)=f_{sin}(f_{enc}(d_i),f_{enc}(d_j);\theta)<br>$$</p><p>其中 $h=f_{enc}$ 是一个预训练的视频编码器，$f_{sin}(h_i,h_j;\theta)$ 是一个参数为 $\theta$ 的全连接神经网络，用来预测两个视频编码特征 $h_i,h_j$ 是否完成同样的任务。</p><p><strong>（2）使用 DVD 执行任务</strong></p><p>使用视觉模型预测控制(Visual Model Predictive Control, VMPC)实现。</p><p><img src="https://img.mahaofei.com/img/202311101115799.png" alt="image.png"></p><ol><li>使用 SV2P 模型训练动作条件视频预测模型 $\rho$</li><li>使用交叉熵和该动作模型 $\rho$ 选择与人类演示最相似的动作<ol><li>对输入图像，从动作分布中采样多个动作序列，并使用动作模型 $\rho$ 预测相应的未来轨迹</li><li>将每个预测轨迹和人类演示视频，输入DVD，得到任务相似性分数</li><li>执行与演示图象具有最高相似性的动作轨迹</li></ol></li></ol><h1>3 PLAS: Latent Action Space for Offline Reinforcement Learning</h1><blockquote><p><strong>标题</strong>：PLAS：离线强化学习的潜在行动空间<br><strong>作者团队</strong>：卡耐基梅隆大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2021<br><strong>代码</strong>： <a href="https://github.com/sfujim/BCQ">https://github.com/sfujim/BCQ</a></p></blockquote><h2 id="3-1-目标问题-2">3.1 目标问题</h2><p>离线强化学习可以从固定的数据集中学习策略。</p><p>在机器人中，数据收集十分麻烦且有一定的危险性，现有的方法从离线数据集中进行学习，性能十分受限。</p><p>本文提出了潜在动作空间中的策略(Policy in the Latent Action Space, PLAS)。</p><h2 id="3-2-方法-2">3.2 方法</h2><p><strong>（0）原理基础-离线 RL</strong></p><p>给定一个固定的离线数据集 $D={(s_t,a_t,r_t,s_{t+1})_i}$，难点在于该数据集没有覆盖马尔科夫决策过程 MDP 的整个状态空间和动作空间。</p><p>离线 RL 目的就是学习能使奖励最大化的策略，而策略受到我们对马尔可夫决策过程的了解，马尔可夫决策过程则是从有限的数据集中推理得到的。</p><p>但如果考虑离线 RL 的目标是最大化 MDP 在有限数据集下的累计回报，也能作为近似替代。并且在近似 Q 函数的时候会存在推理误差，</p><p><img src="https://img.mahaofei.com/img/202311122207187.png" alt="image.png"></p><p>给定一个状态，潜在策略输出一个潜在动作，使用解码器将其解码为动作空间输出。（可以添加扰动层来增加泛化能力）</p><p><strong>（1）潜在动作空间中的策略（Policy in Latent Action Space, PLAS）</strong></p><p>给定离线数据集，本文使用条件变分自动编码器（Conditional Variational Autoencoder, CVAE）对策略进行建模。为了使策略约束在数据集的范围内，考虑使用确定性策略，从状态映射到潜在动作，再用解码器得到实际动作。</p><p><strong>（2）泛化</strong></p><p>潜在策略再数据集范围内能够提供约束，但是在训练的时候，本文允许了从分布外的行为的发生，即添加了一个扰动层，设置了一个超参数限制扰动层的动作输出残差。</p><p>当然，如果数据集再状态-动作空间中有着非常高的覆盖率，那么这个扰动层就是不必要的。</p><h1>4 Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot</h1><blockquote><p><strong>标题</strong>：演示引导强化学习与手术机器人任务自动化的高效探索<br><strong>作者团队</strong>：香港中文大学（刘云辉团队）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/med-air/DEX">https://github.com/med-air/DEX</a></p></blockquote><h2 id="4-1-目标问题-2">4.1 目标问题</h2><p>虽然基于强化学习的方法为手术自动化提供了可能的方案，但是通常需要大量收集数据才能进行学习。因此本文目的是提高从演示中探索学习的效率，有效地利用专家演示数据。</p><p>具体而言，目前的问题如下：</p><ul><li>使用强化学习，如果不给出演示数据而仅通过探索学习，需要收集大量的数据来解决任务；</li><li>使用演示数据的方法，例如赋予演示数据相对于机器人探索数据更高的优先级，效率仍然低下，设置额外奖励函数的方法不仅只能针对特定环境，且容易引起局部最优；</li><li>使用 actor-critic 框架，通过正则化 actor 损失来衡量机器人与专家之间的行为差异，但是这种方式效率较低（尤其在初期机器人与演示差距较大情况下），且没有考虑 critic 的正则化，容易导致高估问题。</li></ul><p>本文贡献：</p><ul><li>提出一种 actor-critic 框架，降低 critic 的高估问题，提高强化学习过程中类似专家的行动进行探索。</li><li>使用非参数引导传播，实现未观测状态的探索</li><li>在 SurRoL 手术机器人上实验验证，效果优秀，同时部署在 dVRK 上，同样表示出强大的潜力。</li></ul><blockquote><p>dVRK(da Vinci Research Kit，达芬奇手术机器人系统)</p></blockquote><h2 id="4-2-方法-3">4.2 方法</h2><p>DEX(Demonstration-guided EXploration)，演示引导探索。</p><p><strong>（0）问题定义</strong></p><p>将手术机器人动作学习考虑为一个 off-policy 的智能体，在由马尔可夫决策过程构建的环境中进行交互。</p><blockquote><p>off-policy，指智能体不使用当前的策略来决定行动，而是使用不同的策略来生成行为数据，从过去的经历中学到最优的行为决策方法。</p></blockquote><p>在 $t$ 时刻，机器人根据当前状态 $s_t$ 以及确定性策略 $\pi$ 执行行动，环境用 $r_t=r(s_t,a_t)$ 奖励智能体，然后状态转移 $s_{t+1}$。</p><p>循环此过程，每次智能体将经验 $(s+t,a_t,r_t,s_{t+1})$ 存入重放缓冲区 $D_A$。</p><p>同时设置一个演示缓冲区 $D_E$，用于存放专家策略 $\pi$ 经验。</p><p><img src="https://img.mahaofei.com/img/202311151037433.png" alt="image.png"></p><p>如图，该方法由两部分组成：</p><ul><li>基于 actor-critic 的策略学习模块（右下角），用于从演示数据中指导探索；</li><li>基于最近邻匹配和局部加权回归的非参数模块（左上角），用于将与当前状态相差过大的演示传播到为当前状态。</li></ul><p><strong>（1）专家引导的 actor-critic 框架</strong></p><p>现有的 actor-critic 方法通过最大化预期回报来学习最优策略，但是如果 Q 值估计不准确，会阻碍探索。本文通过利用智能体和专家策略之间的动作差距来增强环境奖励。</p><p>$$<br>\max_{\pi}\mathbb{E}<em>{\pi}\left[\sum</em>{t=0}^{\infty}\gamma^{t}(r_{t}-\alpha d(a_{t},a_{t}^{e}))\right],a_{t}^{e}:=\pi^{e}(s_{t}),<br>$$</p><p>其中 $\alpha$ 是探索系数，$d()$ 衡量智能体动作和专家动作之间的相似性距离度量。</p><p>基于此奖励，本文设计了正则化 Q 函数（critic），并最小化动作价值和状态价值的差距。</p><p><strong>（2）有限演示情况下的引导的传播</strong></p><p>智能体在初始学习阶段很容易探索演示未覆盖的区域，无法实现监督 actor 探索。</p><p>常规的解决思路有行为克隆，但是当状态相差较大时，策略与专家行动仍会有较大的不同。因此本文使用非参数回归模型，从有限的演示中将经验传播实现更稳定的引导。</p><p>首先从演示缓冲区采样一小批状态和动作，然后给定一个当前状态，在一小批状态中搜索，利用 k 近邻方法找到最接近的状态，然后使用指数和函数的局部加权回归方法近似专家策略。</p><p>$$<br>\hat{\pi}^e(s)=\frac{\sum_{i=1}^k\exp\left(-|s-s^{(i)}|<em>2\right)\cdot a^{(i)}}{\sum</em>{i=1}^k\exp\left(-|s-s^{(i)}|_2)\right)}.<br>$$</p><h1>5 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics</h1><blockquote><p><strong>标题</strong>：剩余技能策略：学习基于技能的适应性行动空间，用于机器人强化学习<br><strong>作者团队</strong>：昆士兰科技大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://krishanrana.github.io/reskill">https://krishanrana.github.io/reskill</a></p></blockquote><h2 id="5-1-目标问题-2">5.1 目标问题</h2><p>基于技能的学习已经成为加速机器人学习的方法，技能从专家演示中提取，是短序列的单步操作（平移、抓取、抬起等动作），这些技能嵌入到潜在空间中，构成上层 RL 策略的行动空间。但是这种方式存在一些问题：</p><ul><li>对所有技能进行随机抽样探索，效率极低，因为其中只有一小部分技能与当前执行的任务相关，并且这些相关的技能通常不会聚集在技能空间的同一邻域内。</li><li>该方法假设技能是最优的，并且下层的任务来自于技能空间的相同分布，因此学习的通用性和变化适应性有限，例如从移动方块中学习技能，则无法应对障碍物、物体变化、不同摩擦等情况。</li></ul><p>为解决上述问题，本文提出了以下创新方法，称为残差技能策略（Residual Skill Policies，ReSkill）：</p><ul><li>状态条件技能先验：对相关技能进行采样来引导探索</li><li>底层残差策略：通过对技能进行细粒度的技能适应，实现任务变化的适应</li></ul><h2 id="5-2-方法">5.2 方法</h2><p>总的来说，该方法将经典控制器产生的演示轨迹分解为与任务无关的技能，并将其嵌入到连续到技能空间中，利用技能空间实现真正的通用学习，上层智能体能够从技能空间中访问但不动作，降低了对数据集详细程度的要求。</p><ul><li>从现有控制器中提取技能</li><li>学习技能嵌入和先验技能</li><li>训练一个分层强化学习策略，在技能空间中使用底层残差适应性策略。</li></ul><p><img src="https://img.mahaofei.com/img/202311161019261.png" alt="image.png"></p><p><strong>（1）数据收集</strong></p><p>本文通过手动控制收集演示数据（基本操作任务，如推物体、抓物体），虽然任务简单，但轨迹包含复杂的技能，可以重新组合解决复杂的任务。</p><p>轨迹是由 state-action 成对组成的，本文从中随机切片 $H$ 长度的片段进行无监督技能提取，利用提取的动作 a 和状态 s 学习下一小节中的 state-action。</p><p>其中状态 s 包括关节角度、关节速度、夹具位置、物体位置，动作是连续的 4D 向量，包括末端位置和速度。</p><p><strong>（2）学习强化学习的状态条件技能空间</strong></p><ul><li>将提取的技能嵌入到潜在空间中：使用变分自动编码器 VAE 将技能 $a$ 嵌入到潜在空间中，VAE 包括编码器和解码器，编码器将完整的 state-action 序列编码为 $z$，解码器根据当前状态 $s_t$ 和技能编码 $z$ 重建动作。</li><li>在探索过程中采样的技能状态条件先验：学习潜在技能空间上的条件概率密度。传统的高斯密度不能处理多模态信息，本文使用 real NVP 方法，实值非体积保留变换。学习从 $Z\times S-&gt;G$ 的映射，该映射就可以从简单分布 G 变换到技能空间 Z，因此 f 就是技能先验。</li></ul><blockquote><p><strong>变分自编码器</strong>，是一种深度生成模型<br><strong>传统</strong>：传统的自编码器包括编码器和解码器两部分，经过反复训练，输入数据被编码成一个编码向量，编码向量的每一个维度表示学习到的数据的特征，解码器尝试从编码向量中解码原始输入<br><strong>缺陷</strong>：传统的方法，使用单个值表示输入在某个潜在特征的表现。但实际上，将潜在特征表示为可能的取值范围会更合理。<br><strong>改进</strong>：因此变分自编码器就是使用取值的概率分布，代替原来的单值表示特征。<br><strong>优势</strong>：每个潜在特征表示为概率分布，解码时从潜在状态分布中随机采样，生成一个编码向量作为解码器的输入。实现了连续且平滑的潜在空间表示（潜在空间中彼此相邻的值重构出的结果相似）<br>参考理解:<a href="https://zhuanlan.zhihu.com/p/64485020">https://zhuanlan.zhihu.com/p/64485020</a></p></blockquote><p><strong>（3）状态条件技能空间中的强化学习</strong></p><p>一旦训练完成，解码器和技能先验权重就会被冻结，并合并到 RL 框架中。高级强化学习策略 $\pi$ 是一个神经网络，将状态映射到技能先验变化中的向量 g，在转换为潜在技能 Z。</p><p>然后解码器根据技能范围 H 的当前状态顺序重构动作。同时有一个底层残差策略，调整解码后的技能。</p><h2 id="5-3-总结">5.3 总结</h2><p>该方法是一种基于技能的强化学习方法。</p><ol><li>数据收集：使用最基本的控制器生成一些基本任务轨迹（移动、抓取），然后将这些轨迹分割成固定长度的序列，每一小段包括动作和对应的状态。</li><li>学习技能空间，使用变分自编码器将技能编码到潜在空间中；使用realNVP将技能潜在空间+机器人状态空间映射到简单分布空间（高斯分布），这样可以直接根据状态采样技能，称为技能先验。</li><li>强化学习：使用一个高层策略网络，根据当前的状态生成一个向量，根据技能先验（与当前状态有关的技能）中选择一个技能，利用技能解码器解码成机器人动作。</li></ol><h1>6 Watch and Match: Supercharging Imitation with Regularized Optimal Transport</h1><blockquote><p><strong>标题</strong>：观看与匹配：通过正则化最优传输增强模仿<br><strong>作者团队</strong>：纽约大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://rot-robot.github.io/">https://rot-robot.github.io/</a></p></blockquote><h2 id="6-1-目标问题-2">6.1 目标问题</h2><p>目前模仿学习通常使用逆强化学习，给出演示的情况下，交替推理奖励函数和策略。但是这种方式需要大量的在线交互来解决复杂的控制问题。</p><p>本文提出了正则化最佳传输（Regularized Optimal Transport，ROT）方法，即使只有少量的演示，也能自适应的匹配轨迹奖励与行为克隆，加速模仿。</p><blockquote><p>基于最佳传输的模仿学习（Optimal Transport，OT）：模仿学习实在给定专家策略或轨迹的情况下学习行为行为策略 $\pi^b$，逆强化学习根据专家轨迹 $T^e$ 推断奖励函数 $r^e$，然后利用奖励优化策略来得到行为策略 $\pi^b$。为了计算 $r^e$，基于 OT 的逆学习方法就是一种思路。专家轨迹和行为轨迹的接近程度可以通过测量两个轨迹之间的最佳传输来计算。</p></blockquote><h2 id="6-2-方法-2">6.2 方法</h2><p><strong>（1）BC 预训练</strong></p><p>使用 BC 对专家演示的数据进行随机初始化策略的训练。</p><p>BC 对应求解公式中的最大似然问题，这里的专家轨迹 $T^e$ 指的是专家演示，训练后，它能够使 $\pi^{BC}$ 模仿与演示中想对应的动作，但是如果出现未见过的状态，那么很容易会导致推理失败。</p><p><strong>（2）在线 IRL 微调</strong></p><p>在 BC 训练的模型基础上，进行在线微调策略。由于本文操作没有明确的任务奖励，因此使用基于 OT 的轨迹匹配获得奖励。（本文使用了 n 步 DDPG 方法实现连续控制）</p><ol><li>正则化微调：由于在线部署期间很容易因为错误累计导致分布偏移，本文通过基于引导 RL 和离线 RL 将 $\pi^{ROT}$ 与 BC 损失相结合来规范 $\pi^{ROT}$ 的训练，此处设置了一个 $\lambda(\pi)$ 自适应权重来控制两个损失项的贡献。</li><li>柔性 Q 滤波的自适应正则化：自适应权重调整 $\lambda(\pi)$，通过比较当前策略 $\pi^{ROT}$ 和与训练策略 $\pi^{BC}$ 在一段重放缓冲区采样的一批数据的性能表现来完成。</li><li>基于图像观测的考虑：对视觉观测进行数据增强，将图像输入 CNN 编码器，获得 OT 奖励的计算，减少 ROT 模仿过程中的非平稳性。</li></ol><h2 id="6-3-实验">6.3 实验</h2><p>本文的模型包括三个神经网络：encoder、actor、critic，三者均使用均方误差进行训练。</p><p>使用 n 步 DDPG 作为 RL 主干，actor 使用确定性策略梯度进行训练。critic 使用 clipped double Q-learning 进行训练，主要时为了减少高估问题，因此使用两个 Q 函数实现 critic 的学习。</p>]]></content>
    
    
    <summary type="html">基于强化学习的机器人动作模仿方法论文调研与笔记。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【模仿学习笔记】行为克隆 Behavior Cloning</title>
    <link href="https://www.mahaofei.com/post/1d3a3b82.html"/>
    <id>https://www.mahaofei.com/post/1d3a3b82.html</id>
    <published>2023-11-03T09:12:44.000Z</published>
    <updated>2023-11-03T09:12:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、行为克隆概念</h1><p>行为克隆属于模仿学习中的方法，不是强化学习。</p><blockquote><p>强化学习：从环境给出的奖励中进行监督；<br>模仿学习：从人类动作经验中监督<br>区分两者主要在于，模仿学习没有奖励回报，知识模仿专家动作。</p></blockquote><h1>二、行为克隆过程</h1><ol><li>观测当前状态 $s_t$</li><li>策略网络做出预测 $p_t$</li><li>专家的动作是 $a_t^*$，向量化从而得到 $y_t$</li><li>计算损失 CrossEntropy($y_t,p_t$)</li><li>使用梯度下降来更新策略网络</li></ol><h1>三、行为克隆的优势与不足</h1><p>如果当前的状态出现在训练数据中，则可以根据行为克隆训练得到的策略网络，执行类似于人类专家的动作。</p><p>但是如果当前状态没有出现在训练数据中，那么策略网络输出的动作可能不会很好，而且错误会累加。这种情况尤其出现在状态极为复杂的情况下。</p>]]></content>
    
    
    <summary type="html">学习模仿学习中的行为克隆部分的笔记</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="模仿学习" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【强化学习笔记】强化学习基础</title>
    <link href="https://www.mahaofei.com/post/cfa8c737.html"/>
    <id>https://www.mahaofei.com/post/cfa8c737.html</id>
    <published>2023-11-02T02:01:55.000Z</published>
    <updated>2023-11-02T02:01:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、基本概念</h1><h2 id="1-1-专业术语">1.1 专业术语</h2><p><strong>（1）状态（State）</strong>：状态可以被理解为当前环境的情况。</p><p><strong>（2）动作（Action）</strong>：动作是智能体（agent）采取的行为。</p><p><strong>（3）策略（Policy）</strong>：策略是用于在给定观测状态下做出决策的函数，通常表示为 $\pi(a|s)$，其中 $a$ 是动作，$s$ 是状态。强化学习的目标是学习策略函数，通常以概率密度函数的形式表示。</p><p><strong>（4）奖励（Reward）</strong>：奖励定义了奖励的方式，对强化学习的结果产生重要影响。</p><p><strong>（5）状态转移（State Transition）</strong>：状态转移表示在当前状态下，当智能体执行一个动作后，环境可能随机转移到的下一个状态的概率，通常表示为 $P(S’|S, A)$。</p><p><strong>（6）回报（Return）</strong>：回报又被称为未来奖励的累积，通常表示为 $U_t = R_t + R_{t+1} + R_{t+2} + \ldots$。</p><p><strong>（7）折扣回报（Discounted Return）</strong>：折扣回报考虑未来奖励的折扣效应，用折扣率 $\gamma$ 表示，通常表示为 $U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots$。</p><p><strong>（8）动作价值函数（Action-Value Function）</strong>：动作价值函数表示在给定状态和动作下，智能体可以获得的期望回报，通常表示为 $Q_\pi(s_t, a_t) = E[U_t | S_t=s_t, A_t=a_t]$。最优策略下的动作价值函数被表示为 $Q^*(s_t, a_t) = \max_\pi Q_\pi(s_t, a_t)$，使用动作价值函数可以评估当前动作的质量。</p><p><strong>（9）状态价值函数（State-Value Function）</strong>：状态价值函数表示在给定状态下，按照策略函数的预期回报，通常表示为 $V_\pi(s_t) = E_A[Q_\pi(s_t, A)]$。状态价值函数可以告诉我们当前状态的好坏程度。</p><p><strong>（10）交叉熵（Cross Entropy）</strong>：交叉熵用于度量两个概率分布之间的差异，通常表示为 $H(\textbf p, \textbf q) = -\sum^m_{j=1}p_j\cdot \log(q_j)$。当两个概率分布相同时，交叉熵达到最小值。</p><h2 id="1-2-强化学习的随机性">1.2 强化学习的随机性</h2><p><strong>（1）Action动作的随机性</strong></p><p>因为动作是根据策略函数随机抽样得到的，因此agent有可能做策略中的任何一种动作，虽然这些动作的概率有大有小，但是动作本身是随机的。</p><p><strong>（2）State transitions状态转移的随机性</strong></p><p>假定agent作出了一个动作，环境会用概率随机抽样，给出下一个状态。</p><h2 id="1-3-强化学习如何控制agent">1.3 强化学习如何控制agent</h2><p><strong>（1）如果有策略函数 $\pi(a|s)$</strong></p><ol><li>给定一个观测状态 $s_t$</li><li>利用策略函数从所有可能的动作中随机采样 $a_t~\pi(\cdot|s_t)$</li></ol><p><strong>（2）如果有最优的动作价值函数 $Q^<em>(s,a)</em>$</strong></p><ol><li>给定一个观测状态 $s_t$</li><li>最大化 $a_t=argmax_a Q^*(s_t,a)$ 来选择动作</li></ol><h1>二、价值学习 Deep Q-Network(DQN)</h1><p>$U_t$ 反映未来奖励的总和，因此我们要知道 $U_t$ 的大小，由于其是一个随机变量，我们可以对 $U_t$ 求期望，只留下 $s_t$ 和 $a_t$ 两个变量。</p><p>$$Q_\pi(s_t,a_t)=E[U_t|S_t=s_t,A_t=a_t]$$</p><p>要想进一步消除策略函数 $\pi$，可以对 $Q_\pi$ 关于 $\pi$ 球最大化，记为 $Q^*$</p><p>$$Q^*(s_t,a_t)=max_\pi Q_\pi(s_t,a_t)$$</p><p>这个参数告诉我们不管在什么情况 $s_t$ 下做动作 $a_t$，那么期望顶多就是 $Q^*(s_t,a_t)$。</p><blockquote><p>目标：完成任务（最大化总回报）<br>问题：如果已知 $Q^<em>(s,a)$，那么最好的动作就是 $a^</em>=argmax_a Q^<em>(s,a)$，因为 $Q^</em>$ 指示了该agent在s状态下选择a动作的好坏程度<br>挑战：我们不知道 $Q^*(s,a)$</p></blockquote><p><strong>（1）什么是 DQN</strong></p><p>我们使用神经网络 $Q(s,a;w)$ 来近似 $Q^*(s,a)$，其中 w 是要近似的参数，s 是输入，a 是输出是对所有动作的打分。</p><p><img src="https://img.mahaofei.com/img/202311030917032.png" alt="image.png"></p><p>当前观测到状态 $s_t$，用DQN把 $s_t$ 作为输入，为所有动作打分，选出分数最高的动作作为 $a_t$。</p><p>agent 作出动作 $a_t$ 后，环境会改变，用状态转移函数 $p$ 随机抽取一个新的状态 $s_{t+1}$，环境还会告诉我们一个回报 $r_t$，这个 $r_t$ 就是训练DQN的关键。</p><p><strong>（2）如何训练 DQN</strong></p><p>常规的网络训练过程如下：</p><ol><li>首先对任务结果做一个预测 $q=Q(w)$</li><li>完成任务后获得目标 $y$</li><li>计算损失 $L=\frac{1}{2}(q-y)^2$</li><li>计算梯度 $\frac{\partial L}{\partial w}=\frac{\partial L}{\partial q}\cdot \frac{\partial q}{\partial w}$</li><li>更新参数 $w_{t+1}=w_t-\alpha \cdot\frac{\partial L}{\partial w}|_{w=w_t}$</li></ol><p>但这种方式需要完整完成一次任务后才能更新参数，而能否执行一部分任务后就开始更新参数，因此有了 Temporal Difference Learning （TD算法），过程如下：</p><ol><li>首先对任务结果做一个预测 $q=Q(w)$</li><li>执行一部分任务后，对任务结果再进行预测 $y$，此时的 $y$ 包括已经完成的部分和对剩下部分的预测，因此比 $q$ 更可靠</li><li>计算损失 $L=\frac{1}{2}(q-y)^2$</li><li>计算梯度 $\frac{\partial L}{\partial w}=\frac{\partial L}{\partial q}\cdot \frac{\partial q}{\partial w}$</li><li>更新参数 $w_{t+1}=w_t-\alpha \cdot\frac{\partial L}{\partial w}|_{w=w_t}$</li></ol><p>在深度强化学习中，也就是下面这个公式</p><p>$$Q(s_t,a_t;w)\approx r_t+\gamma\cdot Q(s_{t+1},a_{t+1};w)$$</p><p>对未来奖励总和的期望，就是真实已经观测到的奖励，加在t+1时刻对未来奖励的期望。</p><ol><li>首先进行预测 $Q(s_t,a_t;w_t)$</li><li>获得TD目标 $y_t=r_t+\gamma\cdot Q(s_{t+1},a_{t+1};w_t)=r_t+\gamma\cdot max_a Q(s_{t+1},a;w_t)$</li><li>计算损失 $L_t=\frac{1}{2}[Q(s_t,a_t;w)-y_t]^2$</li><li>进行剃度下降 $w_{t+q}=w_t-\alpha\cdot\frac{\partial L_t}{\partial w}|_{w=w_t}$</li></ol><p><img src="https://img.mahaofei.com/img/202311031636429.png" alt="image.png"></p><p><strong>（3）经验回放</strong></p><p>之前我们使用在线梯度下降来更新 $w$，以此来减小TD errer $\delta_t=q_t-t_t$。</p><p>我们定义一个经验transition为$(s_t,a_t,r_t,s_{t+1})$，传统的方法再每使用一个transition后就会丢弃它，这回造成经验的浪费。此外传统的方法还忽略了不同经验之间的相关性。</p><p>将最近的n个transition存储进一个replay buffer，当有新的经验进来后，就删除老的transition。</p><ol><li>每次从buffer中随机抽取一个transition</li><li>计算TD error</li><li>极端梯度</li><li>进行随机梯度下降（实际一般使用minibatch SGD，一次取多个transition）</li></ol><p><strong>优先经验回放</strong>：为了解决数据的不均匀性，可以使用重要性抽样代替平均采样。可以根据TD error抽样，误差越大的，transition被抽样的概率越大。</p><p><strong>学习率比例设置</strong>：如果一个transition有较大的抽样概率，那么其学习率应该设置的比较小。</p><p><strong>更新TD error</strong>：如果一个transition没有被用过，那么就设置它的TD error为最大值，在训练DQN的同时，对TD error进行更新。</p><h1>三、策略学习</h1><p>策略函数 $\pi(a|s)$是一个概率密度函数，对每一个给定的状态 $s$，策略函数会抽取一个最优的动作 $a$ 作为将要执行的动作。</p><p>理想情况下，列出所有的状态和动作，计算所有状态和动作之间的概率即可。</p><p>但是实际情况下有无数个状态，不可能记录所有的状态对应的动作，因此需要函数近似。一般使用神经网络进行近似，即policy network $\pi(a|s;\theta)$</p><p>状态价值函数$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]$，状态价值函数可以告诉我们当前的局势好不好。在状态已知时，还可以判断策略好不好，策略越好，$V_\pi$ 越大，任务完成成功率越高，$V_\pi$ 可以表示为：</p><p>$$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]=\sum_a\pi(a|s_t)\cdot Q_\pi(s_t,a)$$</p><p>使用神经网络替换策略函数，因此得到：</p><p>$$V_\pi(s_t;\theta)=\sum_a\pi(a|s_t;\theta)\cdot Q_\pi(s_t,a)$$</p><p>给定状态 $s$，策略函数函数越好，价值函数越大。因此可以考虑通过改变神经网络参数 $\theta$，让 $V(s;\theta)$ 变大，基于这个思想，可以求期望：</p><p>$$J(\theta)=E_s[V(S;\theta)]$$</p><p>策略网络越好，$J(\theta)$ 就越大，为了改变 $\theta$，我们使用策略梯度算法。</p><ol><li>观测状态 $s$</li><li>更新策略 $\theta=\theta+\beta\cdot\frac{\partial V(s;\theta)}{\partial \theta}$，做梯度上升，因为我们希望价值函数越大越好。</li></ol><p>对于离散的动作，使用$\frac{\partial V(s;\theta)}{\partial \theta}=\sum_a \frac{\partial \pi(a|s;\theta)}{\partial \theta}\cdot Q_\pi(s,a)$</p><p>对于连续的动作，使用$\frac{\partial V(s;\theta)}{\partial \theta}=E_{A~\pi(\cdot|s;\theta)} [\frac{\partial \pi(a|s;\theta)}{\partial \theta}\cdot Q_\pi(s,a)]$</p><p><img src="https://img.mahaofei.com/img/202311031636316.png" alt="image.png"></p><h1>四、Actor-Crictic</h1><p>状态价值函数的定义如下：</p><p>$$V_\pi(s_t)=\sum_a\pi(a|s_t)\cdot Q_\pi(s_t,a)$$</p><p>策略网络（产生动作）：</p><ul><li>使用神经网络 $\pi(a|s;\theta)$ 来近似策略函数 $\pi(a|s)$</li><li>其中 $\theta$ 是训练的参数</li></ul><p>价值网络（产生评判标准）：</p><ul><li>使用神经网络 $q(s,a;w)$ 来近似价值函数 $Q_\pi(s,a)$</li><li>其中 $w$ 是训练的参数</li></ul><p>因此状态价值函数可以写成</p><p>$$V_\pi(s_t)=\sum_a\pi(a|s_t;\theta)\cdot Q_\pi(s_t,a;w)$$</p><p>同时训练策略网络和价值网络，就称为 Actor-Critic Method，大致步骤如下：</p><ol><li>观测当前状态 $s_t$</li><li>根据策略函数 $\pi(\cdot|s_t;\theta_t)$ 随机采样获得动作 $a_t$</li><li>执行动作 $a_t$，并观测新的状态 $s_{t+1}$ 和回报 $r_t$</li><li>更新价值网络的参数 $w$，使用TD算法</li><li>更新策略网络的参数 $\theta$，使用策略梯度算法</li></ol><p>训练过程中需要同时训练策略网络和价值网络，利用价值网络对策略网络进行评分。训练完成后就不需要价值网络了，只需要策略网络生成动作。</p><h1>五、蒙特卡洛树搜索（Monte Carlo Tree Search）</h1><h2 id="5-1-基本思想">5.1 基本思想</h2><p>蒙特卡洛树搜索的思想是人们必须要向前看很多步，看到未来时间内所有可能的情况，挑选最优的执行动作。</p><ol><li>如果我在此时选择执行动作 $a_t$</li><li>那么未来一段时间环境的反馈是怎么变化的 $s_{t+1}$</li><li>基于这种环境变化，我又会执行动作 $a_{t+1}$</li><li>此时环境又会如何变化</li></ol><p>如果一个agent能够穷举所有的可能性直到任务完成，那么这个任务一定有很高的成功率。</p><h2 id="5-2-过程">5.2 过程</h2><p><strong>（1）选择</strong></p><p>根据分数选择一个动作（假想的动作，实际上并不会执行）；</p><p>首先对所有可能的动作 $a$，计算得分：</p><p>$$score(a)=Q(a)+\eta\cdot\frac{\pi(a|s_t;\theta)}{1+N(a)}$$</p><p>其中 $Q(a)$ 是蒙特卡洛树搜索计算的动作价值<br>$\pi(a|s_t;\theta)$ 是学习好的策略网络，动作越好，策略分数越高<br>$N(a)$ 是给定环境状态 $s_t$ 情况下，目前为止选择动作 $a$ 的次数，如果同一个动作被探索太多次，该项分母就会变大。</p><p><strong>（2）扩展</strong></p><p>假想环境更新；</p><p><strong>（3）评估</strong></p><p>评价状态价值得分 $v$ 和回报 $r$，将动作的分数设为 $\frac{v+r}{22}$；</p><p><strong>（4）备份</strong></p><p>用动作的分数 $\frac{v+r}{2}$ 更新动作价值：</p><p>$$Q(a_t)=mean(the recorded V’s)$$</p><p>将以后所有步的状态价值进行平均。</p><h1>六、连续控制</h1><p>在实际进行强化学习时，可能有离散动作空间（例如上下左右控制游戏人物），也可能是连续动作（机械臂关节控制）。</p><p>在进行离散控制时，可以直接使用分类的思想，得到一个onehot向量，每个向量元素代表执行该动作的得分，以此来获得应该执行那种动作。而连续控制中动作空间是有无穷维的，因此不能直接使用这种思想实现连续控制。</p><p>比较常规的一种解决思路是将动作空间离散化，但这种方式也有问题，例如机械臂的6个自由度，就算每个自由度离散为360个点，那么整个动作空间也有 $360^6$ 个点，这会造成维度灾难，在训练时非常困难。</p><p>因此有两种方式实现连续控制：</p><ul><li>确定性策略网络</li><li>随机策略网络</li></ul><h2 id="6-1-确定策略梯度（Deterministic-Policy-Gradient-DPG）">6.1 确定策略梯度（Deterministic Policy Gradient DPG）</h2><p>考虑一个只有2自由度的机械臂，基座运动范围为(0,180)，机械臂运动范围为(0,360)，因此机械臂的动作空间是 $A=[0,180]\times[0,360]$ 的连续集合，动作就是一个二维向量。</p><p>DPG 是一种 Actor-Critic 方法</p><ul><li>有一个<strong>策略网络</strong>，控制 agent 运动，它根据状态 s 做出决策 a；<br>使用策略网络 $a=\pi(s;\theta)$ 根据输入状态 s，输出一个<strong>确定</strong>的动作 a，这里的动作 a 就是机器人的二维动作向量。</li><li>有一个<strong>价值网络</strong>，不控制 agent，它根据状态 s，给动作 a 打分，从而指导策略网络做出改进。<br>使用价值网络 $q(s,a;w)$，输入状态 s 和动作 a，输出一个实数 value 是对动作的评价，动作越好，value 越大。</li></ul><p>因此 DPG 的原理就是训练这两个网络。</p><p><img src="https://img.mahaofei.com/img/202311041607515.png" alt="image.png"></p><p><strong>（1）价值网络训练</strong></p><ol><li>每次得到一个训练数据 transition $(s_t,a_t,r_t,s_{t+1})$</li><li>用价值网络预测当前时刻 t 下的动作价值 $q_t=q(s_t,a_t;w)$</li><li>用价值网络预测下一时刻 t+1 的动作价值 $q_{t+1}=q(s_{t+1},a_{t+1}‘;w)$ ，其中 $a_{t+1}’=\pi(s_{t+1};\theta)$，这个动作并不是 agent 真正执行的动作，$a_{t+1}'$ 只用于更新价值网络。</li><li>计算 TD error：$\delta_t=q_t-(r_t+\gamma\cdot q_{t+1})$，其中第二项是 TD Target，它一部分是真实观测到的奖励，另一部分是价值网络自己做出的预测。因为我们认为第二项中由于包含本步真实奖励，比单纯的 $q_t$ 更接近真实情况，因此要让 $q_t$ 与 TD Target 接近，也就是让 TD error 尽可能小。</li><li>进行梯度下降更新 w：$w=w-\alpha\cdot\gamma_t\cdot\frac{\partial q(s_t,a_t;w)}{\partial w}$</li></ol><p>但这其中有一个问题，就是计算 TD error $\delta_t=q_t-(r_t+\gamma\cdot q_{t+1})$ 这一步时，会出现 bootstrapping 问题，也就是如果初始值高估或者低估，那么 TD target 就会有高估或低估，并传播回价值网络自身，导致高估或低估一直存在，解决方案就是用不同的神经网络计算 TD Target，也就是用 Target Networks。</p><ol><li>每次得到一个训练数据 transition $(s_t,a_t,r_t,s_{t+1})$</li><li>用价值网络预测当前时刻 t 下的动作价值 $q_t=q(s_t,a_t;w)$</li><li>用价值网络预测下一时刻 t+1 的动作价值 $q_{t+1}=q(s_{t+1},a_{t+1}‘;w^-)$ ，其中 $a_{t+1}’=\pi(s_{t+1};\theta^-)$<br>$\pi(s_{t+1};\theta^-)$ 是 Target policy network 用来代替策略网络，它的网络结构和策略网络一模一样，但是参数不一样。<br>$q(s_{t+1},a_{t+1}';w^-)$ 是 Target value network，它与价值网络结构一样，参数不同。</li></ol><p><strong>（2）策略网络训练</strong></p><p>训练策略网络，需要靠价值网络评价动作的好坏，从而指导策略网络进行改进。</p><p>也就是更新策略网络的参数 $\theta$ 让价值网络认为动作 $a=\pi(s;\theta)$ 更好，也就是改进 $\theta$ 让价值 $q(s,a;w)=q(s,\pi(s;\theta);w)$ 尽可能大。</p><p>由于给定状态 s，策略网络会输出一个确定的动作 a，而如果价值网络也是确定的，那么输出的价值就是确定的。</p><p>因此问题中只需要改变 $\theta$，使得价值 q 变大，也就是计算 $q(s,a;w)$ 对 $\theta$ 的梯度，然后用梯度上升更新 $\theta$，就可以让 $q$ 变大，这个梯度就叫<strong>确定策略梯度 DPG</strong>。</p><p>$$<br>g=\frac{\partial q(s,\pi(s;\theta);w)}{\partial\theta}=\frac{\partial a}{\partial \theta}\cdot\frac{\partial q(s,a;w)}{\partial a}<br>$$</p><p>其中 $a=\pi(s;\theta)$，然后进行梯度上升 $\theta=\theta+\beta\cdot g$</p><p><strong>策略网络和价值网络联合具体步骤如下：</strong></p><ol><li>策略网络做一个决策：$a=\pi(s;\theta)$</li><li>计算价值网络的输出：$q_t=q(s,a;w)$</li><li>用 DPG 更新策略网络： $\theta=\theta+\beta\cdot \frac{\partial a}{\partial \theta}\cdot\frac{\partial q(s,a;w)}{\partial a}$</li><li>利用 Target networks $\pi(s;\theta^-)$ 和 $q(s,a;w^-)$ 计算 $q_{t+1}$</li><li>计算 TD error：$\delta_t=q_t-(r_t+\gamma\cdot q_{t+1})$</li><li>更新价值网络：$w=w-\alpha\cdot\gamma_t\cdot\frac{\partial q(s_t,a_t;w)}{\partial w}$</li><li>更新 Target networks 的参数：$w^-=\tau\cdot w+(1-\tau)\cdot w^-$，$\theta^-=\tau\cdot \theta+(1-\tau)\cdot \theta^-$，其中 $\tau$ 是超参数</li></ol><h2 id="6-2-随机策略用于连续控制">6.2 随机策略用于连续控制</h2><p>首先考虑自由度等于 1 的随机策略连续控制，也就是动作都是实数。</p><p>设 $\mu$ 代表均值，和 $\sigma$ 代表标准差，都是状态 s 的函数。</p><p>用正态分布的概率密度函数作为策略函数：</p><p>$$<br>\pi(a|s)=\frac{1}{\sqrt{6.28}\sigma}\cdot exp(-\frac{(a-\mu)^2}{2\sigma^2})<br>$$</p><p>对于 d 维情况一样，动作是 d 维向量。</p><p>设向量 $\mu$ 代表均值，和向量 $\sigma$ 代表标准差，都是状态 s 的函数。</p><p>使用特殊正态分布作为策略函数：</p><p>$$<br>\pi(a|s)=\prod_{i=1}^d\frac{1}{\sqrt{6.28}\sigma_i}\cdot exp(-\frac{(a_i-\mu_i)^2}{2\sigma^2_i})<br>$$</p><p>但这里我们不知道 $\mu$ 和 $\sigma$，也就不知道策略函数。</p><p>因此可以用神经网络来近似 $\mu(s;\theta^{\mu})$ 和 $\rho(s;\theta{\rho})$，其中 $\rho_i=ln\sigma_i^2$</p><p>将策略函数进行取对数，将连乘变成连加，得到辅助神经网络 $f(s,a;\theta)=\sum^d_{i=1}[-\frac{\rho_i}{2}-\frac{(a_i-\mu_i)^2}{2\cdot exp(\rho_i)}]$，计算 f 关于其中卷积层和全连接层的参数的梯度，进而实现反向传播更新参数。</p><p><img src="https://img.mahaofei.com/img/202311041725827.png" alt="image.png"></p><blockquote><p>参考：</p><ol><li>王树森.  <a href="https://www.youtube.com/channel/UC9qKcEgXHPFP2-ywYoA-E0Q/playlists?view=50&amp;sort=dd&amp;shelf_id=2">强化学习课程(Youtube)</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">强化学习基础部分笔记</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="强化学习" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="强化学习" scheme="https://www.mahaofei.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础知识" scheme="https://www.mahaofei.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="Python" scheme="https://www.mahaofei.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>【论文复现】MimicPlay从人类演示中学习机器人技能</title>
    <link href="https://www.mahaofei.com/post/21b38b7b.html"/>
    <id>https://www.mahaofei.com/post/21b38b7b.html</id>
    <published>2023-10-21T07:00:02.000Z</published>
    <updated>2023-10-21T07:00:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1>论文笔记</h1><h2 id="1-目标问题-23">1 目标问题</h2><p>从人类演示中学习，是教授机器人操作技能的一种很有前途的方法。</p><p>目前大多数模仿学习算法仍然局限于学习短期的操作，例如开门或抓取特定物品。</p><p>而关于长期任务的研究，目前有两个方向：分层模仿学习和从演示数据中学习。分层学习旨在通过端到端实现高级规划到低级运动控制的学习。从演示数据中学习是指人类通过遥控机器人于环境互动来收集数据。</p><p>本文提出了一个分层学习框架，从大量人类演示数据中学习潜在的计划，来指导机器人在少量演示中实现视觉运动控制。</p><h2 id="2-主要方法">2 主要方法</h2><p><img src="https://img.mahaofei.com/img/202310231710757.png" alt="image.png"></p><h3 id="2-1-收集人类数据">2.1 收集人类数据</h3><p>人类在用手与环境互动的过程中，创造了一个手的轨迹。本文使用两台经过校准的相机来跟踪人类演示数据中的3D手轨迹，手部位置检测使用现有的库。</p><h3 id="2-2-从人类数据中学习3D潜在规划">2.2 从人类数据中学习3D潜在规划</h3><p>问题：给定一个由目标图像表示的长期任务，策略产生以目标为条件的行动。</p><p>将该问题转化为分层学习策略，其中高级规划器从目标图像中提取关键特征，并转化成低维的规划，利用这些规划引导运动控制器动作。为了训练高级规划器，本文使用廉价的数据源（人类演示数据）</p><p><strong>（1）多模式潜在计划学习</strong></p><p>利用收集的人类演示数据和对应的3D手部轨迹，将学习规程转化为目标条件的3D轨迹生成任务。即将人类演示图像，目标图像处理为低维特征，利用MLP编码为潜在计划向量，利用潜在计划向量和手的位置，利用MLP解码为3D手部轨迹的预测。</p><p>为了解决不同人演示同一任务的差异，使用基于MLP的高斯混合模型来对潜在计划的轨迹分布进行建模。</p><p><strong>（2）处理人类演示数据和机器人之间的视觉差异</strong></p><p>本文考虑人与机器人在同一环境中，需要解决机器人与人类外观不同导致的视觉差异。通过计算人类和机器人的视觉编码器的特征嵌入的分布，最小化两者距离（此步骤机器人与人类视频不需要是对应的）</p><h3 id="2-3-多任务模仿学习">2.3 多任务模仿学习</h3><p><strong>（1）用于潜在计划生成的视频提示</strong></p><p>使用单镜头视频作为目标指定提示，发送给训练好的潜在规划器，生成机器人可执行的潜在计划。</p><p>规划器将视频分成多个帧，每个时间步长，规划器从序列中取一个图像帧作为目标图像，生成潜在规划引导机器人动作。</p><p><strong>（2）基于Transformer的计划引导与模仿</strong></p><p>在执行复杂任务时，仅使用高层规划是不够的，还需要考虑底层的细节。因此考虑将机器人腕部相机和本体感觉都转换成低维特征向量，与潜在计划进行结合，利用Trasformer架构（因为其擅长管理长期运动生成）进行处理。</p><p><strong>（3）多任务</strong></p><p>在同一环境中的所有任务中共享相同的规划器和策略模型。</p><h1>算法复现</h1><h2 id="1-环境搭建">1 环境搭建</h2><h3 id="1-1-代码准备">1.1 代码准备</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/j96w/MimicPlay.git</span><br></pre></td></tr></table></figure><h3 id="1-2-conda环境配置">1.2 conda环境配置</h3><p><strong>（1）进入所下载代码环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay</span><br></pre></td></tr></table></figure><p><strong>（2）创建 conda 环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mimicplay python=3.8 -y</span><br><span class="line">conda activate mimicplay</span><br></pre></td></tr></table></figure><p><strong>（3）安装 MuJoCo</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mujoco==2.3.0</span><br></pre></td></tr></table></figure><blockquote><p><strong>如果出现问题</strong>：<code>imgaug 0.4.0 requires XXXXXX, which is not installed.</code><br><strong>解决方法如下</strong>：</p><ol><li>安装报错提示的imgaug所需的依赖项：<code>pip install imageio matplotlib Pillow scikit-image six opencv-python</code></li><li>重新安装mujoco：<code>pip install mujoco</code></li></ol></blockquote><p><strong>（4）安装robosuite</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ARISE-Initiative/robosuite.git</span><br><span class="line">cd robosuite</span><br><span class="line">git checkout v1.4.1_libero</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -r requirements-extra.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><blockquote><p>第一次安装失败，然后<code>git checkout v1.4.1_libero</code>之后才安装成功</p></blockquote><p><strong>（5）安装BDDL</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/StanfordVL/bddl.git</span><br><span class="line">cd bddl</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><strong>（6）安装LIBERO</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git</span><br><span class="line">cd LIBERO</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><strong>（7）安装robomimic</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/ARISE-Initiative/robomimic</span><br><span class="line">cd robomimic</span><br><span class="line">git checkout mimicplay-libero</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><blockquote><p>第一次安装失败，然后<code>git checkout mimicplay-libero</code>之后才安装成功</p></blockquote><p><strong>（8）安装MimicPlay</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">git clone https://github.com/j96w/MimicPlay.git</span><br><span class="line">cd MimicPlay</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h2 id="2-数据准备（从虚拟机器人动作数据中学习机器人动作）">2 数据准备（从虚拟机器人动作数据中学习机器人动作）</h2><h3 id="2-1-官方数据集">2.1 官方数据集</h3><p>训练集和测试视频在<a href="https://drive.google.com/drive/folders/1FUKd3vr-KBiYRnKIymNmGClmVx9U45XG">此处</a>下载。训练集是一系列没有指定特定任务（没有标签）的人类演示视频。</p><p>作者推荐下载原始数据<code>demo.hdf5</code>，然后在本地电脑上将其处理为具有图像观察的训练数据集<code>demo_image.hdf5</code>，因为这样可以很好的检查环境库是否安装正确，具体步骤如下：</p><p><strong>（1）将下载的数据集移动到<code>mimicplay/datasets</code></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">mv mimicplay_release_dataset your_installation_path/mimicplay/datasets</span><br></pre></td></tr></table></figure><p><strong>例如：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">mkdir -p datasets/playdata</span><br><span class="line">mv ~/Downloads/demo.hdf5 ./datasets/playdata/</span><br></pre></td></tr></table></figure><p><strong>（2）将原始数据转换为图像数据集</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">python scripts/preprocess_hdf5.py -i ./datasets/playdata/demo.hdf5 -o ./datasets/playdata/demo_modified.hdf5</span><br><span class="line">python scripts/dataset_states_to_obs.py --dataset &#x27;datasets/playdata/demo_modified.hdf5&#x27; --done_mode 0 --camera_names agentview robot0_eye_in_hand --camera_height 84 --camera_width 84 --output_name image_demo_local.hdf5 --exclude-next-obs</span><br></pre></td></tr></table></figure><blockquote><p><strong>如果出现问题</strong>：<code>ileNotFoundError: [Errno 2] No such file or directory: 'patchelf'</code><br><strong>解决方法如下</strong>：<code>sudo apt-get install patchelf</code></p></blockquote><p><strong>（3）提取末端轨迹用于上层规划器的训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/dataset_extract_traj_plans.py --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27;</span><br></pre></td></tr></table></figure><p><strong>（4）检查数据：重新播放数据集中的图像，保存成视频</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/playback_robomimic_dataset.py --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27; --use-obs --render_image_names agentview_image --video_path image_demo_local_replay.mp4</span><br></pre></td></tr></table></figure><h3 id="2-2-自制数据集">2.2 自制数据集</h3><p><strong>（1）使用BDDL文件收集数据</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/collect_playdata.py --bddl-file &#x27;scripts/bddl_files/KITCHEN_SCENE9_playdata.bddl&#x27; --device &#x27;keyboard&#x27;</span><br></pre></td></tr></table></figure><p>收集的原始数据可以在<code>robosuite/robosuite/models/assets/demonstrations/</code>路径下找到。</p><p><strong>（2）将原始数据转换成robomimic格式</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/convert_playdata_to_robomimic_dataset.py --dataset &#x27;path_to_your_data&#x27;</span><br></pre></td></tr></table></figure><p><strong>（3）现在有了robomimic格式的数据，按照#1.3 公共数据集中的步骤生成特定任务的视频提示</strong></p><h2 id="3-数据准备（从人类演示中学习机器人动作）">3 数据准备（从人类演示中学习机器人动作）</h2><h3 id="3-1-配置人手检测模型">3.1 配置人手检测模型</h3><p><strong>（1）配置开源的 hand_object_detector</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda create --name handobj python=3.6</span><br><span class="line">conda activate handobj</span><br><span class="line">conda install pytorch=1.0.1 torchvision cudatoolkit=10.0 -c pytorch</span><br><span class="line">cd mimicplay/scripts/human_playdata_process</span><br><span class="line">git clone https://github.com/ddshan/hand_object_detector &amp;&amp; cd hand_object_detector</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">cd lib</span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></table></figure><p><strong>（2）下载fast_rcnn模型，并放置在指定位置</strong></p><p>从Google Drive中下载<a href="https://drive.google.com/file/d/1H2tWsZkS7tDF8q1-jdjx6V9XrK25EDbE/view">faster_rcnn_1_8_132028.pth (361M)</a>，移动到下面的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd hand_object_detector</span><br><span class="line">mkdir -p models/res101_handobj_100K/pascal_voc</span><br><span class="line">mv faster_rcnn_1_8_132028.pth models/res101_handobj_100K/pascal_voc/.</span><br></pre></td></tr></table></figure><p><strong>（3）将mimicplay的python脚本放入人手检测器的目录下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd mimicplay/scripts/human_playdata_process/</span><br><span class="line">cp demo_mp4.py hand_object_detector/</span><br></pre></td></tr></table></figure><h3 id="3-2-从人类演示生成数据集">3.2 从人类演示生成数据集</h3><p><strong>（1）复制两个示例视频</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp vis_1.mp4 hand_object_detector/</span><br><span class="line">cp vis_2.mp4 hand_object_detector/</span><br></pre></td></tr></table></figure><p><strong>（2）生成hdf5数据文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd hand_object_detector/</span><br><span class="line">python demo_mp4.py</span><br></pre></td></tr></table></figure><p><strong>（3）可视化数据集</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd scripts/human_playdata_process/</span><br><span class="line">python vis_processed_human_play_data.py</span><br></pre></td></tr></table></figure><h2 id="4-训练">4 训练</h2><h3 id="4-1-训练高级规划器">4.1 训练高级规划器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd MimicPlay/mimicplay</span><br><span class="line">python scripts/train.py --config configs/highlevel.json --dataset &#x27;datasets/playdata/image_demo_local.hdf5&#x27;</span><br></pre></td></tr></table></figure><p>训练结束后，选择评估分数最高的checkpoint，将其路径作为</p><h3 id="4-2-训练低级机器人控制器">4.2 训练低级机器人控制器</h3><blockquote><p>参考：</p><ol><li>Mimicplay: Long-horizon imitation learning by watching human play. [Project](<a href="https://mimic-play.github.io/">MimicPlay | Long-Horizon Imitation Learning by Watching Human Play (mimic-play.github.io)</a>) <a href="https://github.com/j96w/MimicPlay">Code</a>, <a href="https://arxiv.org/abs/2302.12422">arXiv</a></li></ol></blockquote>]]></content>
    
    
    <summary type="html">斯坦福大学李飞飞团队的通过观看人类动作进行长期模仿学习方法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>Nerf(instant-ngp)快速实现三维重建</title>
    <link href="https://www.mahaofei.com/post/ce3c8324.html"/>
    <id>https://www.mahaofei.com/post/ce3c8324.html</id>
    <published>2023-10-10T12:59:32.000Z</published>
    <updated>2023-10-10T12:59:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1>搭建环境</h1><p><strong>（1）创建conda环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n nerf-ngp python=3.8</span><br><span class="line">conda activate nerf-ngp</span><br><span class="line">pip install commentjson imageio numpy opencv-python-headless pybind11 pyquaternion scipy tqdm</span><br></pre></td></tr></table></figure><p><strong>（2）下载instant-ngp应用</strong></p><blockquote><p>项目地址：<a href="https://github.com/NVlabs/instant-ngp">https://github.com/NVlabs/instant-ngp</a></p></blockquote><p>快速使用可以下载官方提供的<code>instant-ngp.exe</code>应用，根据自己的显卡版本下载即可：</p><ul><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-RTX-3000-and-4000.zip"><strong>RTX 3000 &amp; 4000 series, RTX A4000–A6000</strong>, and other Ampere &amp; Ada cards</a></li><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-RTX-2000.zip"><strong>RTX 2000 series, Titan RTX, Quadro RTX 4000–8000</strong>, and other Turing cards</a></li><li><a href="https://github.com/NVlabs/instant-ngp/releases/download/continuous/Instant-NGP-for-GTX-1000.zip"><strong>GTX 1000 series, Titan Xp, Quadro P1000–P6000</strong>, and other Pascal cards</a></li></ul><p>（如果链接失效请参考源项目中Installation部分，如果在ubuntu下使用，需要下载源码构建。）</p><p>根据自己的情况，下载完成后解压即可：</p><p><img src="https://img.mahaofei.com/img/202310102108837.png" alt="image.png"></p><p><strong>（3）测试</strong></p><p>打开<code>instant-ngp.exe</code>，将<code>data\nerf\</code>下的<code>fox</code>文件直接拖到窗口中即可</p><p><img src="https://img.mahaofei.com/img/202310102110957.png" alt="image.png"></p><h1>Colmap计算相机位姿</h1><p><strong>（1）录制视频</strong></p><p>对于要三维重建的物体或场景，使用手机录制一段视频。</p><p>尽量均匀扫描，手机不要移动太快或抖动。</p><p><strong>（2）使用Colmap计算相机位姿</strong></p><p>在项目文件夹内新建一个文件夹，将录制的视频放进去。</p><p><img src="https://img.mahaofei.com/img/202310102113654.png" alt="image.png"></p><p><code>cd</code>到视频所在的目录下。在命令行内执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate nerf-ngp</span><br><span class="line">python ..\..\scripts\colmap2nerf.py --video_in desk.mp4 --run_colmap --overwrite</span><br></pre></td></tr></table></figure><p>需要等待较长的一段时间</p><p>完成后会出现分割好的image文件夹</p><p>再继续执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ..\..\scripts\colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 16 --overwrite</span><br></pre></td></tr></table></figure><p>在等待比较长的一段时间，完成。</p><h1>instant-ngp三维重建</h1><p>打开<code>instant-ngp.exe</code>，将desk文件夹整体拖进去就ok了</p><p><img src="https://img.mahaofei.com/img/202310102127121.png" alt="image.png"></p><p>视觉效果还是相当可以的，不过导出mesh模型效果比较差</p>]]></content>
    
    
    <summary type="html">Instant-NGP全称Instant Neural Graphics Primitives，它通过多分辨率哈希编码，解决了NeRF对全连接神经网络进行参数化时的效率问题，大大提升了网络的训练速度，使三维重建速度可以从几小时缩短到几秒钟。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="三维重建" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="三维重建" scheme="https://www.mahaofei.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    <category term="Nerf" scheme="https://www.mahaofei.com/tags/Nerf/"/>
    
  </entry>
  
  <entry>
    <title>Google_Mediapipe关节检测框架</title>
    <link href="https://www.mahaofei.com/post/5090460e.html"/>
    <id>https://www.mahaofei.com/post/5090460e.html</id>
    <published>2023-09-18T06:40:00.000Z</published>
    <updated>2023-09-18T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 准备工作</h1><h2 id="1-1-安装Baze">1.1 安装Baze</h2><p><strong>（1）下载Bazelisk</strong></p><p>MediaPipe使用bazel进行构建的，安装bazellisk主要是为了更新bazel</p><p>进入bazel的项目<a href="https://github.com/bazelbuild/bazelisk/releases">https://github.com/bazelbuild/bazelisk/releases</a>，下载二进制文件<a href="https://github.com/bazelbuild/bazelisk/releases/download/v1.18.0/bazelisk-linux-amd64">bazelisk-linux-amd64</a>。</p><p>然后将文件移动到<code>/usr/local/bin/bazel/</code>，并修改其可执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mv bazelisk-linux-amd64 /usr/local/bin/bazel</span><br><span class="line">sudo chmod u+x /usr/local/bin/bazel</span><br></pre></td></tr></table></figure><p>检查bazel是否安装成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel version</span><br></pre></td></tr></table></figure><p>可以查看到bazel的版本就算成功。</p><p><strong>（2）安装Bazel</strong></p><p>从 <a href="https://github.com/bazelbuild/bazel/releases">GitHub 上的 Bazel 版本页面</a>下载名为 bazel-version-installer-linux-x86_64.sh的shell脚本文件。</p><p>例如我本次安装的就是<code>bazel-6.3.2-installer-linux-x86_64.sh</code></p><p>执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">给bazel-6.3.2-installer-linux-x86_64.sh脚本可执行权限</span></span><br><span class="line">chmod +x bazel-6.3.2-installer-linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载bazel</span></span><br><span class="line">./bazel-6.3.2-installer-linux-x86_64.sh --user</span><br></pre></td></tr></table></figure><p>根据提示，将 <code>source /home/mahaofei/bin/bazel/bin/bazel-complete.bash</code> 添加到 <code>~/.bashrc</code> 中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;source /home/mahaofei/.bazel/bin/bazel-complete.bash&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><p>重启终端，使用<code>bazel --version</code>命令检查是否安装成功。</p><h2 id="1-2-下载Mediapipe">1.2 下载Mediapipe</h2><p><strong>（1）克隆项目</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:google/mediapipe.git</span><br></pre></td></tr></table></figure><p><strong>（2）安装opencv和ffmpeg</strong></p><p>进入克隆的项目中，为<code>setup_opencv.sh</code>添加可执行权限，并运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x setup_opencv.sh</span><br><span class="line">./setup_opencv.sh</span><br></pre></td></tr></table></figure><h1>2 使用</h1><p><strong>（1）python环境</strong></p><p>创建虚拟环境，并安装必要的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mediapipe python=3.8</span><br><span class="line">conda activate mediapipe</span><br><span class="line">pip install opencv-python</span><br><span class="line">pip install opencv-contrib-python</span><br><span class="line">pip install mediapipe</span><br></pre></td></tr></table></figure><p><strong>（2）使用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line">mp_drawing = mp.solutions.drawing_utils</span><br><span class="line">mp_drawing_styles = mp.solutions.drawing_styles</span><br><span class="line">mp_holistic = mp.solutions.holistic</span><br><span class="line"></span><br><span class="line">cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> mp_holistic.Holistic(</span><br><span class="line">    min_detection_confidence=<span class="number">0.5</span>,</span><br><span class="line">    min_tracking_confidence=<span class="number">0.5</span>) <span class="keyword">as</span> holistic:</span><br><span class="line">  <span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    success, image = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> success:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;Ignoring empty camera frame.&quot;</span>)</span><br><span class="line">      <span class="comment"># If loading a video, use &#x27;break&#x27; instead of &#x27;continue&#x27;.</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    image.flags.writeable = <span class="literal">False</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    results = holistic.process(image)</span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line">    image.flags.writeable = <span class="literal">True</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</span><br><span class="line">    mp_drawing.draw_landmarks(</span><br><span class="line">        image,</span><br><span class="line">        results.face_landmarks,</span><br><span class="line">        mp_holistic.FACEMESH_CONTOURS,</span><br><span class="line">        landmark_drawing_spec=<span class="literal">None</span>,</span><br><span class="line">        connection_drawing_spec=mp_drawing_styles</span><br><span class="line">        .get_default_face_mesh_contours_style())</span><br><span class="line">    mp_drawing.draw_landmarks(</span><br><span class="line">        image,</span><br><span class="line">        results.pose_landmarks,</span><br><span class="line">        mp_holistic.POSE_CONNECTIONS,</span><br><span class="line">        landmark_drawing_spec=mp_drawing_styles</span><br><span class="line">        .get_default_pose_landmarks_style())</span><br><span class="line"></span><br><span class="line">    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)</span><br><span class="line">    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#右手21个节点坐标</span></span><br><span class="line">    <span class="keyword">if</span> results.right_hand_landmarks:</span><br><span class="line">        <span class="keyword">for</span> index, landmarks  <span class="keyword">in</span> <span class="built_in">enumerate</span>(results.right_hand_landmarks.landmark):</span><br><span class="line">            <span class="built_in">print</span>(index,landmarks )</span><br><span class="line"><span class="comment">#鼻子坐标</span></span><br><span class="line">    <span class="comment">#print(results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE])</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;MediaPipe Holistic&#x27;</span>, cv2.flip(image, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">5</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">cap.release()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Google所开发的通用框架。</summary>
    
    
    
    <category term="程序设计" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/"/>
    
    <category term="实例分割" scheme="https://www.mahaofei.com/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/%E8%A7%86%E8%A7%89/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="关节检测" scheme="https://www.mahaofei.com/tags/%E5%85%B3%E8%8A%82%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【模仿动作】从人类演示中学习机器人动作规划方法</title>
    <link href="https://www.mahaofei.com/post/d76756ed.html"/>
    <id>https://www.mahaofei.com/post/d76756ed.html</id>
    <published>2023-09-07T06:41:50.000Z</published>
    <updated>2023-09-07T06:41:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 MimicPlay: Long-Horizon Imitation Learning by Watching Human Play</h1><blockquote><p><strong>标题</strong>：模拟游戏：通过观看人类游戏进行的长期模拟学习<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://mimic-play.github.io/">https://mimic-play.github.io/</a>(code is coming soon)</p></blockquote><h2 id="1-1-目标问题-5">1.1 目标问题</h2><p>由于人类比遥控机器人能更快的完成长时间任务，因此启发从人类演示中学习机器人规划策略。</p><p>为了减少数据需求，采用人类与环境的交互视频作为数据。开发一个分层学习框架，从人类数据中学习潜在的规划控制方法。</p><h2 id="1-2-方法-4">1.2 方法</h2><p><img src="https://img.mahaofei.com/img/202309071622980.png" alt="image.png"></p><p><strong>（1）从人类数据中学习潜在规划</strong></p><p>给定输入：视觉观察$o_t$，未来的目标图像$g_t$，当前手部位置$l_t$<br>训练过程中，$g_t$被视为执行动作后的未来帧<br>规划期的目标是根据视频提示V生成目标图像的动作规划。</p><ol><li>人类演示数据收集</li><li>跟踪人手三维轨迹：使用双目相机获取人手的3D轨迹，利用现成的<a href="https://github.com/ddshan/hand_object_detector">手部检测器</a>确定2维图像中的手部位置，然后利用双目视图重建手的3D轨迹。</li><li>学习潜在规划：使用两个卷积网络分别将当前图像和目标图像处理为低维特征，再与手部位置连接在一起，使用MLP处理为潜在规划特征。生成3D手部运动轨迹。为了处理同一个任务的不同方式的实现，使用高斯混合模型对潜在规划的轨迹分布进行建模。</li></ol><p><strong>（2）计划引导的多任务模仿学习</strong></p><p>机器人的底层策略使用行为克隆算法进行训练，使用通过遥操作收集的机器人演示数据。</p><ol><li>视频条件下的潜在规划生成：使用遥操作机器人任务视频来提示训练时潜在规划器生成相应的规划。</li><li>基于Transformer的规划引导模仿：将机器人手上相机观察和本体姿态信息处理为低维向量，再与潜在计划连接起来，通过Transformer架构来计算最终的机器人控制命令。</li><li>多任务学习</li></ol><h2 id="1-3-思考-4">1.3 思考</h2><p>李飞飞团队的作品，从视频中学习人手的运动轨迹，code is coming soon，等待后续再细看。</p><h1>2 One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</h1><blockquote><p><strong>标题</strong>：通过领域自适应元学习观察人类的一次性模仿<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<br><a href="https://github.com/tianheyu927/mil">官方版: https://github.com/tianheyu927/mil</a><br><a href="https://github.com/daiyk/daml_pytorch">Pytorch版: https://github.com/daiyk/daml_pytorch</a></p></blockquote><h2 id="2-1-目标问题-2">2.1 目标问题</h2><p>提出一种从人类视频中进行学习的方法，通过使用各种先前任务的人类和机器人演示数据，使机器人执行人类演示的任务。</p><h2 id="2-2-方法-2">2.2 方法</h2><p><strong>（1）问题描述</strong></p><p>将先验知识和少量证据组合起来，形成一个人类演示的形式。从中推断出完成任务的机器人的策略参数。</p><p><strong>（2）领域自适应元学习</strong></p><p>能够处理从人类的视频演示中学习，学习一组参数，以便在人类演示的基础上进行梯度下降后，模型可以有效地执行新任务。</p><p>由于人类和机器人的动作无法直接对应，因此考虑学习只对策略激活起作用。</p><p><strong>（3）学习时间适应目标</strong></p><p>要从人类的视频中进行学习，需要捕获视频中的相关信息，例如人类的意图和任务的相关对象。要确定哪些行为正在被演示，哪些对象是相关的，通常需要同时检查多个帧来确定人类的运动。因此本文的学习适应目标将多个时间步长耦合，从多个时间步骤对策略进行操作。</p><p>此处使用卷积网络来表示自适应目标，使用递归神经网络LSTM进行时间卷积。</p><p><strong>（4）概率解释</strong></p><p>将学习到的自适应目标纳入到概率图模型的框架中，推断特定任务的策略参数。</p><h2 id="2-3-思考">2.3 思考</h2><p>思路看起来很可以，就是数学推理比较复杂，很难看得懂。</p><h1>3 Waypoint-Based Imitation Learning for Robotic Manipulation</h1><blockquote><p><strong>标题</strong>：基于航路点的机器人操纵模拟学习<br><strong>作者团队</strong>：斯坦福大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></p></blockquote><p>行为克隆BC目前有很多问题，路径点可以通过减少BC的范围来解决这个问题，但是传统路径点需要人工监督标注。</p><p>本文提出了线性运动近似的，模仿学习的自动轨迹点提取模块，将演示分解为一组轨迹点，进行线性插值，近似实现演示动作。</p><p>并且该方法可以与任务BC算法相结合，提高其成功率。</p><h1>4 Building Robot Intelligence by Scaling Human Supervision</h1><blockquote><p><strong>标题</strong>：通过扩展人类监督构建机器人智能<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2021</p></blockquote><h2 id="4-1-研究背景">4.1 研究背景</h2><p><strong>几十年来，我们一直在想象一个机器人可以充当个人助理的世界，能够完成我们每天做的各种任务和家务，比如做饭、打扫卫生、洗衣，甚至组装橱柜。机器人领域的研究人员一直致力于实现这一梦想。然而，不幸的是，今天的自主机器人远未达到操纵能力的水平。尽管研究在使机器人能够完全自主地完成特定任务的方面取得了令人印象深刻的进展，包括拾取物体，或将它们堆叠在一起。但机器人和人类的操作能力之间存在很大差距。人类智能地使用物体，并在日常生活中以丰富的方式与它们互动，比如当我们用刀切菜做饭时，或者用螺丝刀拧紧螺丝组装橱柜时。这种有目的的与物体的互动对机器人来说是十分困难的。</strong></p><p><strong>作为人类，我们在一生中积累了一系列不同的先前经验，这些经验我们可以在日常生活中借鉴。此外，即使我们不知道如何做某事，我们也可以通过观看其他人的视频来快速学习，例如通过观看YouTube上其他人组装橱柜的视频来学习如何组装橱柜。这就提出了一个问题——我们是否可以类似地为机器人提供丰富多样的先前经验，并使他们能够从这些数据集中学习操作技能？</strong></p><p><strong>这激发了数据驱动的机器人，这是一种有用的范式，让机器人从大型数据集中学习操作。但是这种方法通常有两种变体，第一种是机器人自行收集数据，数据一开始是随机的，但会随着时间推移慢慢变好。由于机器人必须自己学习，限制了可以学习的人物的复杂性。第二种则是人类控制机器人并引导它完成任务，然是这通常是不可扩展的，因此可以收集的数据量很小，这再次限制了任务的复杂性。</strong></p><p><strong>相比之下，计算机数据和自然语言处理等领域已经通过大规模高质量数据集开创了前所未有的成就，我们希望在机器人技术方面看到类似的突破。</strong></p><p><strong>为了复制这一成功经验，并解决数据驱动机器人中任务复杂性有限的问题，我们需要解决两个关键挑战。首先，收集大规模的人类数据具有挑战性。在计算机视觉领域，注释可以由人类直接标注，很容易实现并行标注和大规模人员标注。相比之下，在机器人技术中，人类必须与机器人实时互动，引导机器人完成任务。这使得提供直观和可扩展的方法来收集来自多个人的数据变得很困难。其次，从大规模数据集中学习可能并不简单。在其他领域，我们可以训练网络预测注释，这些注释对应的都是真实的标签。然而在机器人技术中，没有一种真正的方法来执行任务，不同的人可能会收集不同的轨迹，不同的策略，我们需要确定如何从这些数据集中学习。</strong></p><h2 id="4-2-研究目标">4.2 研究目标</h2><p>第一部分，讨论了如何通过充满丰富交互的人类监督来收集大规模数据，这些数据体现了机器人的类人操作能力。包括一个为解决机器人操作中对大规模人类数据集需求构建的平台，和现实世界的数据收集。</p><p>第二部分，讨论了如何使用丰富的数据集来学习机器人操作技能。</p><p>第三部分，讨论了该方法可能的进一步拓展和应用。</p><h2 id="4-3-收集人类操作数据">4.3 收集人类操作数据</h2><p><strong>为了使数据能够捕捉人类的操作，首先数据应该在所展示的解决问题的策略的种类上是多样化的。作为人类，我们很清楚什么时候应该尝试不同的方法类实现目标，而机器人应该从所有这些策略中学习，因为在特定的情况下可能需要其中的一种。其次，数据应该包含灵巧的操作，我们希望我们的机器人了解它们如何通过武力方式操作物体来实现预期的结果。最后，数据应该是大规模的，人类非常擅长在无数情况下解决问题，但机器人还不能做到这一点。我们向他们展示的数据越多，他们也就越有可能获得这种能力。</strong></p><p>在这一部分，我们提出了RoboTurk平台，一个数据收集平台，允许人类实时远程操作机器人。操作员在他们的网络浏览器中看到机器人的工作空间的视频流，用他们的智能手机控制机械臂，他们手机的运动与机器人的运动相耦合，可以自然地控制手臂，这使得人们可以轻松的提供任务演示，连接的过程快速而简单。实验表明，这些数据能够在多步骤操作任务上进行策略学习，并且在策略学习的过程中使用大量的演示可以在学习一致性和最终性能方面带来好处。</p><h2 id="4-4-从大规模人类数据集中学习操作">4.4 从大规模人类数据集中学习操作</h2><p><strong>在这一部分，我们讨论了机器人如何才能够大规模人类数据集中学习操作技能。此类数据集可能表现出巨大的多样性，并由次优解决方案组成，因此从中学习具有挑战性。我们提出了一种从大规模演示数据集中学习的新算法，即无规模交互的内隐强化IRIS算法。IRIS将控制问题分解为目标条件的低级控制器和高级目标选择机制，前者模仿短演示序列，后者为低级控制器设置目标，并选择性的组合部分次优解决方案，从而更成功的完成任务。</strong></p><p>尽管最近在模仿学习和强化学习方面缺乏开源的人类数据集和可重复的学习方法，使得评估该领域的状态变得困难。我们对六个离线学习机器人操作算法，在五个仿真和三个不同复杂度的真实环境中进行多阶段操作任务测试。我们得到了一系列经验，包括对不同算法设计选择的敏感性，对演示质量的依赖性，以及由于训练不同的目标而导致的不同停止标准。我们还强调了从人类数据集学习的可能性，例如在当前强化学习方法范围之外的具有挑战性的多阶段任务中学习熟练策略的能力，以及轻松扩展到只有原始感官信号可用的自然、真实世界操作场景的能力。我们已经开源了我们的数据集和所有算法实现，以促进未来的研究和从人类演示数据中学习的公平比较。</p><h2 id="4-5-使用人类数据集构建能力更强的机器人">4.5 使用人类数据集构建能力更强的机器人</h2><p>这一部分探讨了几个不同的应用程序，使我们更接近于我们希望的机器人能够在未来能够处理的任务。主要探讨了多任务领域（如厨房），高精度操作，和需要协作的多臂操作任务。</p><p><strong>模仿学习方法的一个常见的局限是由于训练集中的数据有限，在所展示的行为之外进行泛化是一个开放的挑战。例如在厨房场景中，我们可能希望机器人实现多种可能的配置，具有多个要操作和交互的对象，如食物、出轨、微波炉、水槽等。本章我们介绍了通过模仿进行任务泛化，这是一种新颖的模仿学习框架，使机器人能够从少量的人类演示中有效的学习复杂的现实世界操作任务。合成收集的演示中未包含的新行为。多任务领域通常呈现出一种潜在的结构，不同的任务轨迹在状态空间的公共区域相交。GTI是一个两阶段在线模仿学习算法，该算法利用交叉结构来训练目标导向的策略，这些测类推广到看不见的开始和目标状态组合。在GTI的第一阶段，我们训练了一个随机策略，该策略利用轨迹交叉点来有能力从不同的演示轨迹中组合行为在一起。在GTI的第二阶段，我们从第一阶段的无条件随机策略中收集了一小组推理，并训练一个目标导向的agent来推广到新的启动和目标配置。我们在模拟领域和现实世界中具有挑战性的长期机器人操作领域中验证了GTI。</strong></p><p><strong>模仿学习方法通常也很难完成高精度的操作任务，因为它们需要一系列精确的动作才能取得有意义的进展，比如机器人将pod插入咖啡机制作咖啡。经过培训的策略可能会在这些场景失败，因为行动上的微小偏差可能会导致策略进入未被演示覆盖的区域。基于干预的策略学习是解决这一问题的一种替代方案——它允许操作员监控经过训练的策略，并在遇到故障时接管控制权。</strong> 我们扩展了RoboTurk，使远程操作员能够监控和干预经过培训的政策。我们开发了一个简单的在系统收集的新数据上迭代训练策略的有效算法。我们证明，根据我们基于干预的系统和算法收集的数据训练的代理优于根据非干预演示者收集的同等数量样本训练的代理，并进一步证明，我们的方法在从具有挑战性的机器人线程任务和咖啡制作任务。</p><p><strong>最后，虽然通过远程操作收集的人类演示中的模仿学习（IL）是教授机器人操作技能的强大范式，但它大多局限于单臂操作。然而，许多现实世界中的任务需要多个手臂，例如举起重物或组装桌子。不幸的是，将IL应用于多臂操作任务一直具有挑战性</strong>——要求人类控制多个机械臂可能会带来巨大的认知负担，而且通常最多只能控制两个机械臂。为了应对这些挑战，我们介绍了多臂RoboTurk（MART），这是一个多用户数据收集平台，允许多个远程用户同时远程操作一组机械臂，并收集多臂任务的演示。使用MART，我们从几个地理位置不同的用户那里收集了五个新的双臂和三臂任务的演示。我们表明，从这些数据中学习因此给集中式代理带来了挑战，这些代理直接尝试同时对所有机器人动作进行建模，并对数据进行全面不同的策略架构，对我们的任务具有不同的集中程度。最后，<strong>我们提出并评估了一个基本残差策略框架，该框架允许经过训练的策略更好地适应多臂操作中常见的混合协调设置，并表明用去中心化残差模型增强的集中式策略在我们的基准任务集上优于所有其他模型。</strong></p><h1>5 Understanding and Learning Robotic Manipulation Skills From Humans</h1><blockquote><p><strong>标题</strong>：从人类身上理解和学习机器人操作技能<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="5-1-背景和动机">5.1 背景和动机</h2><p>制造机器人的性能是通过它们的精度、准确性和速度来衡量的。这导致了刚性和笨重的机器人的设计，这些机器人与人类一起工作是不安全的。他们的控制器在不使用力传感的情况下执行预先编程的轨迹，使其对位置误差高度敏感。通过使用夹具和夹具，例如装配线上的夹具，可以减少环境中的不确定性。</p><p>现实世界的环境需要低重量、人类安全、扭矩控制的机器人。<strong>如果机器人要在环境不断变化、感知能力有限的日常环境中真正发挥作用，就必须找到通过预编程轨迹控制机器人的替代方案。一种很有前途的方法是将复杂的任务划分为健壮且可重用的动作或基元。</strong> 在本文中，我们通过使用可推广的顺应原语，为在更高抽象级别上编程机器人奠定了理论和实践基础。</p><p><strong>方法的第一步是从人类演示中收集数据。然后，我们将数据分割成在任务期间执行的动作序列——基元。接下来，我们将数据投影到一个低维和物理意义的空间中，使我们能够理解人类的策略。最后，我们将这些行为编码到能够执行任务的机器人控制器上。</strong> 此外，我们的框架利用视觉和触觉反馈，让人类处于故障恢复和持续学习的循环中。</p><h2 id="5-2-从人类演示中学习">5.2 从人类演示中学习</h2><p><strong>本文的工作属于示范学习LfD的范畴。人类在操作方面非常有能力，因此从人类演示中收集数据使学习机器人新行为的一种流行方式。事实上，我们不仅可以学习单臂行为，还可以学习双臂行为，我们的系统已经证明了这一点，并在其它工作中进行了探索。</strong> 大多数先前的工作侧重于从视觉数据中学习。而我们的工作强调在执行富含接触的任务时里和数据的重要性。</p><p>近年来，互动学习是一个不断发展的研究领域，它使人类保持在循环学习的过程中。为了实现类似的工作方式，我们的框架通过使用触觉接口使人类处于循环中，我们系统手机故障恢复数据可以与从故障中学习的工作相结合，易产生更稳健的自主行为。</p><h2 id="5-3-机器人基本单元">5.3 机器人基本单元</h2><p>在这项研究中，基元是有一个兼容的框架和一组所需的任务参数定义的。顺应性框架是一个原点和空间中的三个方向，我们沿着它们控制运动和顺应性。柔顺框架附着到要操纵的对象上。任务参数包括所需的力、力矩、位置和方向。这种与机器人无关的任务规范提供了一种有物理意义的低维表示。</p><p><strong>基元库。生成一个由n个基元组成的库，对基本的操作技能进行编码，通过组合这些基元，可以以一种方式解决新的复杂任务，即所需基元的数量不会随着任务数量的增加而增加。广义上讲，关于运动基元的文献主要解决了三个主要的研究问题，生成运动基元，参数化基元以及将基元组合在一起以成功完成任务。</strong></p><p><strong>基元生成。基元的生成可以通过手动编码所需策略或者从数据中自动提取策略来实现。先前的研究已经转向人类寻求灵感，并试图提取策略。</strong></p><p><strong>基元参数化。参数化基元处理定义动作的方式。基元通常使用轨迹段进行参数化。用轨迹定义运动基元已经被证明是成功的，但该方法假设环境不确定性较低。</strong></p><p>兼容基元。我们使用框架的概念来参数化我们的原始控制器。先前的研究使用了以对象为中心的任务控制器的相同概念。然而，与本文中的工作相反，仅从视觉数据中提取控制器参数，我们认为，在处理复杂任务是，考虑序列数据是有利的。在存在位置不确定性的情况下，顺从性在任务中也起着重要作用，对于接触丰富的任务，比如抓获或本文中研究的任务。例如，基元的概念，其中柔顺基元就是用于实现对小物体的鲁棒抓取。</p><p><strong>使用基元进行规划。组合运动基元的概率方法利用了决策过程的固有不确定性，这些方法可以是完全自动化的，也可以是使用混合的方法，将自动决策算法与用户指定的图像相结合。</strong> 最近的其他方法使用语义模型来学习基于是觉得操纵任务计划，或者一些方法使用接触而不是视觉来指导决策过程。</p><h2 id="5-4-多层控制体系结构">5.4 多层控制体系结构</h2><p>该体系结构由三层感知-动作反馈回路组成。每个层都以不同的抽象级别运行，并以不同的频率运行。</p><p>在最底层，完全依赖于控制器，并有助于高速率感官反馈和控制的集成，以实现安全和可预测的机器人运动。该级别向机器人电机发送命令，因此，感知动作回路必须以非常高的频率闭环。下一层向全身控制器提供输入，从而以较慢的速率运行。最后，执行计算成本高昂的感知和规划的最高抽象级别以最低的速率运行。</p><p>全身控制级别使用任务优先级。基于优先级的控制使我们在设计原始动作时能够专注于对象及其几何约束。完整的机器人行为可以被视为由具有不同优先级的不同任务组成。例如，高优先级任务可以是避免奇异配置，另一个任务可以处理障碍或摩擦约束。以类似的方式，有一项任务专门负责实现操纵对象之间所需的几何关系。此任务被编码为基元。换句话说，基本动作只涉及对象，因为任务的所有其他方面，包括非几何约束和机器人运动学，都由控制器的其他组件处理。</p><p>基于优先级的全身控制使用零空间投影来确保满足所有不同的约束。此外，操作空间公式——使用Jacobian的动态解耦逆来计算递归零空间投影——确保具有不同优先级的任务动态解耦。先前的工作也在操作原语的上下文中使用了这种分层框架。</p><h1>6 Scaling Deep Robotic Learning to Broad Real-World Data</h1><blockquote><p><strong>标题</strong>：将深度机器人学习扩展到广泛的真实世界数据<br><strong>作者团队</strong>：Stanford University<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="6-1-背景">6.1 背景</h2><p>机器人的一个长期梦想是一种通用的家用机器人，它可以被放置在家庭环境中，也许是它以前从未见过的，并执行一系列有用的任务，如煮咖啡、清洁和烹饪。这样一个机器人无疑将在经济上和通过他们的帮助提高人类生活质量方面产生巨大影响。当然，这个梦想仍然是这样，在实现这个目标方面存在着无数的挑战，包括更好的机器人硬件、电池技术和传感。然而，核心挑战之一在于泛化，即机器人在新的物体、环境和任务中取得成功的能力。事实上，人类有这种能力，正是这种能力使我们能够完成像煮一杯咖啡这样的任务，即使在有新厨房和新物体的情况下也是如此。因此，相关的问题仍然存在——我们如何训练我们的机器人，使其能够广泛推广。</p><p>解决这个问题的一种方法是利用人类的直觉，以及用于机器人规划和控制的手部设计系统和表示。在这种方式中，人类定义了相关的对象类及其属性和关系（例如，颜色、形状、姿势、上方与下方等），然后可以使用状态估计技术从传感器观测中测量这些量，并且可以使用经典的搜索和运动规划方法来执行任务。然而，至关重要的是，这种方法是基于人类对相关对象、特性的规范，在某些情况下，甚至是每个环境和任务的对象的3D模型，这阻碍了这种方法在存在新对象和环境的情况下容易使用。</p><p>有一项工作研究了机器人如何在制造通用机器人时不依赖人类的直觉和手部设计，而是纯粹通过数据和自己的试错来学习行为。具体来说，深度强化学习研究了学习深度神经网络策略的问题，该策略在给定传感器观测的情况下采取行动，从而使策略通过从交互中学习来最大化一些定义的奖励。原则上，这种方法可以让机器人完全靠自己学习技能，而只有少量的成功指标。然而，在实践中，在机器人上运行深度强化学习带来了许多挑战，例如在重置和奖励方面需要人工监督。然而，最关键的是，深度强化学习通常需要在目标环境中进行数百万次在线环境交互才能进行学习，并且一旦完成，所学习的策略只对所训练的环境和数据有效。因此，标准的深度强化学习在消除了人手设计的大部分需求的同时，仍然没有立即让我们更接近能够在新环境和任务中操作的机器人。</p><p>退一步看，人们可能会从机器学习的其他领域寻找灵感，特别是过去几年里，自然语言处理和计算机视觉的研究领域取得了巨大进展。主要基于一个简单的配方：：（1）大量、多样化的离线数据集，（2）自监督或廉价监督的训练目标，以及（3）表达性的端到端训练的神经网络模型。这种基础模型的范例特别令人兴奋，因为这些模型表现出了令人印象深刻的泛化——例如，来自ImageNet的视觉模型可以适应癌症检测这样的全新任务，而像BERT这样的预训练语言模型的应用范围从医学编码到视觉问答。事实上，这种概括水平正是我们希望在一个通用机器人中看到的，它可以被放入一个新的环境中，并快速地学会解决新的任务。</p><p>所以为什么这个配方还没有在机器人中实现呢？现实世界中的机器人操作带来了许多独特的挑战，这使得直接复制这一配方变得困难——我们既没有足够大和多样化的机器人交互数据集，也不清楚什么类型的学习算法或监督来源可以使我们从这些数据集中大规模学习有用的技能。本文的目标在于解决这些挑战，并在机器人操作的背景下复制大规模数据和学习的配方。具体来说，我的研究集中在回答三个广泛的问题上。首先，我们如何可伸缩地收集在物理世界中交互的机器人的大型和多样化的数据集？其次，我们如何设计能够消耗如此广泛的离线数据的自我监督强化学习算法，这些数据可能来自非专家，缺乏奖励标签，并从中学习达到看不见的目标？第三，我们如何解锁网络上存在的广泛数据来源，如人类视频和自然语言，以便在我们的机器人中进行更有效的学习？</p><h1>7 Learning Perceptual Prediction: Learning From Humans and Reasoning About Objects</h1><blockquote><p><strong>标题</strong>：学习感知预测：向人类学习和对物体的推理<br><strong>作者团队</strong>：University of Pennsylvania<br><strong>期刊会议</strong>：Thesis<br><strong>时间</strong>：2023<br><strong>代码</strong>：</p></blockquote><h2 id="7-1-目标问题-2">7.1 目标问题</h2><p>人类在使用各种各样的感知模式进行预测时，主要关注从视觉学习。人类的视觉似乎经过了高度的优化，可以用于预测未来的视觉观测。</p><p>研究使用视觉传感器的预测也提供了许多实际优势。首先，高质量的相机很容易获得，并且尺寸、重量和功率要求都很低，这使得它们可以被包括在大多数机器人平台上，由于相机在非机器人应用中的普及，它们已经被商品化了。其次，视觉观察提供了关于环境的丰富信息，包括姿势、纹理和语义，这些信息是其他传感器无法轻易匹配的。获取大量丰富的世界信息对于使代理人能够与世界互动非常重要。</p><p>当前学习动作条件视觉预测模型的方法依赖于访问大量的具体数据，这是昂贵且耗时的，从而阻止了基于视觉预测的方法在许多应用中使用。对于机器人来说尤其如此，因为收集大量机器人数据既昂贵又耗时，而且可能不安全。现有工作表明，基于视觉预测的方法随着数据量的增加而扩展良好，因此找到新的数据来源对于使这些模型能够广泛使用至关重要。</p><p>在这篇论文中，我提出了三种不同的方法来利用非机器人数据来改进视觉预测和机器人控制。在前两项工作中，我使用人类数据来提高机器人的性能，而在第三项工作中我使用现有的非机器人数据集来实现以对象为中心的预测框架。</p><h2 id="7-2-从人类学习">7.2 从人类学习</h2><p>大型和多样化的真实世界数据集对于广泛的泛化和高性能至关重要。大型数据集可以通过自动化管道或人类远程操作进行收集。自动化数据收集过程可以收集非常大的数据集，但在到达环境中感兴趣的部分以及需要与环境进行大量交互方面存在问题。习得的探索策略可以提高代理达到有趣配置的能力，但这些方法仍然需要大量的探索。第二种方法是收集人类演示的远程操作轨迹。这种方法允许数据集轻松地达到有趣的和任务相关的配置。然而，它受到了影响，因为它依赖于人类来操作机器人，这需要训练有素的操作员，而且很快变得非常昂贵。通过从人类学习中汲取灵感，可以找到一种避免这两种方法困难的替代方法。</p><p>人类不仅有能力从自己与世界的互动中学习技能，也有能力通过观察他人来学习技能。考虑一个婴儿学习使用工具。为了成功地使用一个工具，它需要学习该工具如何与其他对象交互，以及如何移动工具来触发这种交互。这种直观的物理概念可以通过观察成年人如何使用工具来学习。更普遍地说，观察是关于世界以及行动如何导致结果的强大信息来源。然而，在存在身体差异的情况下（例如成人身体和婴儿身体之间），利用观察是具有挑战性的，因为演示者和观察者的行为之间没有直接对应关系。来自神经科学的证据表明，人类可以有效地推断出这种对应关系，并利用它们从观察中学习。</p><p>利用对人类的观察提供了大幅增加可用数据的规模和有用性的机会。与自主收集的数据不同，人类数据可以只关注配置空间中有趣的部分，避免危险或无聊的交互。与通过远程操作收集的数据集不同，人类数据集可以具有更大的规模。公开可用的人类数据集，如Ego4D或SomethingSomething，包含数十万个视频和数千小时的镜头，分布在数百个任务和数十个地点。这些数据集与自主收集的数据集的大小相当]，并比通过远程操作收集的数据集中的大小高出一到两个数量级。更重要的是，从人类的无行动观察中学习，开启了从互联网上公开生成的视频中学习的可能性，比如YouTube上的视频，这些视频提供了更多数量级的数据。目前的方法只能将这些数据的有限子集用于特定任务，但我们的工作为更广泛的利用提供了一步。</p><p>在这篇论文中，我们考虑了这样一个问题：主体能否学会利用自己的互动和其他主体的被动观察来解决任务？我们在两个环境中探讨了这个问题，第一个是学习动作条件视觉预测模型，第二个是端到端强化学习策略。</p><p>在第3章中，我们提出了一种使用人类的无动作数据和主体自己的探索来执行强化学习的方法。我们提出了克服野外人类数据和模拟机器人数据之间的域转换的方法，将动作添加到无动作的人类数据中，以及估计人类数据的奖励的方法。通过利用在现实世界中收集的人类视频，我们能够加快模拟机器人代理的学习速度。</p><h2 id="7-3-对象推理">7.3 对象推理</h2><p>虽然从人类观察中学习可以获得大量数据，但它并不能回答应该学习什么的问题。我们重点学习端到端模型，这些模型直接从传感器输入映射到预测的未来帧或内部结构很少的期望动作。通过为任务选择正确的归纳偏差集，并利用在非机器人数据上预训练的现有模型，我们应该能够用更少的数据训练我们的模型，并实现更高的性能。正确设计学习问题的结构也可以使模型更容易地用于下游任务。我们关注的是假设世界是由物体组成的简单归纳偏见。</p><p>大多数用于操纵的动态交互可以通过将场景分解为对象来建模。虽然有些材料，如液体或颗粒介质，不容易被表示为对象，但大多数操作任务都涉及操作离散对象。分拣箱子、重组房间，甚至喝杯咖啡，这些任务主要由与离散对象的交互控制。</p><p>以对象为中心的预测模型在预测和困难任务方面表现出了成功的性能。通过将场景分解为离散对象，这些预测模型可以在更长的时间范围内保持每个对象的内聚性。此外，通过将世界状态内部表示为对象集合，以对象为中心的预测模型可以轻松地与规划者对接，并提供一个非常可解释的界面，有助于调试和验证。</p><h1>8 Affordances from Human Videos as a Versatile Representation for Robotics</h1><blockquote><p><strong>标题</strong>：人类视频作为机器人的通用表示<br><strong>作者团队</strong>：CMU, Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://robo-affordances.github.io/">https://robo-affordances.github.io/</a></p></blockquote><h2 id="8-1-目标问题-2">8.1 目标问题</h2><p>从人类视频中学习可操作的动作表示，该模型在未来帧的监督下预测接触点和轨迹路径点。</p><p>论文主要关注三个问题：</p><ol><li>如何表示可操作性？</li><li>如何以数据驱动和可扩展的方式学习这种表示？</li><li>如何实现跨机器人的视觉启发的方法部署？</li></ol><p>对应这三个问题，本文提出了以下三种观点：</p><ol><li>接触点和接触之后的轨迹是比较好的机器人操作的表示方法；</li><li>利用了自我中心的数据集，聚焦于所有有人类的帧，来预测接触点和接触之后的轨迹，通过使用现成的工具来估计自我运动、人体姿势和手-物体交互；</li><li>实现了一种称为Vision-Robotics Bridge(VRB)的方法，实现这些功能与不同类型机器人的无缝集成。</li></ol><h2 id="8-2-方法-2">8.2 方法</h2><p><strong>（1）可操作性表示</strong></p><p>提取人类启示的最自然的方式是观察人们如何与世界互动。常规的思路是从视频中准确模拟人类的运动，但这导致了一个以人为中心的模型，很不容易推广，因为人类的形态和机器人完全不同。</p><p>因此本文采用机器人需求驱动的第一性原理，机器人的本体的信息通常是已知的，因此使用运动规划达到3D空间中的点是很容易实现的，关键难点在于与环境的互动位置在哪里，以及接触之后如何移动。</p><p>受此启发，采用接触点 $c$ 和接触之后的轨迹 $\tau$ 作为视觉启发的简单操作表示，可以很容易的传递给机器人。其中 $\tau=f(I_t,h_t)$，$I_t$ 是时间步长 $t$ 的图像，$h_t$ 是像素空间中人手位置。</p><p><strong>（2）从自我中心的视频中学习操作</strong></p><p>接下来的问题是如何处理视觉输入的人体或手，从人类视频中提取接触点 $c$ 和轨迹 $\tau$。</p><ol><li>从人类视频中提取操作</li></ol><p>对于给定的视频 $V$，例如人开门，使用现有的手部对象检测模型，对每一帧图像 $I_t$ 生成手的2D边界框和离散接触变量 $o_t$，使用这些信息，我们可以过滤每个图像中 $o_t$ 表示接触的帧，从而找到发生<strong>接触的第一个时间步长</strong> $t_{contanct}$。</p><p><strong>手的像素空间位置构成了接触之后的轨迹</strong> $\tau$，为了提取接触点，我们使用手边界框以及颜色分割来找到手与其他物体边界框相交的点，利用高斯混合模型拟合这些接触点。</p><p>同时要考虑，一个人打开门的时候，人手不仅会移动，<strong>相机也会移动</strong>，需要补偿相机的运动，使用但应矩阵来解决这一问题，通过匹配连续帧之间的特征来获得单应性矩阵，产生变换后的轨迹。</p><p>需要完成<strong>视觉的转移</strong>，即训练视频中包含人手，但机器人任务中的视角不会有，因此考虑将所有的可操作性映射回第一帧，即人类还没有进入场景时。如果人总在视频中，要么将人裁剪出去，要么丢弃。</p><ol start="2"><li>训练模型</li></ol><p>以输入图像为条件，训练模型预测接触点和接触后的轨迹。然而由于学习的任务是多模态的，比如人从桌子上拿起杯子可能是要喝水或者倒到其他地方，因此考虑建立空间概率分布，预测多个heatmap处理这一问题。</p><p>输入图像使用ResNet进行编码，给出潜在空间表示，然后使用卷积层将这个潜在表示投影到K个概率分割中，得到GMM均值的标签的估计。</p><p><img src="https://img.mahaofei.com/img/202311011426427.png" alt="image.png"></p><p>为了估计接触后的轨迹，本文使用基于Transformer的预测。给定场景中，人类可能与许多对象进行交互，这些对象可能不存在在训练数据中，因此我们通过对接触点周围进行采样来解决，实现更好的泛化。</p><p><strong>（3）机器人学习</strong></p><p>本文用于引导现有的机器人学习方法，考虑了四种不同的机器人模式。</p><p><img src="https://img.mahaofei.com/img/202311011430303.png" alt="image.png"></p><ol><li>离线数据采集中的模仿学习</li></ol><p>给定一个图像输入，模型产生接触点和轨迹，我们将这一组数据存储在数据集中。收集到足够的数据后，我们使用模仿学习来控制策略，实现特定的任务。</p><ol start="2"><li>自由奖励的探索</li></ol><p>目标是发现尽可能多的不同技能，然而现实中从头开始探索效率太低了，因为机器人可能会花费大量时间尝试探索，但仍然无法学习有意义的技能来解决人类想要的任务。</p><p>我们考虑先收集数据，然后对所有轨迹进行排序。对于后续的数据采集从高度探索性的轨迹开始进行引导，进一步探索。</p><ol start="3"><li>目标条件的学习</li></ol><p>利用已知的知识，例如打开的门的图像，监督其进行探索学习。</p><ol start="4"><li>可操作性作为动作空间</li></ol><p>将机器人在连续空间中的操作，以空间的方式进行参数化，为每个位置分配一个基元。通过学习获得大量预测，利用GMM拟合到这些轨迹点上，获得离散的接触点和轨迹，机器人只需要在这个空间上进行搜索。</p>]]></content>
    
    
    <summary type="html">从人类抓取物体的视频中学习机器人的动作生成与规划方法，相关的研究的调研。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
    <category term="机器人动作" scheme="https://www.mahaofei.com/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【模仿抓取】从人类演示中学习机械臂抓取</title>
    <link href="https://www.mahaofei.com/post/f9be0f4e.html"/>
    <id>https://www.mahaofei.com/post/f9be0f4e.html</id>
    <published>2023-08-13T02:50:01.000Z</published>
    <updated>2023-08-13T02:50:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1 DemoGrasp: Few-Shot Learning for Robotic Grasping with Human Demonstration</h1><blockquote><p><strong>标题</strong>：DemoGrasp: 机器人抓握的少镜头学习与人体演示<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：</p></blockquote><h2 id="1-1-目标问题-4">1.1 目标问题</h2><h3 id="1-1-1-现存问题">1.1.1 现存问题</h3><p>现有的位姿估计方法要么需要计算目标物体的6D位姿，要么需要学习一组抓取点。前者的方法不能很好的扩展到多个对象实例或类，后者需要大型注释数据集，并且由于其对新几何图形的泛化能力交叉而受到阻碍。</p><h3 id="1-1-2-解决思路">1.1.2 解决思路</h3><p>通过简单简短的人类演示教机器人如何抓取物体，不需要许多带注释的图像，也不局限于特定的几何形状。</p><h3 id="1-1-3-大致方法">1.1.3 大致方法</h3><p>首先构建一个人机交互的RGB-D图像序列。利用该序列来构建表示交互的手和对象网格。完成重建对象形状的缺失部分，并估计重建模型与场景中可见对象之间的相对变换。最后将物体和人手之间的相对姿态的先验知识以及对场景中当前物体姿态的估计转化为机器人必要的抓取指令。</p><h3 id="1-1-4-引言总结">1.1.4 引言总结</h3><p><strong>为什么要做这个研究：</strong><br>当前的机器人抓取缺乏泛化能力，因为它们要么专注于估计物体姿态，要么学习抓取点，这需要物体的详细先验信息或大量注释。就像人手一样，机器人的抓取器和手臂的运动范围也有自然的限制，自由度也有限，这限制了它们可能的抓取姿势。虽然机器人抓取器和人手的运动模型可能有很大差异，但应该可以从人类操作中提取信息，并从中推断出目标机器人的足够抓取命令。通过有限的人类演示，机器人可以模仿人类行为，从而无缝抓取物体。</p><p><strong>本文主要做了什么：</strong><br>我们专注于这种模仿，机器人反映了人类的互动，如图1。该任务可以分为视觉感知和解释部分，其中人类教员演示先验操纵（Demo），机器人从中推断出操纵当前场景所需的抓握信息（抓握）。如果从人手到机器人抓取器有足够的映射，将任务分解为这两个阶段可以使我们的方法扩展到大量不同的抓取器。最终，这为通过自然人类演示来教授机器人铺平了道路，从而实现更高水平的自动化，尤其是在结构较少的环境中。</p><p><strong>本文大致是如何实现的：</strong><br>在从各种不同的角度向机器人演示物体（Demo）的过程中，我们的方法不断跟踪手和物体，这些手和物体被融合到截断有符号距离场（TSDF）中，用于3D重建。使用手和对象的语义分割，可以分离并进一步处理重建，以检索对象和手的完整3D表示。然后，我们利用MANO手部模型提取相关的3D手部网格，并将其与重建对象紧密对齐。在推理过程中，我们使用PPF FoldNet来预测对象是否存在，以及它从对象到相机空间的相对变换。然后，应用所估计的姿势从所估计的手网格导出最终抓握指令。</p><h2 id="1-2-方法-3">1.2 方法</h2><p><strong>总体流程：</strong></p><ol><li>在一组人类演示RGB-D图像上分割手和物体，并使用记录的深度图重建它们的形状</li><li>补全物体形状</li><li>提取手部姿态</li><li>估计对象的6D姿态，转换手部模型，推理抓取指令</li></ol><p><img src="https://img.mahaofei.com/img/202308131124867.png" alt="image.png"></p><h3 id="1-2-1-人-物交互的三维重建">1.2.1 人-物交互的三维重建</h3><p>使用MaskRCNN对手和物体进行分割，并且应用了二进制交叉熵来防止类间竞争。</p><p>利用分割后的深度图像，通过KinectFusion创建相应的TSDF体素，并通过输入帧与TSDF之间的ICP配准实现无漂移跟踪。<br>（因为家用物体几何形状简单，因此同时跟踪手和物体，手的结构复杂稳定了跟踪结果）</p><p>利用分割结果，通过两个单独的TSDF重建将手和物体分离开。</p><h3 id="1-2-2-物体形状补全">1.2.2 物体形状补全</h3><p>由于自遮挡和部分可见性，重建的模型还不完整。</p><p>使用3D CNN直接矫正TSDF体积，然后通过行进立方体进行形状提取。<br>（这里使用了UNet的3D变体，输入是64x64x64的体素，输出每个体素的预测分数表示体素是否被占用。</p><h3 id="1-2-3-手部姿态估计">1.2.3 手部姿态估计</h3><p>从重建的手形状中估计手部参数模型。</p><p>使用MANO手部模型，将手部姿态和形状参数映射到网格中。由于手部也受到了部分遮挡，因此使用辅助接触和碰撞损失联合训练CNN进行手部网格和物体网格估计。</p><p>为了进一步改进抓握位置，使用ICP将手部网格与手部TSDF体素对齐。</p><h3 id="1-2-4-抓取指令生成">1.2.4 抓取指令生成</h3><p>首先检索物体姿态，然后用它来变换手部网格，并用手部模型的拇指和食指计算抓握点。</p><h2 id="1-3-思考-3">1.3 思考</h2><ol><li>物体的三维重建可以采用其他方式，或者结合CAD模型补全的方式，相比于使用3D CNN预测效果会更好。</li><li>手部姿态的提取也可以考虑采用更新的算法，例如识别手部关键点，而不是预测手部网格的方式。</li><li>抓取姿态生成是直接使用拇指食指作为二指抓取姿态，是否可以考虑其他方式，提高抓取的可靠性。</li></ol><h1>2 Learning to Grasp Familiar Objects Based on Experience and Objects’ Shape Affordance</h1><blockquote><p><strong>标题</strong>：基于经验和物体形状的相似目标抓取<br><strong>作者团队</strong>：慕尼黑工业大学<br><strong>期刊会议</strong>：IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS<br><strong>时间</strong>：2019<br><strong>代码</strong>：</p></blockquote><h2 id="2-1-目标问题">2.1 目标问题</h2><h3 id="2-1-1-现存问题">2.1.1 现存问题</h3><p>对于已知物体的抓取方法，物体具有抓取数据库，机器人通过估计物体姿态，然后利用国旅行假设找到合适抓取姿态，但是这些方法的缺点是不可能将所有对象的模型都放入机器人的数据库。</p><p>需要一种从以前的经验推广到新对象的模型的能力。</p><h3 id="2-1-2-解决思路">2.1.2 解决思路</h3><p>整合人类抓握经验中的关键线索（拇指指尖和手腕的位置方向），提出了一种有效的抓握方法。</p><h2 id="2-2-方法">2.2 方法</h2><h3 id="2-2-1-从不完整点云上生成抓取点">2.2.1 从不完整点云上生成抓取点</h3><p>在抓取时，熟悉对象上的抓取点在对象上具有相似的相对位置。</p><p>基于这个原理，使用3D SHOT形状描述符描述物体，能够精确的描述兴趣点相对于整个对象和表面的位置。具体学习抓取点的过程如下：</p><ol><li>收集从部分点云中选择的兴趣点的SHOT特征、LR特征、RGB特征</li><li>通过计算简单的统计数据，如范围、均值、标准差、熵等，降低沿点维度的特征维度</li><li>将特征输入到用于对象分类的极限学习机中。</li></ol><h3 id="2-2-2-构建抓取模型">2.2.2 构建抓取模型</h3><p>没看懂。</p><p>大概是建立大拇指和物体之间的坐标变换关系，然后将其转换为三指夹爪与物体之间的坐标变换。</p><h3 id="2-2-3-腕关节约束估计">2.2.3 腕关节约束估计</h3><p>主要是解决受外在单一视角下点云被遮挡，无法精确确定手腕方向的问题。</p><h1>3 R3M: A Universal Visual Representation for Robot Manipulation</h1><blockquote><p><strong>标题</strong>：R3M:机器人操纵的通用视觉表示<br><strong>作者团队</strong>：斯坦福大学，Meta AI<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://tinyurl.com/robotr3m">https://tinyurl.com/robotr3m</a></p></blockquote><h2 id="3-1-目标问题">3.1 目标问题</h2><p>训练机器人根据图像完成操作任务。<strong>给定一段文字，例如“将铲子放到锅里”，机器人根据视觉执行相应的动作</strong>。</p><p><strong>（1）传统方法的局限性</strong></p><p>传统且广泛使用的方法是使用同构数据从头开始训练端到端的模型，但是由于训练数据难以获取，限制了这种方法的泛化。而我们还有没合适的机器人数据集，最近的数据集都是由少数不同环境有限任务组成，因此泛用性受到限制。</p><p><strong>（2）本文的突破思想</strong></p><p>参考<code>ImageNet</code>等通用有效的模型，机器人领域目前还没有类似的模型出现，但是思想可以借鉴，就是使用丰富的<code>in-the-wild data</code>（野生数据？），也就是使用人类与环境交互的视频，这些数据庞大且多样化，包含全球各种场景与任务。</p><p><strong>（3）本文方法简述</strong></p><p>训练了一种机器人操纵表示方法R3M。R3M能够学习具有挑战性的任务，例如将菜放入锅中，折叠毛巾等。</p><h2 id="3-2-方法">3.2 方法</h2><p>本文认为，机器人操作的良好表现由以下三个方面组成</p><ul><li>机器人应该捕获时间动态，因为机器人在环境中要按时间顺序完成任务</li><li>机器人应该捕获于一相关的特征</li><li>机器人应该是紧凑的</li></ul><p><strong>（1）时间对比学习</strong></p><p>训练编码器生成一个表示，是的时间上较近的图像之间的距离小于时间上较远的图像或来自不同视频的图像。</p><p><strong>（2）视频语言对齐</strong></p><p>捕获语言的特征，学习视频场景中的语义部分。</p><p><strong>（3）正则化</strong></p><p>降低状态空间的维度来保证克隆训练的策略符合专家状态分布。</p><h2 id="3-3-思考">3.3 思考</h2><p>与本人方向有差别，本文更偏向于语义，视觉只是作为一个感知手段。</p><h1>4 Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video</h1><blockquote><p><strong>标题</strong>：对抗性技能网络：来自视频的无监督机器人技能学习<br><strong>作者团队</strong>：德国弗赖堡大学<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="http://robotskills.cs.uni-freiburg.de/">http://robotskills.cs.uni-freiburg.de/</a></p></blockquote><h2 id="4-1-目标任务">4.1 目标任务</h2><p>从未标记的多视角视频中学习机器人操作任务。</p><p><strong>（1）传统方法的局限性</strong></p><p>现有的强化学习方法尽管有一些进展，但是这些方法都是学习每项任务的解决方案，并且依赖于手动的、面向任务设置的奖励函数，所获得的策略也是针对于特定任务的，无法转移到新任务上。</p><p><strong>（2）本文的创新点</strong></p><p>提出一种无监督的技能学习方法，称为对抗性技能网络ASN，通过观看视频来发现和学习可转移的技能。学习到的技能被用于RL，以便通过组合以前的技能来解决更广泛的任务。</p><p>该方法不需要帧和任务ID的对应关系，不需要任何额外的监督。</p><h2 id="4-2-方法">4.2 方法</h2><p><strong>Adversarial Skill Networks对抗性技能网络</strong></p><p>我们在对抗性框架中学习技能度量空间。网络的编码部分试图最大化熵以增强通用性。鉴别器在测试时不使用，它试图最小化其预测的熵，以提高对技能的识别。最后，最大化所有技能的边际类熵会导致所有任务类的统一使用。请注意，不需要关于框架和它们所源自的任务之间关系的信息。<br>（没看懂）</p><h2 id="4-3-思考">4.3 思考</h2><p>似乎可以从无标签的视频中学习任务。但是过于理论化。</p><h1>5 BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning</h1><blockquote><p><strong>标题</strong>：BC-Z:利用机器人模仿学习实现零样本任务泛化<br><strong>作者团队</strong>：谷歌、加州大学伯克利分校、斯坦福<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://sites.google.com/view/bc-z/home">https://sites.google.com/view/bc-z/home</a></p></blockquote><h2 id="5-1-目标问题">5.1 目标问题</h2><p>使基于视觉的机器人操作系统能推广到新任务。</p><p>为此，开发了一个交互式模仿学习系统，可以传达人物的不同信息作为条件，包括自然语言或者人类演示视频，该系统可以从演示中进行学习。并且发现学习到100个任务之后，可以执行24个未训练的任务且不需要演示。</p><p><strong>（1）现存问题</strong></p><p>机器人技术的一大挑战就是创造一种能够在非结构化环境中基于任意的用户命令执行大量任务。这一工作的关键挑战是泛化。机器人必须要能处理新的环境，识别和操纵以前从未见过的物体，并且理解从未被要求执行过的命令的意图。</p><p>传统的方法是在像素级进行端到端的学习，然后由足够的真实世界的数据，这些方法原则上能够使机器人在新的任务、对象、场景中进行泛化。但实际上这一目标还是遥不可及。</p><p>本文要解决的问题就是通过零样本或者少样本推广基于视觉的机器人操纵任务的问题。</p><p><img src="https://img.mahaofei.com/img/202308151409631.png" alt="image.png"></p><h2 id="5-2-数据收集">5.2 数据收集</h2><p>为100个预先指定的任务手机了人类演示的视频，这些视频包含了推物体、拿取放置物体等9项基本任务。</p><p>搭建一套远程操作系统，远程操作设备通过USB连接到机器人上，通过两个手持控制器遥控操作站在机器人后面，使用控制器以第三人称视角操作机器人，机器人实时响应跟随操作员演示各种任务。</p><h2 id="5-3-方法">5.3 方法</h2><h3 id="5-3-1-语言和视频编码">5.3.1 语言和视频编码</h3><p>编码器以语言命令或人类视频作为输入，并生成任务。</p><ul><li>如果是语言命令，使用预训练的多语言语句编码器为每个任务生成512维语言向量</li><li>如果是视频，使用基于ResNet18的卷积网络</li></ul><h3 id="5-3-2-训练策略">5.3.2 训练策略</h3><p>给定固定的任务，我们通过XYZ和轴角预测的Huber损失和抓取器角度的对数损失来训练。</p><p>开环辅助检测，如果以开环的方式运行，将采取是个行动的开环轨迹。开环预测提供了一个辅助训练目标，并可以离线检查闭环规划质量。</p><p>将状态差异作为操作，标准的模仿学习会将演示动作直接作为目标标签，而本文的专家克隆行为会导致一些小动作或抖动，因此考虑将动作定义为未来目标和下一步的差异，使用自适应算法确定手臂和夹爪的移动量。</p><h3 id="5-3-3-网络架构">5.3.3 网络架构</h3><p>使用ResNet18作为主干，从主干最后一个平均池化层分出多个head，每个head是一个多层感知机，对末端执行器动作的一部分进行建模，具体见原文。</p><h2 id="5-3-思考">5.3 思考</h2><p>首先提供演示视频和文字，然后手动控制机器人执行任务收集数据。似乎仍然较为繁琐。</p><h1>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training</h1><blockquote><p><strong>标题</strong>：VIP：通过价值内隐预训练实现普遍的视觉奖励和表现<br><strong>作者团队</strong>：Meta AI，宾夕法尼亚大学<br><strong>期刊会议</strong>：ICLR<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://sites.google.com/view/vip-rl">https://sites.google.com/view/vip-rl</a></p></blockquote><h2 id="6-1-目标问题">6.1 目标问题</h2><p>特定任务的机器人数据的成本较高且稀缺。从大型、多样化的离线人类视频中学习已经成为获得普遍有效的途径。然而如何将这些<strong>人类视频</strong>用于通用的<strong>奖励学习</strong>仍然是一个未解决的问题。</p><p>与模拟环境中的机器人控制不同，真实世界中的机器人任务无法获得很好的环境状态信息或者定义良好的奖励函数。现有的方法学习每一项任务都需要大量的准备工作。相反，一个简单的方法来指定真实世界操作任务就是提供一个目标图像，图像捕捉环境所需要的视觉变化。然而现有的方法不能产生有效的奖励函数。</p><p>本文提出了一种隐含价值预训练方法（Value-Implicit Pre-training, VIP），一种自监督的预训练视觉表示，为机器人任务生成奖励函数。</p><p>本文的关键在于，将强化学习本身作为强化学习的预训练机制，但是由于人类视频中没有可以用于策略学习的动作信息，因此我们使用这种双价值函数，在没有动作的情况下以完全自我监督的方式进行预训练。</p><h2 id="6-2-方法">6.2 方法</h2><p><strong>（1）从人类视频中自我监督的价值学习</strong></p><p>虽然人类视频不是机器人域的数据，但是它们是学习人类行动的目标条件策略的领域中的数据。因此考虑使用离线人类视频进行学习的一个合理方法是在人类的策略空间上解决目标条件的强化学习问题，提取视觉表示（本文考虑使用 KL 方法进行离线强化学习）。</p><p>由于动作不出现在这个强化学习的目标中，并且所有数据都可以用离线数据集采样，因此可以通过适当选择奖励函数来对双价值函数进行自监督。</p><p><strong>（2）隐含的时间对比学习</strong></p><p>当有意义的指示任务的开始和结束的两个帧在嵌入空间中接近时，初始帧和目标帧之间能够捕获长程语义时间依赖性。</p><p><strong>（3）基于隐含价值的预训练</strong></p><p>具体算法见原文。</p><h2 id="6-3-代码实验">6.3 代码实验</h2><p>VIP 算法使用 ResNet50 作为视觉 backbone，并在 Ego4D 数据集上进行训练。</p><p>算法与 R3M 进行了比较</p><h2 id="6-3-思考">6.3 思考</h2><h1>7 Graph-Structured Visual Imitation</h1><blockquote><p><strong>标题</strong>：图形结构的视觉模仿<br><strong>作者团队</strong>：索尼<br><strong>期刊会议</strong>：CoRL<br><strong>时间</strong>：2019<br><strong>代码</strong>：无</p></blockquote><h2 id="7-1-目标问题">7.1 目标问题</h2><p>当机器人动作使工作空间中检测到的相应视觉实体的相对空间配置与演示更好的匹配时，会得到奖励。</p><p>本文使用人类手指关键点检测器、使用合成增强进行离线训练的对象检测器、由视点变化监督的点检测器。在没有人类注释数据或机器人交互的情况下为每次演示学习多个视觉实体检测器。</p><h2 id="7-2-方法">7.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308151609226.png" alt="image.png"></p><p><strong>（1）检测视觉实体</strong></p><p>人手关键点检测：使用现有的手部检测器，并使用D435i获取3D位置。将机器人平行钳口夹持器映射到演示者的拇指和食指指尖。通过在两个指尖设置距离阈值来检测抓取和释放动作。</p><p>点特征检测器：训练后，在模仿者和演示者的环境中匹配点特征，建立对应关系。</p><p>合成数据扩充：使用背景移除来提取出2D掩模，并使用合成数据增强来训练视觉检测器。</p><p><strong>（2）动态图构造的运动显著性</strong></p><p><strong>（3）基于可视化实体图的策略学习</strong></p><p>目标是当机器人从单个人类演示中模仿物体操纵任务。具体的成本代价函数参考原文。</p><h2 id="7-3-思考">7.3 思考</h2><p>提取手部关键点映射到机器人夹爪，同时使用物体关键点检测来实现运动策略的生成。思路上不如DemoGrasp更直观。</p><p>可行的方法</p><ol><li>提取演示视频中物体位姿</li><li>模仿执行动作<ol><li>将当前视角下物体位姿与演示视频中每一帧位姿计算误差损失</li><li>根据误差实时计算末端位姿调整姿态</li><li>将当前帧手臂关键点加入，获取机械臂各关节应到的位姿</li><li>执行机械臂动作（期间加入机械臂避障与轨迹平滑）</li></ol></li></ol><h1>8 Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</h1><blockquote><p><strong>标题</strong>：通过观看学习：人体视频中操纵技能的物理模拟<br><strong>作者团队</strong>：多伦多大学<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="http://www.pair.toronto.edu/lbw-kp/">http://www.pair.toronto.edu/lbw-kp/</a></p></blockquote><h2 id="8-1-目标问题">8.1 目标问题</h2><p>通过观看学习，通过模仿指定任务的单个视频来进行策略学习的算法框架。</p><ul><li>由于人类手臂与机器人手臂形态不同，我们的框架学习无监督的人-机器人的翻译来克服形态不匹配问题。</li><li>为了捕捉对学习状态至关重要的显著区域的细节，我们的模型采取了无监督关键点检测。检测到的关键点形成包含语义上有意义的信息的结构化表示，并可以直接用于计算奖励和策略学习。</li></ul><h2 id="8-2-方法">8.2 方法</h2><p>本文所提出的LbW框架由三个部分组成</p><ul><li>图像到图像的翻译网络：逐帧翻译输入的人类演示视频，生成机器人演示视频</li><li>关键点检测器：将生成的机器人演示视频作为输入，提取每帧的关键点，形成关键点轨迹</li><li>策略网络：将当前的基于关键点的观察表示传递给策略网络，用于预测与环境交互的动作</li></ul><p><img src="https://img.mahaofei.com/img/202308151841683.png" alt="image.png"></p><h2 id="8-3-思考">8.3 思考</h2><p>与其说是模仿学习网络，不如说是一个图像翻译网络，基于CycleGAN的图像翻译，将人手演示翻译成机器人动作视频，然后提取视频中机器人的关键点轨迹，通过策略函数实现实物机器人的动作。</p><h1>9 Learning Periodic Tasks from Human Demonstrations</h1><blockquote><p><strong>标题</strong>：从人类演示中学习周期性任务<br><strong>作者团队</strong>：卡内基梅隆大学<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="9-1-目标问题">9.1 目标问题</h2><p>使用主动学习来优化参数，提出了一个目标最大限度的提高机器人操纵物体的运动与演示视频中物体运动之间的相似性。重点在于可变形物体和颗粒物体。（用布擦拭表面，缠绕电缆，用勺子搅拌颗粒物质等）</p><h2 id="9-2-方法">9.2 方法</h2><p>本文提出的框架由两部分组成</p><ul><li>表示学习模块：关键点检测模型从独立收集的非特定任务的人类和机器人数据中提取一致的关键点</li><li>姿态优化模块：将产生在检测的关键点方面与人类演示相匹配的机器人视频</li></ul><h2 id="9-3-思考">9.3 思考</h2><p>给定人类演示动作和手动操控机器人演示动作，机器人学习两者的相似性，然后重复演示动作使其更接近人类演示效果。</p><h1>10 One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks</h1><blockquote><p><strong>标题</strong>：复合视觉运动任务的一次性层次模拟学习<br><strong>作者团队</strong>：加州大学伯克利分校<br><strong>期刊会议</strong>：arXiv<br><strong>时间</strong>：2018<br><strong>代码</strong>：<a href="https://sites.google.com/view/one-shot-hil">https://sites.google.com/view/one-shot-hil</a></p></blockquote><h2 id="10-1-目标问题">10.1 目标问题</h2><p>真实机器人上从人类执行任务的视频中学习多阶段任务。</p><h2 id="10-2-方法">10.2 方法</h2><p>对于每个子任务，我们提供多个人类演示和多个机器人演示（需要对象和执行的任务对应，但是不用相同的对象位置、执行速度）</p><p><strong>（1）基元的合成</strong>：训练了一个人类相位预测器和机器人相位预测器，从人类执行视频中学习特定的机器人策略</p><p><strong>（2）原始相位预测</strong>：学习如何分割复合任务的人类演示；何时学习策略过度到下一个。</p><h2 id="10-3-思考">10.3 思考</h2><p>提供人的演示视频，机器人的演示视频，然后训练策略。最后利用训练的策略，提供一段人类演示视频，机器人执行对应的操作。</p><h1>11 Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller</h1><blockquote><p><strong>标题</strong>：基于解耦层次控制器的第三人称视觉模仿学习<br><strong>作者团队</strong>：MIT<br><strong>期刊会议</strong>：NeurIPS<br><strong>时间</strong>：2019<br><strong>代码</strong>：<a href="https://pathak22.github.io/hierarchical-imitation/">https://pathak22.github.io/hierarchical-imitation/</a></p></blockquote><h2 id="11-1-目标问题">11.1 目标问题</h2><p>通过从第三人称视角观看人类演示视频，可以在未知场景中操纵新物体。</p><h2 id="11-2-方法">11.2 方法</h2><p><img src="https://img.mahaofei.com/img/202308152040902.png" alt="image.png"></p><p><strong>（1）目标生成器</strong></p><p>从人类演示视频中推断像素空间中的目标，并以像素级的表示形式将其转化为机器人环境中的目标。</p><p>也是使用图像翻译的方法，将人类演示图像翻译为机器人演示图象。</p><p><strong>（2）反向控制器</strong></p><p>跟踪视觉目标推理模型中生成的线索，并生成机器人要执行的动作。</p><p>使用ResNet18模型。</p><p><strong>（3）第三人称模仿</strong></p><p>以交替方式运行目标生成器和反向控制器。目标生成器生成子目标，低级控制器生成机器人关节角度，直到人类演示结束。</p><h2 id="11-3-思考">11.3 思考</h2><p>还是使用图像翻译的思路，把人手操作图像翻译成机械臂操作图像，再由控制器生成机器人关节角度。</p><h1>12 You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration</h1><blockquote><p><strong>标题</strong>：Yodo：单一视觉演示的类别级操作<br><strong>作者团队</strong>：罗格斯大学<br><strong>期刊会议</strong>：RSS<br><strong>时间</strong>：2022<br><strong>代码</strong>：</p></blockquote><h2 id="12-1-目标问题">12.1 目标问题</h2><p>由于最近的跨对象类别级操作虽然有很好的结果，但通常需要昂贵的真实数据收集和为每个对象类别和任务手动指定语义关键点。并且粗略的关键点预测和忽略中间动作序列阻止了在抓取和防止之外的复杂任务的应用。</p><p>本工作提出了一种新的操作框架。该框架利用了无模型6D跟踪技术，解析单个演示视频中的类别级任务轨迹，整个执行过程被分解为远程、无碰撞运动和最后一英寸操作三个步骤。</p><h2 id="12-2-方法">12.2 方法</h2><p>对于每个演示视频帧，通过无模型6D位姿估计跟踪目标位姿，对象位姿在容器的坐标系中表示，这样允许泛化到新的场景。</p><p><strong>（1）类别级表示的离线学习</strong></p><p>建立了一个9D物体表示方法，6D位姿+3D缩放</p><p><strong>（2）无模型的物体6D跟踪</strong></p><p>物体运动跟踪要实现两个目的</p><ul><li>演示阶段，解析录制的视频，提取容器坐标系中被操纵的对象的6D运动轨迹</li><li>在线执行期间，为闭环控制器提供视觉反馈</li></ul><p><strong>（3）类别级行为克隆作为最后一步策略</strong></p><p>产生密集的离散轨迹，以便机器人能沿轨迹到达下一个目标</p><p><strong>（4）基于局部注意的动态类别级框架</strong></p><p>自动动态地规范坐标系原点。</p><p><strong>（5）抓取物体并使其沿关键点移动</strong></p><p>常规的抓取方法</p><h2 id="12-3-思考">12.3 思考</h2><p>将目标位姿表示为相对于另一个物体的相对位姿，这样有助于场景的泛化。</p><p>整体思想就是使用6D位姿估计获得目标的运动轨迹，然后重复这条轨迹。</p>]]></content>
    
    
    <summary type="html">从人类抓取物体的视频中学习机器人抓取的相关工作调研。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="模仿动作" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%A8%A1%E4%BB%BF%E5%8A%A8%E4%BD%9C/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="模仿" scheme="https://www.mahaofei.com/tags/%E6%A8%A1%E4%BB%BF/"/>
    
  </entry>
  
  <entry>
    <title>ROS系统Buglist（不定时更新）</title>
    <link href="https://www.mahaofei.com/post/4add66b0.html"/>
    <id>https://www.mahaofei.com/post/4add66b0.html</id>
    <published>2023-05-15T09:05:05.000Z</published>
    <updated>2023-05-15T09:05:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、安装问题</h1><h2 id="ROS安装时rosdep-init与rosdep-update问题解决方法">ROS安装时rosdep_init与rosdep_update问题解决方法</h2><p><strong>解决方法</strong></p><p>使用下面的命令替代上面两行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3-pip</span><br><span class="line">sudo pip3 install rosdepc</span><br><span class="line">sudo rosdepc init</span><br><span class="line">rosdepc update</span><br></pre></td></tr></table></figure><h1>二、环境问题</h1><h2 id="Unable-to-find-either-executable-‘empy’-or-Python-module-‘em’…-try-installing-the-package-‘python3-empy’">Unable to find either executable ‘empy’ or Python module ‘em’…  try  installing the package ‘python3-empy’</h2><p><strong>（1）问题原因</strong></p><p>Anaconda使用的是Python3版本，但是ROS使用的Python2</p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="Could-not-find-a-package-configuration-file-provided-by-“某某包”-with-any-of-the-following-names">Could not find a package configuration file provided by “某某包” with any of  the following names</h2><p><strong>（1）问题原因</strong></p><p>缺少<code>某某包</code></p><p><strong>（2）解决方法</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ros-noetic-某某包</span><br></pre></td></tr></table></figure><h1>三、配置问题</h1><h2 id="ERROR-cannot-launch-node-of-type-robot-state-publisher-state-publisher-Cannot-locate-node-of-type-state-publisher-in-package-robot-state-publisher-Make-sure-file-exists-in-package-path-and-permission-is-set-to-executable-chmod-x）">ERROR: cannot launch node of type [robot_state_publisher/state_publisher]: Cannot locate node of type [state_publisher] in package [robot_state_publisher]. Make sure file exists in package path and permission is set to executable (chmod +x）</h2><p><strong>（1）问题原因</strong></p><p>使用launch文件启动某个节点时出现这个问题，是因为launch文件中name、pkg、type不统一导致的。</p><p><strong>（2）解决方法</strong></p><p>检查launch文件，确保name、pkg、type一样，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot; /&gt;</span><br></pre></td></tr></table></figure><h2 id="joint-state-publisher-gui没有显示">joint state publisher gui没有显示</h2><p><strong>（1）问题描述</strong></p><p>使用ROS进行仿真，想用joint state publisher进行机械臂控制，但是启动launch文件后没有报错信息，但也没有joint state publisher gui。</p><p><strong>（2）解决方法</strong></p><p>2020年开始，gui已经移出了 joint state publisher, 并且成为了一个新的package：joint state publisher gui. 之前那种使用gui参数的方式调用joint state publisher 是仍然可行的，但是不会调用gui。</p><p>在launch文件中，将joint state publisher 替换成joint__state__publisher_gui。</p>]]></content>
    
    
    <summary type="html">在使用ROS系统进行机器人实验中，遇到的各种错误信息汇总，不定时更新。</summary>
    
    
    
    <category term="机器人" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    
    <category term="ros" scheme="https://www.mahaofei.com/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/ros/"/>
    
    
    <category term="bugs" scheme="https://www.mahaofei.com/tags/bugs/"/>
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
  </entry>
  
  <entry>
    <title>ROS Gazebo 6D机械臂抓取仿真实验</title>
    <link href="https://www.mahaofei.com/post/7fec171b.html"/>
    <id>https://www.mahaofei.com/post/7fec171b.html</id>
    <published>2023-05-15T07:23:56.000Z</published>
    <updated>2023-05-15T07:23:56.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、基础知识</h1><h2 id="1-1-URDF">1.1 URDF</h2><p>URDF是ROS中机器人模型的描述格式，包括机器人的外观、物理属性、关节类型等方面。</p><ul><li><code>&lt;robot&gt;</code>：最顶层标签</li><li><code>&lt;link&gt;</code>：描述刚提的外观形状、碰撞几何、颜色、惯性矩阵等</li><li><code>&lt;joint&gt;</code>：描述两个link之间的关系，有6种类型，最常用的是<code>revolute</code>类型，有关节位置限制的旋转关节</li></ul><p>xacro模型可以将部分URDF打包成一个&quot;类&quot;，在其他模型中调用。</p><p>功能包中一般包括以下四个部分</p><ol><li><code>cfg</code>：配置文件</li><li><code>launch</code>：加载urdf模型，并在rviz中展示</li><li><code>meshes</code>：urdf用到的外观模型</li><li><code>urdf</code>：urdf模型定义</li></ol><h2 id="1-2-Gazebo">1.2 Gazebo</h2><h2 id="1-3-Moveit控制">1.3 Moveit控制</h2><p><strong>（1）Moveit!大致功能</strong></p><ul><li>运动学计算</li><li>运动规划</li><li>碰撞检测</li></ul><p>最重要的节点是<code>move_group</code>，输入可以是RVIZ中的数据或点云和深度图。路径规划一般使用的OMPL库，碰撞检测使用FCL库。最后发送个机械臂让机械臂执行轨迹。</p><h2 id="1-4-机械臂运动规划">1.4 机械臂运动规划</h2><h2 id="1-5-基于深度学习的视觉避障">1.5 基于深度学习的视觉避障</h2><h1>二、实验环境搭建</h1><h2 id="1-1-安装ROS">1.1 安装ROS</h2><p>参考<a href="https://www.mahaofei.com/post/b278544f.html">Ubuntu20.04安装ROS Noetic</a>文章</p><p>![[01-Ubuntu20.04安装ROS Noetic#二、安装ROS]]</p><h2 id="1-2-安装Moveit">1.2 安装Moveit!</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ros-noetic-moveit</span><br></pre></td></tr></table></figure><h2 id="1-3-安装UR机器人及驱动">1.3 安装UR机器人及驱动</h2><p>复制代码后，修改下面的内容，使其能在noetic版本的ros上运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/srv/SetPayload.srv</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">float32 payload</span><br><span class="line">geometry_msgs/Vector3 center_of_gravity</span><br><span class="line">-----------------------</span><br><span class="line">bool success</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/CMakeLists.txt</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8.3)</span><br><span class="line">project(ur_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Find catkin macros and libraries</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># is used, also find other catkin packages</span></span></span><br><span class="line">find_package(catkin REQUIRED COMPONENTS message_generation std_msgs geometry_msgs)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate messages in the &#x27;msg&#x27; folder</span></span></span><br><span class="line">add_message_files(</span><br><span class="line">   FILES</span><br><span class="line">   Analog.msg</span><br><span class="line">   Digital.msg</span><br><span class="line">   IOStates.msg</span><br><span class="line">   RobotStateRTMsg.msg</span><br><span class="line">   MasterboardDataMsg.msg</span><br><span class="line">   RobotModeDataMsg.msg</span><br><span class="line">   ToolDataMsg.msg</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate services in the &#x27;srv&#x27; folder</span></span></span><br><span class="line">add_service_files(</span><br><span class="line">   FILES</span><br><span class="line">   SetPayload.srv</span><br><span class="line">   SetSpeedSliderFraction.srv</span><br><span class="line">   SetIO.srv</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Generate added messages and services with any dependencies listed here</span></span></span><br><span class="line">generate_messages(</span><br><span class="line">   DEPENDENCIES</span><br><span class="line">   std_msgs</span><br><span class="line">   geometry_msgs</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># catkin specific configuration ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##################################</span></span></span><br><span class="line">catkin_package(</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> INCLUDE_DIRS include</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> LIBRARIES ur_msgs</span></span><br><span class="line">   CATKIN_DEPENDS message_runtime std_msgs geometry_msgs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> DEPENDS system_lib</span></span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Build ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##########</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Install ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Testing ##</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">############</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/catkin_ws/src/universal_robot/ur_msgs/package.xml</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;package format=&quot;2&quot;&gt;</span><br><span class="line">  &lt;name&gt;ur_msgs&lt;/name&gt;</span><br><span class="line">  &lt;version&gt;1.2.5&lt;/version&gt;</span><br><span class="line">  &lt;description&gt;The ur_msgs package&lt;/description&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;author&gt;Andrew Glusiec&lt;/author&gt;</span><br><span class="line">  &lt;author&gt;Felix Messmer&lt;/author&gt;</span><br><span class="line">  &lt;maintainer email=&quot;g.a.vanderhoorn@tudelft.nl&quot;&gt;G.A. vd. Hoorn&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;miguel.prada@tecnalia.com&quot;&gt;Miguel Prada Sarasola&lt;/maintainer&gt;</span><br><span class="line">  &lt;maintainer email=&quot;nhg@ipa.fhg.de&quot;&gt;Nadia Hammoudeh Garcia&lt;/maintainer&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;license&gt;BSD&lt;/license&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt;</span><br><span class="line">  &lt;build_depend&gt;message_generation&lt;/build_depend&gt;</span><br><span class="line">  &lt;depend&gt;std_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;depend&gt;geometry_msgs&lt;/depend&gt;</span><br><span class="line">  &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;</span><br><span class="line"></span><br><span class="line">  &lt;export&gt;</span><br><span class="line">  &lt;/export&gt;</span><br><span class="line">&lt;/package&gt;</span><br></pre></td></tr></table></figure><p>完成之后，就可以编译了。<code>source</code>之后使用<code>roslaunch ur5_moveit_config demo.launch</code></p><h2 id="1-4-简单测试">1.4 简单测试</h2><p><strong>（1）Rviz打开UR5模型</strong></p><p>机械臂夹爪模型路径为<code>universal_robot/urdf/ur5_gripper_joint_limited_robot.urdf.xacro</code>。</p><p>可以通过<code>universal_robot/ur_description/launch/view_ur5_with_gripper.launch</code>启动，从Rviz中查看模型情况，并使用<code>joint_state_publisher_gui</code>对机械臂模型拖动控制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_description view_ur5_with_gripper.launch</span><br></pre></td></tr></table></figure><p><strong>（2）Rviz中Moveit测试</strong></p><p>使用下面的程序可以在 Rviz 中进行 Moveit 轨迹规划测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur5_moveit_config demo.launch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur5_gripper_moveit_config demo.launch</span><br></pre></td></tr></table></figure><p><strong>（3）Gazebo中Moveit测试</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">无夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5.launch</span><br><span class="line">roslaunch ur5_moveit_config ur5_moveit_planning_execution.launch sim:=true</span><br><span class="line">roslaunch ur5_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><code>ur5.launch</code>：用于启动 gazebo 仿真环境。具体包括以下几个部分，启动空环境、定义 robot_description 参数服务器、发送到gazebo中生成机器人、启动并加载控制器。</p><p><code>ur5_moveit_planning_execution.launch</code>：用于启动 MoveIt 相关组件。具体包括以下几个部分：设置 sim参数， 根据 sim 参数重映射 follow_joint_trajectory 话题，启动MoveIt。</p><p><code>moveit_rviz.launch</code>：用于启动 Rviz 相关组件。具体包括以下几个部分：加载配置参数，启动Rviz。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有夹爪</span></span><br><span class="line">roslaunch ur_gazebo ur5_with_gripper.launch</span><br><span class="line">roslaunch ur5_single_arm_moveit_config ur5_moveit_planning_execution.launch</span><br><span class="line">roslaunch ur5_gripper_moveit_config moveit_rviz.launch config:=true</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/202308041008966.png" alt=""></p><h2 id="1-5-导入自定义物体">1.5 导入自定义物体</h2><p><strong>（1）网络方法</strong></p><p>使用 MeshLab 加载自己的物体模型。</p><p>点击【Filters -&gt; Normals … -&gt; Compute normals for points sets】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Remeshing -&gt; Surface Reconstruction: Screened Poisson】，按照默认设置确定即可。</p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Trivial Per-Triangle】，按照如下设置，重要的是Method。</p><p><img src="https://img.mahaofei.com/img/202305242228283.png" alt="image.png"></p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】，按照如下设置，重要的是Source Mesh和Target Mesh。</p><p><img src="https://img.mahaofei.com/img/202305242231021.png" alt="image.png"></p><p><strong>（2）摸索方法</strong></p><p>点击【Filters -&gt; Texture -&gt; Parametrization: Flat Plane】</p><p>点击【Filters -&gt; Texture -&gt; Transfer Vertex Attributes to Textur(1 or 2 meshes)】</p>]]></content>
    
    
    <summary type="html">使用Gazebo搭建6D机械臂仿真环境，并添加相机，然后实现抓取仿真实验。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E5%AE%9E%E9%AA%8C/"/>
    
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="ROS" scheme="https://www.mahaofei.com/tags/ROS/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>【目标检测算法】YOLOV8代码复现</title>
    <link href="https://www.mahaofei.com/post/16b5f6b3.html"/>
    <id>https://www.mahaofei.com/post/16b5f6b3.html</id>
    <published>2023-05-06T06:13:12.000Z</published>
    <updated>2023-05-06T06:13:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、算法笔记</h1><h1>二、代码复现</h1><h2 id="2-1-搭建环境">2.1 搭建环境</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolov8 python=3.7</span><br><span class="line">conda activate yolov8</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>下载作者开源的程序，并安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/ultralytics</span><br><span class="line">cd ultralytics</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-命令行使用教程">2.2 命令行使用教程</h2><p><strong>（1）语法规则</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yolo TASK MODE ARGS</span><br><span class="line"></span><br><span class="line">Where   TASK (optional) is one of [detect, segment, classify]</span><br><span class="line">        MODE (required) is one of [train, val, predict, export, track]</span><br><span class="line">        ARGS (optional) are any number of custom &#x27;arg=value&#x27; pairs like &#x27;imgsz=320&#x27; that override defaults.</span><br></pre></td></tr></table></figure><p><strong>（2）训练</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01</span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo predict model=yolov8n-seg.pt source=&#x27;https://youtu.be/Zgi9g1ksQHc&#x27; imgsz=320</span><br></pre></td></tr></table></figure><p><strong>（4）评价</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640</span><br></pre></td></tr></table></figure><h2 id="2-3-Python使用教程">2.3 Python使用教程</h2><p><strong>（1）训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n.pt&#x27;</span>) <span class="comment"># 从预训练模型开始</span></span><br><span class="line">model.train(epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><strong>（2）评价</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line">model.val()  <span class="comment"># 使用model.pt的data yaml进行评价</span></span><br><span class="line">model.val(data=<span class="string">&#x27;coco128.yaml&#x27;</span>)  <span class="comment"># 或指定数据进行评价</span></span><br></pre></td></tr></table></figure><p><strong>（3）预测</strong></p><p>获取预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">&quot;model.pt&quot;</span>)</span><br><span class="line"><span class="comment"># 接受所有类型 - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam</span></span><br><span class="line"><span class="comment"># 从摄像头</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;0&quot;</span>)</span><br><span class="line"><span class="comment"># 从文件夹</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>, show=<span class="literal">True</span>) <span class="comment"># Display preds. Accepts all YOLO predict arguments</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL图像</span></span><br><span class="line">im1 = Image.<span class="built_in">open</span>(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im1, save=<span class="literal">True</span>)  <span class="comment"># save plotted images</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从ndarray</span></span><br><span class="line">im2 = cv2.imread(<span class="string">&quot;bus.jpg&quot;</span>)</span><br><span class="line">results = model.predict(source=im2, save=<span class="literal">True</span>, save_txt=<span class="literal">True</span>)  <span class="comment"># save predictions as labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从PIL/ndarray的列表</span></span><br><span class="line">results = model.predict(source=[im1, im2])</span><br></pre></td></tr></table></figure><p>预测结果分析（results会包含预测所有结果的列表，当有很多图像的时候要注意避免内存溢出，特别是在实例分割时）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. return as a list</span></span><br><span class="line">results = model.predict(source=<span class="string">&quot;folder&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  return as a generator (stream=True)</span></span><br><span class="line">results = model.predict(source=<span class="number">0</span>, stream=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># Detection</span></span><br><span class="line">    result.boxes.xyxy   <span class="comment"># box with xyxy format, (N, 4)</span></span><br><span class="line">    result.boxes.xywh   <span class="comment"># box with xywh format, (N, 4)</span></span><br><span class="line">    result.boxes.xyxyn  <span class="comment"># box with xyxy format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.xywhn  <span class="comment"># box with xywh format but normalized, (N, 4)</span></span><br><span class="line">    result.boxes.conf   <span class="comment"># confidence score, (N, 1)</span></span><br><span class="line">    result.boxes.cls    <span class="comment"># cls, (N, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Segmentation</span></span><br><span class="line">    result.masks.data      <span class="comment"># masks, (N, H, W)</span></span><br><span class="line">    result.masks.xy        <span class="comment"># x,y segments (pixels), List[segment] * N</span></span><br><span class="line">    result.masks.xyn       <span class="comment"># x,y segments (normalized), List[segment] * N</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Classification</span></span><br><span class="line">    result.probs     <span class="comment"># cls prob, (num_class, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Each result is composed of torch.Tensor by default, </span></span><br><span class="line"><span class="comment"># in which you can easily use following functionality:</span></span><br><span class="line">result = result.cuda()</span><br><span class="line">result = result.cpu()</span><br><span class="line">result = result.to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">result = result.numpy()</span><br></pre></td></tr></table></figure><h2 id="2-3-数据集制作（实例分割）">2.3 数据集制作（实例分割）</h2><p><strong>（1）使用Labelme创建实例分割数据集</strong></p><p>安装labelme</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme</span><br></pre></td></tr></table></figure><p>安装完成后直接在命令行输入<code>labelme</code>即可打开。</p><p>使用label进行标注，将生成的json文件和原始图像jpg，放入同一个文件夹中。</p><p><strong>（2）Labelme格式转COCO格式</strong></p><p>参考<a href="https://pypi.org/project/labelme2coco/">pypi的labelme2coco包</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme2coco</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者使用清华源</span></span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple labelme2coco</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labelme2coco path/to/labelme/dir --train_split_rate 0.85</span><br></pre></td></tr></table></figure><p><strong>（3）COCO格式转YOLO格式</strong></p><p>使用<code>labelme</code>制作实例分割的coco格式数据集，然后使用<a href="https://github.com/ultralytics/JSON2YOLO">ultralytics/JSON2YOLO</a>项目将json文件转换成yolo的训练格式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ultralytics/JSON2YOLO.git</span><br><span class="line">cd JSON2YOLO</span><br></pre></td></tr></table></figure><p>创建一个虚拟环境，然后使用下面的命令安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第387行，设置为刚才得到的COCO注释的位置，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> source == <span class="string">&#x27;COCO&#x27;</span>:</span><br><span class="line">convert_coco_json(<span class="string">&#x27;datasets/20230223_Phone_4Obj_Coco/annotations&#x27;</span>,  <span class="comment"># directory with *.json</span></span><br><span class="line">  use_segments=<span class="literal">True</span>,</span><br><span class="line">  cls91to80=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>修改<code>general_json2yolo.py</code>的第289行，因为不是coco80中的物体类型，是自己设置的，因此需要修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cls = coco80[ann[&#x27;category_id&#x27;] - 1] if cls91to80 else ann[&#x27;category_id&#x27;] - 1  # class</span></span><br><span class="line">cls = ann[<span class="string">&#x27;category_id&#x27;</span>]  <span class="comment"># class</span></span><br></pre></td></tr></table></figure><p>运行程序将COCO格式json文件转换为YOLO格式txt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python general_json2yolo.py</span><br></pre></td></tr></table></figure><p>结果保存在new_dir中，需要手动把images复制过去。最后得到的数据集如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_root</span><br><span class="line">├── images</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.jpg</span><br><span class="line">└── ...</span><br><span class="line">└── labels</span><br><span class="line">├── train2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br><span class="line">    └── val2017</span><br><span class="line">├── youimagename.txt</span><br><span class="line">└── ...</span><br></pre></td></tr></table></figure><p><strong>（4）创建数据集的YAML文件</strong></p><p>打开目录<code>ultralytics/datasets</code>，复制一份其中的<code>coco128-seg.yaml</code>，重命名为<code>custom-seg.yaml</code>，然后根据自己的数据集进行修改。</p><p>例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span></span><br><span class="line"><span class="attr">path:</span> <span class="string">/media/mahaofei/OneTouch/Dataset/Program_data/image_processing/ultralytics/20230223_Phone_4Obj_YOLO</span>  <span class="comment"># dataset root dir</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">images/train2017</span>  <span class="comment"># train images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">images/train2017</span>  <span class="comment"># val images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">test:</span>  <span class="comment"># test images (optional)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classes</span></span><br><span class="line"><span class="attr">names:</span></span><br><span class="line">  <span class="attr">0:</span> <span class="string">ammeter</span></span><br><span class="line">  <span class="attr">1:</span> <span class="string">coffeebox</span></span><br><span class="line">  <span class="attr">2:</span> <span class="string">realsensebox</span></span><br><span class="line">  <span class="attr">3:</span> <span class="string">sucker</span></span><br></pre></td></tr></table></figure><h2 id="2-4-开始训练">2.4 开始训练</h2><p>新建一个python文件如<code>train.py</code>，添加内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a model</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;)  # build a new model from YAML</span></span><br><span class="line">model = YOLO(<span class="string">&#x27;yolov8n-seg.pt&#x27;</span>)  <span class="comment"># load a pretrained model (recommended for training)</span></span><br><span class="line"><span class="comment"># model = YOLO(&#x27;yolov8n-seg.yaml&#x27;).load(&#x27;yolov8n.pt&#x27;)  # build from YAML and transfer weights</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.train(data=<span class="string">&#x27;custom-seg.yaml&#x27;</span>, epochs=<span class="number">100</span>, imgsz=<span class="number">3904</span>, batch=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="2-5-结果预测">2.5 结果预测</h2>]]></content>
    
    
    <summary type="html">使用经典的YOLO算法进行实例分割</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="目标检测" scheme="https://www.mahaofei.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【抓取姿态估计算法】RGB Matters论文笔记与复现</title>
    <link href="https://www.mahaofei.com/post/a1b0a01b.html"/>
    <id>https://www.mahaofei.com/post/a1b0a01b.html</id>
    <published>2023-05-04T00:30:31.000Z</published>
    <updated>2023-05-04T00:30:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><p><strong>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</strong></p><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="1-1-目标问题-3">1.1 目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="1-2-方法-2">1.2 方法</h2><p>大致流程为：</p><ol><li>使用Angle View Net网络生成图像不同位置的抓取器方向 $P_{img}=(u,v,r_x,r_y,r_z,c)$，即图像中坐标的位置，其对应的夹爪旋转姿态，以及置信度。</li><li>对置信度高的预测，结合深度图计算距离和夹爪宽度$P_{cam}=x,y,z,rx,ry,rz,w$</li></ol><p><strong>（0）定义</strong></p><p>抓握姿势定义为 (x, y, z, rx, ry, rz, w)，其中(x, y, z)代表夹持器的位置，(rx, ry, rz)代表夹持器的旋转，w代表夹持器的宽度。</p><p>夹持器本文仅考虑平行夹爪，使用 (h, l, wmax) 定义，三个参数分别代表夹具的 高度、长度和最大宽度。</p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒（因为同一个位置进行抓取有不止一个可行的旋转）。</p><p>可以使用下面的模型，将方向解耦为接近方向和绕平面的旋转，将夹持器旋转预测作为一个分类问题进行预测，共有VxA类方向。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>网络通过将RGB图像栅格化，对于每一个网格，AVN预测一个1维VxA个元素的向量，包含每个方向的置信度。最终得到(VxA)xGHxGW的tensor。AVN最终的输出表示为每个角度的heatmap。</p><p>作者在代码中给出的是V=60,A=6的测试。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="1-3-思考-2">1.3 思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1>二、复现过程</h1><h2 id="2-1-环境搭建-2">2.1 环境搭建</h2><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n rgb_matters python=3.7</span><br><span class="line">conda activate rgb_matters</span><br></pre></td></tr></table></figure><p>下载程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/GouMinghao/rgb_matters</span><br><span class="line">cd rgb_matters</span><br></pre></td></tr></table></figure><p>安装PyTorch1.8.0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="2-2-测试Demo">2.2 测试Demo</h2><p>下载作者训练好的模型：<a href="https://drive.google.com/drive/folders/1upW4gvQk5ftXfpLHtvCogudpP4kNyoGq?usp=sharing">Google Drive</a></p><p>在代码目录创建一个<code>weights</code>的目录，然后将下载的模型放入其中，完成后文件夹结构如下</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rgbd_graspnet/</span><br><span class="line">├── check_label_integrity.py</span><br><span class="line">├── train.py</span><br><span class="line">├── train.sh</span><br><span class="line">├── vis_label.py</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">└── weights</span><br><span class="line">    ├── kn_jitter_79200.pth</span><br><span class="line">    ├── kn_no_norm_76800.pth</span><br><span class="line">    ├── kn_norm_63200.pth</span><br><span class="line">    ├── kn_norm_only_73600.pth</span><br><span class="line">    └── rs_norm_56400.pth</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230424162408.png" alt=""></p><h1>三、代码分析</h1><h2 id="3-1-输出结果分析">3.1 输出结果分析</h2><p><strong>（1）热力图获取</strong></p><p>使用下面的代码进行预测热力图</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = RGBNormalNet(num_layers=args.num_layers, use_normal=args.use_normal, normal_only=args.normal_only)</span><br><span class="line">state_dict = torch.load(weights_path)</span><br><span class="line">net.load_state_dict(state_dict[&quot;net&quot;], strict=False)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net.eval()</span><br><span class="line"></span><br><span class="line">rgb, _ = load_data(rgb_path, depth_path)</span><br><span class="line"></span><br><span class="line">rgb = rgb.unsqueeze(0).to(device)</span><br><span class="line"></span><br><span class="line">prob_map = net(rgb)</span><br></pre></td></tr></table></figure><p>其中的<code>prob_mat</code>是预测的热力图<code>shape=(batch_size, 360, h, w)</code>一共360张热力图（360张包括接近方向v=60和平面内旋转A=6）</p><p><strong>（2）夹爪姿态获取</strong></p><p>使用<code>convert_grasp()</code>函数从360张热力图中提取夹爪姿态。</p><p>夹爪姿态为<code>Grasp</code>实例，包括以下几个参数：</p><ul><li><code>score</code>：float类型，抓取得分</li><li><code>width</code>：float类型，夹爪宽度</li><li><code>height</code>：float类型，夹爪高度</li><li><code>depth</code>：float类型，夹爪深度</li><li><code>rotation_matrix</code>：shape(3, 3)数组，旋转矩阵</li><li><code>translation</code>：shape(3)数组，平移向量</li><li><code>object_id</code>：int类型，抓取物体类别</li></ul><p>具体参考下面两张图：</p><p><img src="https://img.mahaofei.com/img/202305142107952.png" alt="image.png"></p><p><img src="https://img.mahaofei.com/img/202304241634347.png" alt=""></p>]]></content>
    
    
    <summary type="html">复现上交提出的RGB Matters算法。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>抓取姿态估计算法调研</title>
    <link href="https://www.mahaofei.com/post/791dd0f5.html"/>
    <id>https://www.mahaofei.com/post/791dd0f5.html</id>
    <published>2023-04-23T07:16:08.000Z</published>
    <updated>2023-04-23T07:16:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Hybrid Physical Metric For 6-DoF Grasp Pose Detection</h1><blockquote><p><strong>标题</strong>：用于6D抓取检测的混合物理度量<br><strong>作者团队</strong>：清华大学（王生进）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/luyh20/FGC-GraspNet">https://github.com/luyh20/FGC-GraspNet</a></p></blockquote><h2 id="一、目标问题">一、目标问题</h2><p>单个物理指标会导致离散的抓取置信度分数，在百万抓取数据训练时会导致预测结果不准确。</p><p>本文定义了一种新的度量方式，基于力封闭度量、物体平面度、重力和碰撞测量。</p><p>本文设计了平面重力碰撞FGC-GraspNet，适用于多任务多分辨率学习体系。</p><h2 id="二、混合物理度量">二、混合物理度量</h2><p><img src="https://img.mahaofei.com/img/202304231518654.png" alt="image.png"></p><p><strong>（1）平面度</strong></p><p>平面度越高的抓的越稳。利用点的局部法向量的相似性计算平坦度得分。</p><p><strong>（2）重心度量</strong></p><p>夹持力更接近物体重心的更稳定。计算物体重心到两个接触点的连线的距离作为重力得分。</p><p><strong>（3）碰撞扰动度量</strong></p><p>当夹爪接近物体时容易发生碰撞，因此取夹爪两个最大行程端点与物体接触点的欧氏距离最小值作为碰撞扰动得分。</p><p><strong>（4）混合物理度量</strong></p><p>混合物理度量是上面度量的加权组合。</p><h2 id="三、-FGC-GraspNet">三、 FGC-GraspNet</h2><p>通过最远点采样FPS得到20000x3对点云输入，网络由PointNet++，FA分支、RD分支组成。</p><p><img src="https://img.mahaofei.com/img/202304231530817.png" alt=""></p><ul><li>PointNEt++用于提取点特征</li><li>低分辨率的特征进入FA分支进行前景分割和逐点逼近方向得分回归</li><li>高分辨率的特征用于RD旋转分支。</li></ul><h2 id="四、思考">四、思考</h2><p>将混合物理度量纳入LOSS的计算过程确实有意义。而且具有一定的复用性，其它算法也可借鉴此设计。</p><h1>Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter</h1><blockquote><p><strong>标题</strong>：基于体素的抓取网络：杂乱场景中实时6D抓取检测<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：CoRL2020<br><strong>时间</strong>：2020<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/vgn">https://github.com/ethz-asl/vgn</a></p></blockquote><h2 id="一、目标问题-2">一、目标问题</h2><p>本文提出了一种网络从深度相机中获得场景信息，预测6D抓取的网络。</p><h2 id="二、论文方法">二、论文方法</h2><p><strong>（1）网络架构</strong></p><p>由滤波器、卷积层组成的感知模块将输入体素映射为特征图，然后进行卷积、上采样操作，最后是三个独立的分支用于预测抓取质量、旋转和夹爪宽度。</p><p><strong>（2）抓取检测</strong></p><p>使用一些方法去除不可能的抓取姿势，然后应用非极大抑制来获得候选的抓取列表。</p><h2 id="三、思考">三、思考</h2><p>非常基础的方法，已经有人在此基础上进行了扩展并发表了顶会。</p><h1>Efficient Learning of Goal-Oriented Push-Grasping Synergy in Clutter</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：浙江大学（熊蓉）<br><strong>期刊会议</strong>：RAL<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy">https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy</a></p></blockquote><h2 id="一、目标问题-3">一、目标问题</h2><p>在混乱场景中抓取物体时，有时需要一些预抓取动作，例如推动。使机械臂能够分离目标对象并稳定的实现抓取。</p><h2 id="二、方法">二、方法</h2><p><img src="https://img.mahaofei.com/img/202304231536100.png" alt=""></p><p>环境准备：固定的RGBD相机拍摄工作空间，将RGBD投影到重力方向，使用颜色高度图和深度高度图表示每个状态。</p><p><strong>（1）有目标的抓取训练</strong></p><p>训练一个有目标条件下的抓取网络，当有足够的训练之后，成功抓取的Q值稳定。</p><p><strong>（2）有目标的推动训练</strong></p><p>训练一个有目标条件下的推动网络，推动的奖励函数是基于抓取网络反向训练设计的。</p><p><strong>（3）交替训练</strong></p><p>利用交替训练来解决物体分布不匹配的问题，进一步提高混乱环境中抓取策略性能。</p><h2 id="三、思考-2">三、思考</h2><p>推物体再抓取物体相当于一个两阶段方法，可以不用在底层进行训练，而是在高层的规划决策层来进行判断，发布任务是推物体还是抓物体。</p><h1>TransGrasp: Grasp Pose Estimation ofaCategory ofObjects byTransferring Grasps fromOnlyOne Labeled Instance</h1><blockquote><p><strong>标题</strong>：杂乱场景中面向目标的的推/抓协同有效学习<br><strong>作者团队</strong>：大连理工大学（孙怡）<br><strong>期刊会议</strong>：ECCV<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/yanjh97/TransGrasp">https://github.com/yanjh97/TransGrasp</a></p></blockquote><h2 id="一、目标问题-4">一、目标问题</h2><p>现有大多数方法需要大量的抓取数据来训练，为了解决这个问题，本文实现只标记一个对象预测一类对象的抓取姿态。</p><h2 id="二、方法-2">二、方法</h2><p><strong>（1）学习类别的对应关系</strong></p><ol><li>Shape Encoder和DIFDecoder组成神经网络，训练得到对象变形到模板的密集对应关系</li></ol><p><strong>（2）抓取姿态估计</strong></p><ol><li>点云首先从相机坐标系转换到对象坐标系</li><li>生成对象实例的变形到模板</li><li>将带有抓取注释的模型输入到DeformNet中获得模型的变形</li><li>由两者的共同模板见你对应关系，通过对准物体表面上的抓握点来引导抓握姿势的变换</li><li>通过refine模块进行优化</li><li>将优化后的抓握知识转换为相机坐标进行抓取</li></ol><h2 id="三、思考-3">三、思考</h2><p>这种算法只能实现与模板形状相似的物体进行抓取，而且每个类别要先手工标记1000个抓握姿势。</p><h1>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</h1><blockquote><p><strong>标题</strong>：ContactGraspNet：在杂乱场景中高效生成6-DoF抓取<br><strong>作者团队</strong>：NVIDIA<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a></p></blockquote><h2 id="1-目标问题-22">1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="2-方法-22">2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/202304231545643.png" alt=""></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/202304231546650.png" alt=""></p><h2 id="3-思考-22">3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1>RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images</h1><blockquote><p><strong>标题</strong>：RGB Matters：单RGBD图像学习学习7D抓取姿态<br><strong>作者团队</strong>：上海交通大学（卢策吾）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/GouMinghao/RGB_Matters">https://github.com/GouMinghao/RGB_Matters</a></p></blockquote><h2 id="一、目标问题-5">一、目标问题</h2><p>现有方法要么生成自由度很少的抓取姿态，要么只将不稳定的深度点云输入。</p><h2 id="二、方法-3">二、方法</h2><p><strong>（1）Angle-View Net</strong></p><p>预测像素级的夹持器旋转配置。直接回归四元数不太现实，而且不鲁棒。可以使用下面的模型，将夹持器旋转预测作为一个分类问题进行预测。</p><p><img src="https://img.mahaofei.com/img/202304231549478.png" alt=""></p><p>AVN最终的输出表示为角视图热图。</p><p><strong>（2）快速分析搜索</strong></p><p>AVN识别了7个自由度的其中五个，但是夹持器的宽度和夹持器沿轴方向的自由度还没有确定。</p><p>本文提出了基于碰撞和空抓取检测的快速分析搜索来计算宽度和距离。</p><p>通过对从0到Wmax采样，假设抓取器靠近由深度图重建的点云的对应点。过滤掉夹持器占用的空间中存在点、抓取空间没有点的两种情况。</p><p><img src="https://img.mahaofei.com/img/202304231549054.png" alt=""></p><h2 id="三、思考-4">三、思考</h2><p>本文使用了尽可能简单的思路解决抓取预测问题</p><p>将末端夹持器的旋转方向通过分类器进行回归计算。</p><p>将夹持器位置和宽度通过采样测试逐一排除得到最优解。</p><p>思路直观简单，可以尝试。</p><h1>CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation</h1><blockquote><p><strong>标题</strong>：CaTGrasp：从仿真中学习杂乱场景的类别级抓取<br><strong>作者团队</strong>：Rutgers University（美国罗格斯大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/wenbowen123/catgrasp">https://github.com/wenbowen123/catgrasp</a></p></blockquote><h2 id="一、目标问题-6">一、目标问题</h2><p>提出了一个框架学习工业对象的抓取，不需要真实的数据或手动注释</p><h2 id="二、方法-4">二、方法</h2><p>给定同一类别的3D模型的数据库，该方法学习</p><ul><li>以对象为中心的NUNOCS表示</li><li>hotmap：抓握过程中手-对象接触区域的任务实现成功的可能性</li><li>抓取姿势的编码本</li></ul><p><strong>（1）类别级标准NUNOCS表示</strong></p><p>将同一个类别的不同实例对象转换到标准空间，并缩放为标准大小。</p><p><strong>（2）稳定抓取学习</strong></p><p>首先给定从当前实例到规范模型的9D变换，将相同的变换应用于抓取来得到抓取建议。</p><p>将生成的抓取用于训练基于PointNet构建的网络，预测抓取质量。</p><p><strong>（3）实例分割</strong></p><p>使用了3D U-Net，将整个场景的点云作为输入，预测每点偏移到物体中心，将偏移点聚类为实例段。</p><p><strong>（4）仿真中生成训练数据</strong></p><p>利用PyBullet模拟生成合成数据。</p><h2 id="三、思考-5">三、思考</h2><p>该方法提出了一种使用仿真数据进行训练，减少人工标注的方法。抓取的方法没有太多创新，仍然需要每个类别提供多个预先的实例以及抓取姿态用于训练。</p><h1>Closed-Loop Next-Best-View Planning for Target-Driven Grasping</h1><blockquote><p><strong>标题</strong>：闭环次优视图规划用于目标驱动的抓取<br><strong>作者团队</strong>：ETH Zurich（苏黎世联邦理工学院）<br><strong>期刊会议</strong>：IROS<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/ethz-asl/active_grasp">https://github.com/ethz-asl/active_grasp</a></p></blockquote><h2 id="一、目标问题-7">一、目标问题</h2><p>从密集遮挡环境中抓取物体</p><h2 id="二、方法-5">二、方法</h2><p>该方法具有以下前提条件</p><ul><li>机械臂末端连接深度相机</li><li>相机光学中心和手抓中心已经校准</li><li>已知物体的部分视图和3D边界框</li></ul><p><img src="https://img.mahaofei.com/img/202304231603042.png" alt=""></p><p>首先将点云观测yt和相机姿态xt继承，重建为体素图。计算体素的可抓取性，以及可能的抓取姿态。如果可抓取性不满足要求，就调整机械臂位置计算下一张图</p><p><strong>（1）抓取检测</strong></p><p>使用体积抓取网络VGN进行抓取。该网络将体素网格M映射到抓握质量分数Q、平行抓握方向R、开口宽度W。</p><p>过滤掉指尖不在目标边界框的抓取姿态、无法找到反向运动学解的抓取姿态。</p><p><strong>（2）次优视图规划器</strong></p><p><strong>世界表示</strong>：使用TSDF（截断有符号距离函数）表示大小为 l 的立方体体素。</p><p><strong>视图生成</strong>：将候选视图生成在目标边界上半球内。</p><p><strong>信息增益</strong>：TSDF 重建的完整性对于抓取的检测和预测准确性有很大影响。因此使用了后侧体素 IG 公式的变体，对被遮挡具有负距离的体素使用光线投影来计算隐藏对象体素的数量。</p><ol><li>在策略更新的最大数量中加入时间预算</li><li>如果抓取分数低于给定的阈值，就会停止算法，因为获取不到有用信息</li><li>如果VGN在几帧内保持稳定的抓取配置，就停止</li></ol><h2 id="三、思考-6">三、思考</h2><p>该方法是在位姿估计的基础上进行的抓取预测，可以将该方法与Gen6D结合起来，获得物体的6D抓取位姿。</p><h1>Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection</h1><blockquote><p><strong>标题</strong>：边缘抓取网络：一种基于图的SE(3)不变的抓取检测方法<br><strong>作者团队</strong>：Northeastern University（美国东北大学）<br><strong>期刊会议</strong>：ICRA<br><strong>时间</strong>：2023<br><strong>代码</strong>：<a href="https://github.com/HaojHuang/Edge-Grasp-Network">https://github.com/HaojHuang/Edge-Grasp-Network</a></p></blockquote><h2 id="一、目标问题-8">一、目标问题</h2><p>以单个视角观察到的点云为输入，得到一组抓取姿态</p><h2 id="二、方法-6">二、方法</h2><p><strong>（1）裁剪点云</strong></p><p>给定一个点云p和接近点pa，只有接近点pa的相邻点会影响抓取，因此以pa为中心裁剪一个球。</p><p><strong>（2）PointNet卷积</strong></p><p>使用PointNet计算接近点与最近邻点，每个点的特征。</p><p><strong>（3）计算全局特征</strong></p><p>将逐点特征传递给MLP，用最大池化层生成一级全局特征。这些全局特征再与点特征相连传递到第二个MLP计算全局特征。</p><p>对于每个抓取，通过将全局特征与点特征连接来计算边缘特征，用分类器表示边缘抓取。</p><p><strong>（4）抓取评估</strong></p><p>使用sigmoid函数的四层MLP来预测抓取成功率，以边缘特征为输入计算抓取是否成功。</p><h2 id="三、思考-7">三、思考</h2><p>该方法类似于DenseFusion的思想，即提取逐点特征和全局特征，进行特征融合，本文得到的融合特征即边缘特征，利用该特征再使用分类器得到抓取位姿。</p>]]></content>
    
    
    <summary type="html">调研近三年顶会顶刊上的抓取姿态估计的论文</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="抓取姿态估计" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>CVPR2022高被引论文笔记</title>
    <link href="https://www.mahaofei.com/post/6ec34466.html"/>
    <id>https://www.mahaofei.com/post/6ec34466.html</id>
    <published>2023-04-23T02:45:17.000Z</published>
    <updated>2023-04-23T02:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、目标检测</h1><h2 id="1-1-视频目标检测">1.1 视频目标检测</h2><h3 id="Video-Swin-Transformer">Video Swin Transformer</h3><blockquote><p><strong>标题</strong>：视频 Swin Transformer<br><strong>作者团队</strong>：Microsoft Research Asia<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>现今大多数的视觉识别模型都是基于Transformer建立的，本文在此基础上进行调整，得到更好的速度和精度。</p><p><strong>（2）方法</strong></p><ol><li>总体架构</li></ol><p>视频定义为TxHxWx3，patch为2x4x4x3的块，每个patch有96个特征维度。该架构的主要组件是Video Swin Transformer模块，通过将标准的Transformer的Multihead self-attention(MSA)模块替换为基于3D Shift Window的MSA模块，来实现。</p><p><img src="https://img.mahaofei.com/img/202305071021361.png" alt="image.png"></p><ol start="2"><li>3D MSA模块</li></ol><p>由于视频有时间维度，全局自注意模块会导致巨大的计算和内存成本。MSA模块就比传统的全局自注意模块要高效。</p><p>更进一步，基于Swin Transformer的2D移位窗口扩展到3D，实现了跨窗口链接，保证了体系结构的表达能力。</p><p><img src="https://img.mahaofei.com/img/202305071033856.png" alt="image.png"></p><h1>二、图像分割</h1><h1>三、图像处理</h1><h2 id="3-1-图像合成">3.1 图像合成</h2><h3 id="High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models">High-Resolution Image Synthesis with Latent Diffusion Models</h3><blockquote><p><strong>标题</strong>：具有潜在扩散模型的高分辨率图像合成<br><strong>作者团队</strong>：海德堡大学；Runway ML<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>扩散模型已经在包括图像数据在内的很多数据上，实现了很好的数据合成效果。但这些模型由于直接操作像素，需要昂贵的GPU资源。</p><p>本文提出的潜在扩散模型，达到了降低复杂性和保留细节的平衡点。</p><p><strong>（2）方法</strong></p><p>主要方法是：使用自动编码模型，学习一个在感知上与图像空间等效的空间，压缩学习阶段和生成学习阶段来减少资源需求。</p><ol><li>感知压缩模型<br>利用了结合perceptual loss, patch-based, adversarial objective的自动编码器。</li><li>潜在扩散模型<br>扩散模型是概率模型，通过逐渐对正态分布变量去噪来学习数据分布。<br>通过由自动编码器得到的高效、低维的空间，与高维像素空间相比更适合生成模型。</li><li>调节机制<br>通过使用交叉注意力机制增强基础网络UNet，能够处理各种模态的输入。</li></ol><p><strong>（3）思考</strong></p><p>将需要高运算量的像素操作，通过自动编码转换为了低维空间的操作，节省了计算量。</p><h1>四、三维视觉</h1><h1>五、位姿估计</h1><h1>六、机器人</h1><h1>七、神经网络</h1><h2 id="7-1-神经网络结构设计">7.1 神经网络结构设计</h2><h3 id="A-ConvNet-for-the-2020s">A ConvNet for the 2020s</h3><blockquote><p><strong>标题</strong>：2020s的ConvNet<br><strong>作者团队</strong>：Facebook AI<br><strong>期刊会议</strong>：CVPR<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a href="https://github.com/facebookresearch/ConvNeXt">https://github.com/facebookresearch/ConvNeXt</a></p></blockquote><p><strong>（1）目标问题</strong></p><p>20年以来，由于Vision Transformers的引入，它开始快速取代卷积神经网络。但只使用Transformers也有些问题，因此后来又出现了hierarchical Transformers，其中加入了几个卷积神经网络作为先验。但这些方法都可以归结为Transformers的优势。</p><p>本文想要探讨纯卷积神经网络所能实现的极限。</p><p><strong>（2）最佳方法</strong></p><ol><li><strong>训练技术</strong>：使用AdamW优化器、数据增强、随机擦除、正则化等方法可以显著提高训练模型的性能</li><li><strong>宏观设计</strong>：<ul><li>阶段比例：ResNet中各阶段的比例很大程度是经验获得的，SwinTransformer的比例是1:1:3:1，传统的ResNet比例是(3,4,6,3)，此处调整为(3,3,9,3)与SwinT相同，发现也提高了模型准确率</li><li>模块设计：标准的ResNet模块包括一个7x7步长2的卷积层，然后是一个最大池化层。此处模仿SwinT，设计为4x4步长为4的卷积层作为基础模块。</li></ul></li><li><strong>使用分组卷积技术</strong>，可以有效提高网络性能</li><li><strong>反向瓶颈</strong>：使MLP的隐藏维度比输入维度宽4倍，这在几个ConvNet中以及Transformer中设计思路相同。</li><li><strong>更大的卷积核</strong>：尽管堆叠小卷积核可以有效利用硬件，但测试证明，总体上大卷积核能够提高模型性能</li><li><strong>微观设计</strong>：<ul><li>更少的归一化层</li><li>使用层归一化LN代替批归一化BatchNorm</li><li>分离下采样层：ResNet中，下采样是通过每个阶段开始的残差块实现的，在层和层之间加入单独的下采样层发现可以提高准确率</li></ul></li></ol><p><strong>（3）总结</strong></p><ol><li>尽可能丰富数据，增大随机化程度：使用AdamW优化器、数据增强、随机擦除、正则化等方法</li><li>使用更优化的网络结构：调整各阶段卷积比例、使用反向瓶颈设计、更少的归一化层、更大的卷积核、在每个阶段之间加入下采样层。</li></ol>]]></content>
    
    
    <summary type="html">阅读CVPR的高被印论文，开拓视野。</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="顶会顶刊" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E9%A1%B6%E4%BC%9A%E9%A1%B6%E5%88%8A/"/>
    
    
    <category term="笔记" scheme="https://www.mahaofei.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="CVPR" scheme="https://www.mahaofei.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>如何使用Git管理项目代码</title>
    <link href="https://www.mahaofei.com/post/dd16f220.html"/>
    <id>https://www.mahaofei.com/post/dd16f220.html</id>
    <published>2023-04-17T07:00:50.000Z</published>
    <updated>2023-04-17T07:00:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、注册Github并创建仓库</h1><p>这一步不细说了，需要科学上网，参考<a href="https://www.mahaofei.com/post/96c83ac9.html">这篇文章</a>，[[03_科学上网方法（如何访问Google, ChatGPT）|Google学术访问方法]]。</p><h1>二、下载Git并配置</h1><h2 id="2-1-Git安装">2.1 Git安装</h2><p>下载安装<a href="https://link.zhihu.com/?target=http%3A//git-scm.com/downloads">Git</a>。</p><p>在资源管理器内右键，选择 <code>Git bash here</code> 打开 Git 界面。</p><h2 id="2-2-Git配置">2.2 Git配置</h2><p>输入下面的代码，按下回车，生成ssh密钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;邮箱地址&quot;</span><br></pre></td></tr></table></figure><p>出现要求设置密码，可以不用设置，连续回车两次就可以。</p><p>打开<code>C:\Users\用户名\.ssh</code>，可以看到有一个<code>id_rsa.pub</code>文件，这就是刚才生成的密钥。</p><p>使用记事本打开此文件，复制里面的密钥内容。</p><h2 id="2-3-Github添加ssh-key">2.3 Github添加ssh key</h2><p>进入<a href="https://github.com/">Github官网</a>，点击右上角【setting --&gt; SSH and GPG keys --&gt; New SSH key】，在这里添加密钥，其中</p><ul><li>Title：自己写一个ssh key的名字，用于区分多个ssh key</li><li>Key：刚刚复制的密钥<br>填写完成后点击Add SSH key添加。</li></ul><p>然后在git bash中输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>如果连接成功，会让你输入<code>yes/no</code>，输入yes即可。</p><h2 id="2-4-配置用户名和邮箱">2.4 配置用户名和邮箱</h2><p>输入下面的代码配置自己的用户名和邮箱，两个信息都要和Github账号的信息一致</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;用户名&quot;</span><br><span class="line">git config --global user.email &quot;邮箱&quot;</span><br></pre></td></tr></table></figure><h1>三、代码管理</h1><h2 id="3-1-代码上传">3.1 代码上传</h2><p><strong>（1）初始化</strong></p><p>创建一个文件夹，在这个文件夹内，右键<code>git bash here</code>，然后输入<code>git init</code>完成初始化。</p><p>可以看到目录中出现了一个<code>.git</code>隐藏文件夹，这说明已经完成了初始化。</p><p><strong>（2）链接远程仓库</strong></p><p>在刚刚的<code>git bash</code>窗口，输入下面的命令同步到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p>如果出现fatal: remote origin already exists.可按以下步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br><span class="line">git remote add origin git@github.com:用户名/仓库名.git</span><br><span class="line">git pull git@github.com:用户名/仓库名.git</span><br></pre></td></tr></table></figure><p><strong>（3）上传本地文件</strong></p><p>添加本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add.</span><br></pre></td></tr></table></figure><p>提交本地文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;说明信息，一般说明本次提交更新了什么&quot;</span><br></pre></td></tr></table></figure><p>推送到远端仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push git@github.com:用户名/仓库名.git</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure><h2 id="3-2-拉取代码">3.2 拉取代码</h2><p>从项目中拉取代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><p>如果出现<code>fatal: refusing to merge unrelated histories</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master --allow-unrelated-histories</span><br></pre></td></tr></table></figure><h2 id="3-3-分支管理">3.3 分支管理</h2><p><strong>（1）查看分支</strong></p><p>在命令行窗口的光标处，输入git branch命令，查看 Git 仓库的分支情况。分支前有*表示是当前所在的分支。</p><p><strong>（2）创建分支</strong></p><p>使用下面的命令创建一个名为a的分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch a</span><br></pre></td></tr></table></figure><p><strong>（3）分支切换</strong></p><p>在命令行窗口的光标处，输入git checkout a命令，切换到a分支。</p><p><strong>（4）合并分支</strong></p><p>切换到master分支，然后输入git merge a命令，将a分支合并到master分支。</p><p><strong>（5）删除分支</strong></p><p>在命令行窗口的光标处，输入git branch -d a命令，删除a分支。</p><p><strong>（6）为分支添加标签</strong></p><p>在命令行窗口的光标处，输入git tag test_tag命令，为当前分支添加标签test_tag</p><h2 id="3-4-修改分支名称">3.4 修改分支名称</h2><p>假设分支名称为oldName，想要修改为 newName</p><ol><li>本地分支重命名(还没有推送到远程)</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><ol start="2"><li>远程分支重命名 (已经推送远程-假设本地分支和远程对应分支名称相同)</li></ol><p>重命名远程分支对应的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m oldName newName</span><br></pre></td></tr></table></figure><p>到github修改默认分支的分支名。</p><p>上传新命名的本地分支</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin newName</span><br></pre></td></tr></table></figure><p>把修改后的本地分支与远程分支关联</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --set-upstream-to origin/newName</span><br></pre></td></tr></table></figure><p>注意：如果本地分支已经关联了远程分支，需要先解除原先的关联关系：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch --unset-upstream </span><br></pre></td></tr></table></figure><h2 id="3-5-报错信息">3.5 报错信息</h2><p><strong>（1）error: src refspec master does not match any. error: failed to push some refs to</strong></p><p>仔细检查push的是<code>master</code>分支还是<code>main</code>分支。</p><h1>附：Github Profile 美化</h1><blockquote><p>参考: <a href="https://zhuanlan.zhihu.com/p/416759197">https://zhuanlan.zhihu.com/p/416759197</a></p></blockquote><p><strong>（1）准备工作</strong></p><p>需要新建一个和 Github 用户名相同的仓库，新建时下面会有绿框提示。</p><p>注意设置为Public并添加Readme。</p><p><strong>（2）打字特效</strong></p><p>打字特效生成网站：<a href="https://readme-typing-svg.herokuapp.com/demo/">Readme Typing SVG - Demo Site</a></p><p>然后把生成的代码粘贴进去 Readme</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">&quot;center&quot;</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://readme-typing-svg.herokuapp.com?font=Monospace&amp;pause=1000&amp;color=000000&amp;random=false&amp;width=435&amp;lines=Haofei+Ma+-+Infinite+Advancing&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;Typing SVG&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://readme-typing-svg.herokuapp.com?font=Monospace&amp;pause=1000&amp;color=000000&amp;random=false&amp;width=435&amp;lines=Haofei+Ma+-+Infinite+Advancing" alt=""></p><p><strong>（3）Github Badge</strong></p><p>网站：<a href="https://shields.io/">https://shields.io/</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;div align=&quot;center&quot;&gt;</span><br><span class="line"></span><br><span class="line">[![](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dgithub%26queryKey%3DHaofeiMa&amp;query=%24.data.totalSubs&amp;suffix=%20followers&amp;logo=github&amp;label=Github&amp;color=black)](https://github.com/HaofeiMa)</span><br><span class="line"></span><br><span class="line">[![](https://img.shields.io/badge/dynamic/xml?url=https%3A%2F%2Fblog.csdn.net%2Fweixin_44543463&amp;query=%2F%2Fli%5B3%5D%2Fdiv%2Fspan%5B1%5D%2Ftext()%5B1%5D&amp;suffix=%20stars&amp;logo=csdn&amp;label=CSDN&amp;color=red)](https://blog.csdn.net/weixin_44543463)</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dgithub%26queryKey%3DHaofeiMa&amp;query=%24.data.totalSubs&amp;suffix=%20followers&amp;logo=github&amp;label=Github&amp;color=black" alt=""></p><p><img src="https://img.shields.io/badge/dynamic/xml?url=https%3A%2F%2Fblog.csdn.net%2Fweixin_44543463&amp;query=%2F%2Fli%5B3%5D%2Fdiv%2Fspan%5B1%5D%2Ftext()%5B1%5D&amp;suffix=%20stars&amp;logo=csdn&amp;label=CSDN&amp;color=red" alt=""></p><p><strong>（4）Github统计信息</strong></p><p>替换自己的用户名</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://github-readme-stats.vercel.app/api?username=HaofeiMa&amp;show_icons=true&amp;icon_color=CE1D2D&amp;text_color=718096&amp;bg_color=ffffff&amp;hide_title=true&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://github-readme-stats.vercel.app/api?username=HaofeiMa&amp;show_icons=true&amp;icon_color=CE1D2D&amp;text_color=718096&amp;bg_color=ffffff&amp;hide_title=true" alt=""></p><p><strong>（4）GitHub 连续打卡</strong></p><p>Github 地址：<a href="https://github.com/DenverCoder1/github-readme-streak-stats">DenverCoder1/github-readme-streak-stats</a></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">align</span>=<span class="string">&quot;center&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span>  <span class="attr">src</span>=<span class="string">&quot;https://github-readme-streak-stats.herokuapp.com/?user=HaofeiMa&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://github-readme-streak-stats.herokuapp.com/?user=HaofeiMa" alt=""></p><p><strong>（5）Github统计图</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;div align=&quot;center&quot;&gt;</span><br><span class="line">    &lt;img  src=&quot;https://github-readme-activity-graph.vercel.app/graph?username=HaofeiMa&quot; /&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><img src="https://github-readme-activity-graph.vercel.app/graph?username=HaofeiMa&amp;bg_color=ffffff&amp;theme=github-compact&amp;color=666666&amp;hide_title=true" alt=""></p>]]></content>
    
    
    <summary type="html">当有多台设备，或者同一个项目有多个版本的代码时，利用git管理项目代码就十分必要了。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="科研利器" scheme="https://www.mahaofei.com/tags/%E7%A7%91%E7%A0%94%E5%88%A9%E5%99%A8/"/>
    
    <category term="Git" scheme="https://www.mahaofei.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>【抓取算法】Contact GraspNet</title>
    <link href="https://www.mahaofei.com/post/c18d351e.html"/>
    <id>https://www.mahaofei.com/post/c18d351e.html</id>
    <published>2023-04-07T07:52:27.000Z</published>
    <updated>2023-04-07T07:52:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1>一、论文笔记</h1><blockquote><p><strong>标题</strong>：Contact-GraspNet: 在杂乱场景中高效生成6-DoF抓取<br><strong>期刊会议</strong>：ICRA2021<br><strong>作者团队</strong>：Martin Sundermeyer（NVIDIA）<br><strong>时间</strong>：2021<br><strong>代码</strong>：<a href="https://github.com/NVlabs/contact_graspnet">https://github.com/NVlabs/contact_graspnet</a><br><strong>数据集</strong>：</p></blockquote><h2 id="1-1-目标问题-2">1.1 目标问题</h2><p>提出了一种端到端的网络，从图像的深度数据中生成6D抓取分布。</p><h2 id="1-2-方法">1.2 方法</h2><p>使用原始的深度图，以及（可选使用对象掩码），生成6D抓取建议以及抓取宽度。</p><p><strong>（1）抓取表示方法</strong></p><p>可以发现，大多是可以预测的两手指抓取，在抓取前至少可以看到两个接触点的一个。因此可以将抓取问题简化为估计平行板抓取器的3D抓取旋转和抓取宽度。</p><p><img src="https://img.mahaofei.com/img/20230404152359.png" alt="image.png"></p><p>其中a是接近向量，b是抓取基线向量，d是从抓取基线到抓取基座的距离。使用这种表示方法可以加速学习过程，提高预测精度，且没有歧义和间断区域。</p><p><strong>（2）数据生成</strong></p><p>使用了ACRONYM数据集。在场景中以随机稳定的姿态放置具有密集抓取注释的对象网格。其中会导致夹爪与模型碰撞的抓取姿态将被删除。</p><p><strong>（3）网络</strong></p><p>使用PointNet++中提出的集合概要和特征传播层来构建非对称的U形网络。</p><p>网络有四个检测头，每个检测头包括两个1D卷积层，每个点输出s∈R，z1∈R3，z2∈R3、o∈R10，从中我们形成了我们的抓取表示。</p><p>将抓取的宽度划分为10个等距的抓取宽度，来抵消数据不平衡问题，然后选择置信度最高的抓取宽度表示。由于接近方向和基线方向是正交的，通过进行正交归一化预测，将这一性质加入到训练过程，有助于3D旋转的回归。</p><p><img src="https://img.mahaofei.com/img/20230404153946.png" alt="image.png"></p><h2 id="1-3-思考">1.3 思考</h2><p>在数据集中预先定义好了抓取姿态，然后进行监督训练。使用时根据深度图首先确定物体所在区域，然后利用其点云预测抓取分布。</p><p>自定义物体的数据集不易制作。</p><h1>二、算法复现</h1><h2 id="2-1-准备工作-2">2.1 准备工作</h2><p><strong>（1）环境搭建</strong></p><p>下载代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/NVlabs/contact_graspnet.git</span><br></pre></td></tr></table></figure><p>创建虚拟环境<br>（这个环境是没问题的，如果出现依赖不满足要求的情况，可以先删掉那项，创建完环境后再手动安装）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f contact_graspnet_env.yml</span><br></pre></td></tr></table></figure><p>重新编译<code>pointnet_tfops</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh compile_pointnet_tfops.sh</span><br></pre></td></tr></table></figure><p><strong>（2）模型和数据准备</strong></p><p>从作者给出的连接下载<a href="https://drive.google.com/drive/folders/1tBHKf60K8DLM5arm-Chyf7jxkzOr5zGl?usp=sharing">trained models</a>，并将它们放到<code>./checkpoints/</code>，下载<a href="https://drive.google.com/drive/folders/1v0_QMTUIEOcu09Int5V6N2Nuq7UCtuAA?usp=sharing">test data</a>，并将它们放到<code>./test_data</code></p><h2 id="2-2-预测抓取">2.2 预测抓取</h2><p>给定一个深度图(.npy文件/单位m)，相机内参，2D实例分割图，运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/inference.py \</span><br><span class="line">       --np_path=test_data/0.npy \</span><br><span class="line">       --local_regions --filter_grasps</span><br></pre></td></tr></table></figure><p><code>--np_path</code>：输入的.npz/.npy文件，带有深度、内参、实力分割图、RGB信息<br><code>--ckpt_dir</code>：checkpoint目录，默认为<code>checkpoint/scene_test_2048_bs3_hor_sigma_001</code>，非常干净的深度数据使用<code>scene_2048_bs3_rad2_32</code>，非常混乱的深度数据使用<code>scene_test_2048_bs3_hor_sigma_0025</code><br><code>--local_regions</code>：裁剪的3D实例分割<br><code>--filter_grasps</code>：筛选抓取点，使他们只为于对象的表面<br><code>--skip_border_objects</code>：忽略碰到深度图边缘的实例分割<br><code>--forward_passes</code>：前向计算的次数，增加可以提高抓取的采样点<br><code>--z_range</code>：[min, max]的z值来裁剪输入点云<br><code>--arg_configs TEST.second_thres:0.19 TEST.first_thres:0.23</code>：覆盖抓取的配置置信度来获得更多或更少的抓取候选</p><h2 id="2-3-训练网络">2.3 训练网络</h2><p><strong>（1）下载数据集</strong></p><ul><li>下载<a href="https://drive.google.com/file/d/1zcPARTCQx2oeiKk7a-wdN_CN-RUVX56c/view?usp=sharing">Acronym</a>数据集</li><li>从<a href="https://www.shapenet.org/">https://www.shapenet.org/</a>下载ShapeNet meshe</li><li>创建watertiget<ul><li>下载并构建<a href="https://github.com/hjwdzh/Manifold">https://github.com/hjwdzh/Manifold</a></li><li>创建watertight mesh，假设物体路径为model.obj：<code>manifold model.obj temp.watertight.obj -s</code></li><li>简化：<code>simplify -i temp.watertight.obj -o model.obj -m -r 0.02</code></li></ul></li></ul><p>下载10000个带有Contact抓取信息的桌面训练场景<a href="https://drive.google.com/drive/folders/1eeEXAISPaStZyjMX8BHR08cdQY4HF4s0">Google Drive</a>，解压为下面的格式</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">acronym</span><br><span class="line">├── grasps</span><br><span class="line">├── meshes</span><br><span class="line">├── scene_contacts</span><br><span class="line">└── splits</span><br></pre></td></tr></table></figure><p><strong>（2）训练Contact-GraspNet</strong></p><p>如果在没有外设的服务器上训练，设置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYOPENGL_PLATFORM=&#x27;egl&#x27;</span><br></pre></td></tr></table></figure><p>使用配置文件<code>contact_graspnet/config.yaml</code>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python contact_graspnet/train.py --ckpt_dir checkpoints/your_model_name \</span><br><span class="line">                                 --data_path /path/to/acronym/data</span><br></pre></td></tr></table></figure><p><strong>（3）生成自己的Contact Grasps和场景</strong></p><p>所下载的<code>scene_contacts</code>是从Acronym数据集生成的，要生成自己的数据集，下载安装<a href="https://github.com/NVlabs/acronym">acronym_tools</a>。</p><p>第一步，对象的6D抓取被映射到保存在<code>mesh_contacts</code>的接触点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_contact_infos.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>根据生成的<code>mesh_contacts</code>，可以创建桌面场景保存到<code>scene_contacts</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym</span><br></pre></td></tr></table></figure><p>一个线程大约花费3天，可以多次运行命令在多个核上并行处理。</p><p>可视化显示创建的桌面场景和抓取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python tools/create_table_top_scenes.py /path/to/acronym \</span><br><span class="line">       --load_existing scene_contacts/000000.npz -vis</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">复现NVIDIA提出的抓取估计算法Contact GraspNet</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%8A%93%E5%8F%96/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="抓取" scheme="https://www.mahaofei.com/tags/%E6%8A%93%E5%8F%96/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>使用内网穿透SakuraFrp远程连接服务器</title>
    <link href="https://www.mahaofei.com/post/9ed2c32f.html"/>
    <id>https://www.mahaofei.com/post/9ed2c32f.html</id>
    <published>2023-04-05T05:59:35.000Z</published>
    <updated>2023-04-05T05:59:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Linux端配置</h1><p><strong>（1）ssh配置</strong></p><p>安装ssh服务器与客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt -y install openssh-server</span><br><span class="line">sudo apt -y install openssh-client</span><br></pre></td></tr></table></figure><p>配置ssh客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><ul><li>​将<code>PermitRootLogin prohibt-password</code> 修改为 <code>PermitRootLogin yes</code></li><li>将<code>PasswordAuthentication yes</code> 前的#删除，取消注释</li></ul><p>重启ssh服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh restart</span><br></pre></td></tr></table></figure><p>查看ssh服务运行状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh status</span><br></pre></td></tr></table></figure><p><strong>（2）Sakura配置</strong></p><p><a href="https://www.natfrp.com/user/">SakuraFrp</a></p><p>进入隧道列表新建隧道</p><ul><li>尽量选择国内节点</li><li>隧道类型为TCP隧道</li><li>本机端口为SSH</li><li>主机ip默认127.0.0.1即可(代指内网穿透本机)</li></ul><p><img src="https://img.mahaofei.com/img/20230405140317.png" alt=""></p><p>在官网下载对应版本的frpc，复制下载链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget -O frpc &lt;下载地址&gt;</span><br><span class="line">chmod 755 frpc</span><br><span class="line">ls -ls frpc</span><br><span class="line">md5sum frpc</span><br><span class="line">frpc -v</span><br></pre></td></tr></table></figure><p>隧道配置文件中复制隧道密钥</p><p>Ubuntu中使用下面的命令开启隧道</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frpc -f &lt;复制的密钥&gt;</span><br></pre></td></tr></table></figure><h1>Windows端配置</h1><p>打开【设置-应用-添加功能】，添加OpenSSH 服务器和OpenSSH 客户端。</p><p>打开服务，找到 OpenSSH SSH Server 和 OpenSSH Authentication Agent -&gt; 启动服务并设为自动。</p><p>打开 power shell，使用以下命令检查安装和运行情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-Service sshd</span><br></pre></td></tr></table></figure><p>打开Sakura官网，打开隧道列表，点击要连接的隧道，点击一键认证，下载exe认证程序并运行。</p><p>然后使用<code>ssh -p &lt;端口号&gt; &lt;用户名&gt;@&lt;地址&gt;</code>进行远程连接</p><h1>VSCode远程ssh开发环境</h1><p>安装插件 <code>Remote - SSH</code></p>]]></content>
    
    
    <summary type="html">不想使用向日葵和todesk等工具远程连接桌面，而且个人电脑和服务器也不在一个局域网下，想要远程连接服务器，因此考虑使用内网穿透。</summary>
    
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
    <category term="Linux工具" scheme="https://www.mahaofei.com/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/Linux%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="实用工具" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>【6D位姿估计算法】Gen6D算法</title>
    <link href="https://www.mahaofei.com/post/76335f84.html"/>
    <id>https://www.mahaofei.com/post/76335f84.html</id>
    <published>2023-03-29T13:22:39.000Z</published>
    <updated>2023-03-29T13:22:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1>论文笔记</h1><h2 id="1-介绍">1. 介绍</h2><h3 id="1-1-目标问题">1.1 目标问题</h3><p>现有的位姿估计算法要么需要高质量的物体模型，要么需要提供额外的深度图或物体掩码图，这对于位姿估计的实际应用有很大的限制。本文提出的方法只需要一些物体的姿态图像，就能够在任意环境中预测物体位姿。</p><p>作者认为一个位姿估计器应该具有以下特点：</p><ul><li>通用性：可以应用于任意物体，而无需对对象或类别进行训练</li><li>无模型：用于一个未见过的物体时，只需要一些已知姿态的参考图像来定义物体参考坐标系即可</li><li>输入简单：仅输入RGB图像来估计位姿，而不需要深度图或物体掩码图</li></ul><p><strong>（1）如何设计视角选择器，从参考图像中找到与查询图像视角最接近的</strong></p><p>本文使用神经网络对查询图像和参考图像进行逐像素比较，产生相似性得分，并选择具有最高相似性得分的参考图像。并添加了全局归一化层和自注意层来共享不同参考图像之间的相似性信息，为选择最相似的参考图像提供了上下文信息。</p><p><strong>（2）实现没有模型的姿态优化</strong></p><p>本文提出了一种新的基于三维空间的姿态优化方法，给定一个查询图像和一个输入姿态，找到几个接近输入姿态的参考图像，将这些参考图像投影回3D空间中，构建特征空间，通过3D的CNN将构建的特征空间与查询图像的特征相匹配，来优化姿态。</p><h3 id="1-2-现有工作">1.2 现有工作</h3><p>现有位姿估计方法大都是基于特定实例的，不能推广到未见过的物体，通常都需要根据物体3D模型来渲染大量图像进行训练。有一些方法可以推广到类别级，也不需要对象的模型，但仍然无法预测没见过的类别的物体。</p><h2 id="2-实现方法">2. 实现方法</h2><p><strong>数据规范化</strong>：对于每个物体，通过对参考图像中的点进行三角测量等方法估计物体的大致大小，然后对物体坐标系进行归一化，使物体中心位于原点，大小为1，此时物体位于原点的单位球体内。</p><p>Gen6D包括一个物体检测器，一个视角选择器，一个姿态优化器。</p><p><img src="https://img.mahaofei.com/img/20230403203020.png" alt="image.png"></p><p>物体检测其首先利用查询图像和参考图像来检测物体所在区域。然后视角选择器将查询图像于参考图像相匹配，产生粗略的初始姿态。最后由姿态优化器进一步细化以得到精确的对象姿态。</p><h3 id="2-1-物体检测">2.1 物体检测</h3><p>将检测问题分解成两部分</p><ol><li>找到对象中心的2D投影点q</li><li>估计包围单位球体的正方形边界框。</li></ol><p><img src="https://img.mahaofei.com/img/20230403204751.png" alt="image.png"></p><p>物体中心的深度可以使用$d=2f/S_q$求得，其中2是单位球体的直径，f是虚拟焦距（将主点设为投影点q），$S_q$是边界框边长。这就是物体的初始平移。</p><blockquote><p>问题：这里将物体归一化之后求出的深度d还是真实深度吗？虚拟焦距又是如何确定的？</p></blockquote><p>检测器使用了VGG网络提取参考图像和查询图像的特征图，然后将所有参考图像的特征图作为卷积核与查询图像的特征图卷积，得到分数图。考虑尺度差异，设置再多个预定义尺度上进行卷积，最后得到热力图和比例图。选择热力图上的最大值位置作为对象中心2D投影，使用比例图上相同比例的比例作为边界框的大小$S_q=S_r*s$。</p><blockquote><p>问题：这里将所有参考图像的特征图都进行卷积，那么参考图像上物体特征和背景特征是如何区分的？</p></blockquote><h3 id="2-2-视角选择">2.2 视角选择</h3><p>将查询图像与每个参考图像比较，计算相似性得分。计算每个参考图像和查询图像的元素乘积，获得得分图，并计算相似性参数。</p><p><img src="https://img.mahaofei.com/img/20230404192903.png" alt="image.png"></p><p><strong>（1）平面内旋转</strong><br>为了考虑平面内旋转，本文将参考图像旋转Na个预定义角度，查询时使用所有旋转版本进行逐元素乘积。</p><p><strong>（2）全局归一化</strong><br>使用参考图像的所有特征图计算的均值和方差，对相似度网络生成的特征图进行归一化。这样做可以用特征图的分布来编码上下文相似性，并放大不同图像之间的相似性差异。</p><p><strong>（3）参考视角变换</strong><br>在所有参考图像的相似性特征向量上应用变换，包括它们的视角、注意力层。这样的变换器使得特征向量相互通信以编码上下文信息，有助于确定最相似的参考图像。</p><h3 id="2-3-姿态优化">2.3 姿态优化</h3><p>经过上面两个步骤，我们已经有了粗略的物体位姿。本步骤对位姿进行优化。</p><p>选择接近输入姿态的6个参考图像，通过CNN提取特征图，然后将特征图投影到3D空间中，并计算特征的均值和方差作为空间顶点的特征。<br>对于查询图像，使用同样的CNN提取特征图，将特征图投影到3D空间中，并将查询特征与参考图像特征的均值和方差连接起来。</p><p>最后在空间特征上使用3DCNN预测残差来更新输入姿态。</p><p><img src="https://img.mahaofei.com/img/20230404195340.png" alt="image.png"></p><h1>3. 实验分析</h1><h1>二、算法复现</h1><h2 id="2-1-环境搭建">2.1 环境搭建</h2><h3 id="2-1-1-Python环境">2.1.1 Python环境</h3><p>创建[[02_Anaconda的基本使用与在Pycharm中调用|Anaconda虚拟环境]]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gen6d python=3.7</span><br><span class="line">conda activate gen6d</span><br></pre></td></tr></table></figure><p>安装pytorch环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 -c pytorch</span><br></pre></td></tr></table></figure><p>安装依赖，打开<code>requirements.txt</code>，删除其中的pytorch, torchvision, cudatoolkit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="2-1-2-自制数据集工具">2.1.2 自制数据集工具</h3><p><strong>（1）COLMAP</strong></p><p>参考官网教程：<a href="https://colmap.github.io/install.html">https://colmap.github.io/install.html</a></p><p>安装依赖库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-regex-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev \</span><br><span class="line">    libcgal-qt5-dev</span><br></pre></td></tr></table></figure><p>下载COLMAP源代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/colmap/colmap</span><br><span class="line">cd colmap</span><br></pre></td></tr></table></figure><p>修改<code>CMakeLists.txt</code>文件，添加下面的内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_CUDA_ARCHITECTURES &quot;70&quot;)</span><br></pre></td></tr></table></figure><p>开始编译、安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -GNinja</span><br><span class="line">ninja</span><br><span class="line">sudo ninja install</span><br></pre></td></tr></table></figure><p><strong>（2）CloudCompare</strong></p><p>安装Flatpak</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install flatpak</span><br></pre></td></tr></table></figure><p>安装Software Flatpak plugin</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-software-plugin-flatpak</span><br></pre></td></tr></table></figure><p>添加Flathub repository</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo</span><br></pre></td></tr></table></figure><p>安装CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak install flathub org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p>运行CloudCompare</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><strong>（3）安装ffmpeg</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ffmpeg</span><br></pre></td></tr></table></figure><h2 id="2-2-数据集准备">2.2 数据集准备</h2><h3 id="2-2-1-官方数据集">2.2.1 官方数据集</h3><p><strong>（1）下载数据集</strong></p><p>从<a href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EkWESLayIVdEov4YlVrRShQBkOVTJwgK0bjF7chFg2GrBg?e=Y8UpXu">原作者给出的链接</a>中下载预训练模型，GenMOP数据集和processed LINEMOD数据集。</p><p><strong>（2）组织数据集</strong></p><p>将下载的文件按照下面的格式进行整理。</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- model</span><br><span class="line">        |-- detector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- selector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- refiner_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br></pre></td></tr></table></figure><h3 id="2-2-2-自制数据集">2.2.2 自制数据集</h3><p><strong>（1）视频录制</strong></p><p>使用手机录制目标物体的参考视频和测试视频。注意：参考视频需要满足以下条件</p><ul><li>参考视频中对象是静态的</li><li>参考视频中背景尽可能纹理丰富且平整，摄像角度要尽可能覆盖每个角度，以便COLMAP恢复相机姿态</li></ul><p><strong>（2）组织文件</strong></p><p>将视频按照下面的路径进行组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- video</span><br><span class="line">           |-- mouse-ref.mp4</span><br><span class="line">           |-- mouse-test.mp4</span><br></pre></td></tr></table></figure><p><strong>（3）将参考视频拆分为图像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 每10帧保存一张图像，最大图像边长为960</span></span></span><br><span class="line">python prepare.py --action video2image \</span><br><span class="line">                  --input data/custom/video/ref/coffeebox-ref.mp4 \</span><br><span class="line">                  --output data/custom/coffeebox/images \</span><br><span class="line">                  --frame_inter 10 \</span><br><span class="line">                  --image_size 960 \</span><br><span class="line">                  --transpose</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 或者</span></span></span><br><span class="line">python prepare.py --action video2image --input data/custom/video/ammeter-ref.mp4 --output data/custom/ammeter/images --frame_inter 10 --image_size 960 --transpose</span><br></pre></td></tr></table></figure><p>拆分后的视频保存在<code>data/custom/mouse/images</code>中。</p><p><strong>（4）运行COLMAP SfM恢复相机姿态</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action sfm --database_name custom/ammeter --colmap &lt;path-to-your-colmap-exe&gt;</span><br></pre></td></tr></table></figure><p>注：<code>&lt;path-to-your-colmap-exe&gt;</code>可以通过命令<code>which colmap</code>来查找，一般ubuntu路径为<code>/usr/local/bin/colmap</code>，windows路径为<code>E:/Programming/COLMAP-3.8-windows-cuda/COLMAP.bat</code></p><p><strong>（5）手动处理点云</strong></p><p>通过裁减对象点云来手动确定对象所在区域。例如使用<a href="https://www.cloudcompare.org/">CloudCompare</a>来可视化处理COLMAP重建的点云，重建的点云位于<code>data/custom/mouse/colmap/pointcloud.ply</code>中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure><p><img src="https://img.mahaofei.com/img/20230327215520.png" alt=""></p><p>导出裁剪后的点云为<code>data/custom/mouse/object_point_cloud.ply</code>。</p><p><img src="https://img.mahaofei.com/img/20230327220042.png" alt=""></p><p><strong>（6）手动确定对象的X轴正方向和Y轴正方向</strong></p><p><img src="https://img.mahaofei.com/img/20230327220221.png" alt=""></p><p><img src="https://img.mahaofei.com/img/20230327220225.png" alt=""></p><p>编辑一个<code>data/custom/mouse/meta_info.txt</code>文件来保存你的X+和Z+信息，例如</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.297052 0.350839 -0.000593</span><br><span class="line">0.973488 0.054352 -0.222188</span><br></pre></td></tr></table></figure><p><strong>（7）确保您具有以下文件，这些文件由上述步骤生成</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- mouse</span><br><span class="line">           |-- object_point_cloud.ply  ## object point cloud</span><br><span class="line">           |-- meta_info.txt           ## meta information about z+/x+ directions</span><br><span class="line">           |-- images                  ## images</span><br><span class="line">           |-- colmap                  ## colmap project</span><br></pre></td></tr></table></figure><p><strong>（8）从处理后的参考图像中预测姿势</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python predict.py --cfg configs/gen6d_pretrain.yaml \</span><br><span class="line">                  --database custom/coffeebox_lied \</span><br><span class="line">                  --video data/custom/video/coffeebox-test.mp4 \</span><br><span class="line">                  --resolution 960 \</span><br><span class="line">                  --transpose \</span><br><span class="line">                  --output data/custom/ammeter_processed/test \</span><br><span class="line">                  --ffmpeg &lt;path-to-ffmpeg-exe&gt;</span><br></pre></td></tr></table></figure><h2 id="2-3-训练与评估">2.3 训练与评估</h2><p>将文件按照下面的形式组织</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br><span class="line">    |-- shapenet</span><br><span class="line">        |-- shapenet_cache</span><br><span class="line">        |-- shapenet_render</span><br><span class="line">        |-- shapenet_render_v1.pkl</span><br><span class="line">    |-- co3d_256_512</span><br><span class="line">        |-- apple</span><br><span class="line">            ...</span><br><span class="line">    |-- google_scanned_objects</span><br><span class="line">        |-- 06K3jXvzqIM</span><br><span class="line">            ...</span><br><span class="line">    |-- coco</span><br><span class="line">        |-- train2017</span><br></pre></td></tr></table></figure><h3 id="2-3-1-训练detector">2.3.1 训练detector</h3><p>修改<code>train_meta_info.py</code>的第86行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;genmop_train&#x27;</span>: [<span class="string">f&#x27;genmop/<span class="subst">&#123;name&#125;</span>-test&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">&#x27;ammeter&#x27;</span>, <span class="string">&#x27;coffeebox&#x27;</span>, <span class="string">&#x27;realsensebox&#x27;</span>]],</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第109行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">genmop_meta_info=&#123;</span><br><span class="line">    <span class="string">&#x27;ammeter&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0222805</span>, -<span class="number">0.409031</span>, <span class="number">0.912248</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.401556</span>, <span class="number">0.773825</span>, <span class="number">0.340199</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;coffeebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0718405</span>, -<span class="number">0.471545</span>, <span class="number">0.878911</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.582604</span>, -<span class="number">0.490501</span>, -<span class="number">0.219265</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;realsensebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.103463</span>, -<span class="number">0.521284</span>, <span class="number">0.847088</span>],np.float32), <span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">1.690831</span>, <span class="number">0.688506</span>, <span class="number">0.590004</span>],np.float32)&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改<code>database.py</code>的第212行，修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras, images, points3d = read_model(<span class="string">f&#x27;<span class="subst">&#123;GenMOP_ROOT&#125;</span>/<span class="subst">&#123;seq_name&#125;</span>/colmap/sparse/0&#x27;</span>)</span><br></pre></td></tr></table></figure><p>开始训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/detector/detector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-2-训练selector">2.3.2 训练selector</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/selector/selector_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-3-训练refiner">2.3.3 训练refiner</h3><p>为refiner训练进行数据准备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database linemod/cat \</span><br><span class="line">                  --que_split linemod_val \</span><br><span class="line">                  --ref_database linemod/cat \</span><br><span class="line">                  --ref_split linemod_val</span><br><span class="line"></span><br><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database genmop/tformer-test \</span><br><span class="line">                  --que_split all \</span><br><span class="line">                  --ref_database genmop/tformer-ref \</span><br><span class="line">                  --ref_split all </span><br></pre></td></tr></table></figure><p>该命令会在<code>data/val</code>生成信息，该信息会被用于生成refiner的有效数据</p><p>训练refiner</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/refiner/refiner_train.yaml</span><br></pre></td></tr></table></figure><h3 id="2-3-4-评估所有组件">2.3.4 评估所有组件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object TFormer from the GenMOP dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name genmop/tformer</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object <span class="built_in">cat</span> from the LINEMOD dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name linemod/cat</span><br></pre></td></tr></table></figure><h1>三、现存问题</h1><p><strong>优点</strong></p><ol><li>只需要对给定物体录制1-2分钟的视频，使用程序1-2小时<strong>添加数据集</strong>，即可实现新物体的位姿估计，不需要再训练网络</li><li><strong>精度</strong>还可以</li></ol><p><strong>缺点</strong></p><ol><li>对于<strong>方形凸形物体识别较好，对于物体内部存在中空区域</strong>，例如圆环等物体识别效果较差</li><li>由于<strong>参考视频要求物体静止，因此无法录到物体底面的特征</strong>，对于物体底面识别效果较差（可考虑物体正反放置录制两次，对于同一个物体使用两个参考视频进行预测，选择置信度高的位姿）</li><li>当进行识别时，如果<strong>图像中不存在物体也会生成一个估计位姿</strong>（可以考虑根据置信度判断输出，或者在位姿估计前使用yolo等算法预判断物体位置）</li><li>当存在<strong>遮挡时位姿估计效果较差</strong>，可能会出现只框处未被遮挡的部分，或者在遮挡物体上强行进行位姿估计。</li><li>当要同时识别的物体很多时，对于显卡显存要求比较大，而且计算会很慢，服务器1.5s/it。如果每次只对某个特定物体进行识别，速度还可以。</li></ol>]]></content>
    
    
    <summary type="html">算法复现</summary>
    
    
    
    <category term="科研" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文笔记" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    
    <category term="深度学习" scheme="https://www.mahaofei.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="实验" scheme="https://www.mahaofei.com/tags/%E5%AE%9E%E9%AA%8C/"/>
    
    <category term="视觉" scheme="https://www.mahaofei.com/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="位姿估计" scheme="https://www.mahaofei.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="Gen6D" scheme="https://www.mahaofei.com/tags/Gen6D/"/>
    
  </entry>
  
</feed>
